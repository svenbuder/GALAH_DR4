{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa851d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from astropy.table import Table,join,hstack,vstack\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import corner\n",
    "import time\n",
    "import pickle\n",
    "import imageio as iio\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honest-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar Abundances\n",
    "marcs2014_a_x_sun = dict()\n",
    "elements = [\n",
    " \"H\",  \"He\",  \"Li\",  \"Be\",   \"B\",   \"C\",   \"N\",   \"O\",   \"F\",  \"Ne\",\n",
    "\"Na\",  \"Mg\",  \"Al\",  \"Si\",   \"P\",   \"S\",  \"Cl\",  \"Ar\",   \"K\",  \"Ca\",\n",
    "\"Sc\",  \"Ti\",   \"V\",  \"Cr\",  \"Mn\",  \"Fe\",  \"Co\",  \"Ni\",  \"Cu\",  \"Zn\",\n",
    "\"Ga\",  \"Ge\",  \"As\",  \"Se\",  \"Br\",  \"Kr\",  \"Rb\",  \"Sr\",   \"Y\",  \"Zr\",\n",
    "\"Nb\",  \"Mo\",  \"Tc\",  \"Ru\",  \"Rh\",  \"Pd\",  \"Ag\",  \"Cd\",  \"In\",  \"Sn\",\n",
    "\"Sb\",  \"Te\",   \"I\",  \"Xe\",  \"Cs\",  \"Ba\",  \"La\",  \"Ce\",  \"Pr\",  \"Nd\",\n",
    "\"Pm\",  \"Sm\",  \"Eu\",  \"Gd\",  \"Tb\",  \"Dy\",  \"Ho\",  \"Er\",  \"Tm\",  \"Yb\",\n",
    "\"Lu\",  \"Hf\",  \"Ta\",   \"W\",  \"Re\",  \"Os\",  \"Ir\",  \"Pt\",  \"Au\",  \"Hg\",\n",
    "\"Tl\",  \"Pb\",  \"Bi\",  \"Po\",  \"At\",  \"Rn\",  \"Fr\",  \"Ra\",  \"Ac\",  \"Th\",\n",
    "\"Pa\",   \"U\",  \"Np\",  \"Pu\",  \"Am\",  \"Cm\",  \"Bk\",  \"Cs\",  \"Es\"\n",
    "]\n",
    "zeropoints = [\n",
    "12.00, 10.93,  1.05,  1.38,  2.70,  8.39,  7.78,  8.66,  4.56,  7.84,\n",
    " 6.17,  7.53,  6.37,  7.51,  5.36,  7.14,  5.50,  6.18,  5.08,  6.31,\n",
    " 3.17,  4.90,  4.00,  5.64,  5.39,  7.45,  4.92,  6.23,  4.21,  4.60,\n",
    " 2.88,  3.58,  2.29,  3.33,  2.56,  3.25,  2.60,  2.92,  2.21,  2.58,\n",
    " 1.42,  1.92, -8.00,  1.84,  1.12,  1.66,  0.94,  1.77,  1.60,  2.00,\n",
    " 1.00,  2.19,  1.51,  2.24,  1.07,  2.17,  1.13,  1.70,  0.58,  1.45,\n",
    "-8.00,  1.00,  0.52,  1.11,  0.28,  1.14,  0.51,  0.93,  0.00,  1.08,\n",
    " 0.06,  0.88, -0.17,  1.11,  0.23,  1.25,  1.38,  1.64,  1.01,  1.13,\n",
    " 0.90,  2.00,  0.65, -8.00, -8.00, -8.00, -8.00, -8.00, -8.00,  0.06,\n",
    "-8.00, -0.52, -8.00, -8.00, -8.00, -8.00, -8.00, -8.00, -8.00]\n",
    "for (element, zeropoint) in zip(elements, zeropoints):\n",
    "    marcs2014_a_x_sun[element] = zeropoint\n",
    "\n",
    "galah_zeropoints = Table.read('galah_dr4_zeropoints.fits')\n",
    "\n",
    "# Parameter Biases\n",
    "parameter_biases = dict()\n",
    "parameter_biases['teff']  = 5772.0 - galah_zeropoints['teff'][0]\n",
    "parameter_biases['logg']  = 4.438 - galah_zeropoints['logg'][0] # DR3: offset without non-spectroscopic information\n",
    "parameter_biases['fe_h']  = marcs2014_a_x_sun['Fe'] - galah_zeropoints['A_Fe'][0]  # -0.017 VESTA, GAS07: 7.45, DR3: 7.38\n",
    "parameter_biases['vmic']  = 0.\n",
    "parameter_biases['vsini'] = 0.\n",
    "for element in [\n",
    "    'Li','C','N','O',\n",
    "    'Na','Mg','Al','Si',\n",
    "    'K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn',\n",
    "    'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "    'Ba','La','Ce','Nd','Sm','Eu'\n",
    "]:\n",
    "    parameter_biases[element.lower()+'_fe'] = marcs2014_a_x_sun[element] - galah_zeropoints['A_'+element][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fatty-field",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVERSING LOGG PARAMETER_BIAS\n",
      "141 ['131216', '131217', '131220', '140111', '140112', '140113', '140114', '140115', '140116', '140117', '140118', '140207', '140208', '140209', '140303', '140304', '140305', '140307', '140308', '140309', '140310', '140312', '140313', '140314', '140315', '140316', '140409', '140412', '140413', '140414', '140607', '140608', '140609', '140610', '140611', '140707', '140708', '140709', '140710', '140711', '140713', '140805', '140806', '140807', '140808', '140809', '140810', '140811', '140812', '140813', '140814', '140822', '140823', '140824', '141031', '141102', '141103', '141104', '141202', '141231', '150101', '150102', '150103', '150105', '150106', '150107', '150108', '150109', '150112', '150330', '150504', '150531', '150703', '150704', '150705', '150706', '150718', '150719', '150901', '150902', '150903', '151008', '151009', '151109', '151110', '151111', '160325', '160326', '160327', '160328', '160330', '160331', '160602', '160610', '160611', '160612', '160613', '160723', '160724', '170312', '170313', '170314', '170828', '170829', '170830', '190614', '190615', '201130', '201201', '201202', '201204', '201207', '210113', '210114', '210115', '210202', '210203', '210802', '210803', '211113', '211114', '220322', '220324', '220515', '220516', '220517', '220518', '220520', '220522', '220622', '220706', '220708', '220709', '220711', '220712', '220714', '220806', '220807', '220808', '220809', '220816']\n",
      "1 131216\n",
      "2 131217\n",
      "3 131220\n",
      "4 140111\n",
      "5 140112\n",
      "6 140113\n",
      "7 140114\n",
      "8 140115\n",
      "9 140116\n",
      "10 140117\n",
      "11 140118\n",
      "12 140207\n",
      "13 140208\n",
      "14 140209\n",
      "15 140303\n",
      "16 140304\n",
      "17 140305\n",
      "18 140307\n",
      "19 140308\n",
      "20 140309\n",
      "21 140310\n",
      "22 140312\n",
      "23 140313\n",
      "24 140314\n",
      "25 140315\n",
      "26 140316\n",
      "27 140409\n",
      "28 140412\n",
      "29 140413\n",
      "30 140414\n",
      "31 140607\n",
      "32 140608\n",
      "33 140609\n",
      "34 140610\n",
      "35 140611\n",
      "36 140707\n",
      "37 140708\n",
      "38 140709\n",
      "39 140710\n",
      "40 140711\n",
      "41 140713\n",
      "42 140805\n",
      "43 140806\n",
      "44 140807\n",
      "45 140808\n",
      "46 140809\n",
      "47 140810\n",
      "48 140811\n",
      "49 140812\n",
      "50 140813\n",
      "51 140814\n",
      "52 140822\n",
      "53 140823\n",
      "54 140824\n",
      "55 141031\n",
      "56 141102\n",
      "57 141103\n",
      "58 141104\n",
      "59 141202\n",
      "60 141231\n",
      "61 150101\n",
      "62 150102\n",
      "63 150103\n",
      "64 150105\n",
      "65 150106\n",
      "66 150107\n",
      "67 150108\n",
      "68 150109\n",
      "69 150112\n",
      "70 150330\n",
      "71 150504\n",
      "72 150531\n",
      "73 150703\n",
      "74 150704\n",
      "75 150705\n",
      "76 150706\n",
      "77 150718\n",
      "78 150719\n",
      "79 150901\n",
      "80 150902\n",
      "81 150903\n",
      "82 151008\n",
      "83 151009\n",
      "84 151109\n",
      "85 151110\n",
      "86 151111\n",
      "87 160325\n",
      "88 160326\n",
      "89 160327\n",
      "90 160328\n",
      "91 160330\n",
      "92 160331\n",
      "93 160602\n",
      "94 160610\n",
      "95 160611\n",
      "96 160612\n",
      "97 160613\n",
      "98 160723\n",
      "99 160724\n",
      "100 170312\n",
      "101 170313\n",
      "102 170314\n",
      "103 170828\n",
      "104 170829\n",
      "105 170830\n",
      "106 190614\n",
      "107 190615\n",
      "108 201130\n",
      "109 201201\n",
      "110 201202\n",
      "111 201204\n",
      "112 201207\n",
      "113 210113\n",
      "114 210114\n",
      "115 210115\n",
      "116 210202\n",
      "117 210203\n",
      "118 210802\n",
      "119 210803\n",
      "120 211113\n",
      "121 211114\n",
      "122 220322\n",
      "123 220324\n",
      "124 220515\n",
      "125 220516\n",
      "126 220517\n",
      "127 220518\n",
      "128 220520\n",
      "129 220522\n",
      "130 220622\n",
      "131 220706\n",
      "132 220708\n",
      "133 220709\n",
      "134 220711\n",
      "135 220712\n",
      "136 220714\n",
      "137 220806\n",
      "138 220807\n",
      "139 220808\n",
      "140 220809\n",
      "141 220816\n",
      "167752\n"
     ]
    }
   ],
   "source": [
    "def combine_allstar():\n",
    "    \n",
    "    dates = glob.glob('daily/*plxcom.fits')\n",
    "    dates = [date[38:38+6] for date in dates]\n",
    "    dates = np.unique(dates)\n",
    "    dates.sort()\n",
    "    \n",
    "    print('REVERSING LOGG PARAMETER_BIAS')\n",
    "    \n",
    "    print(len(dates),list(dates))\n",
    "    \n",
    "    data_allstar  = Table.read('daily/galah_dr4_allspec_not_validated_'+str(dates[0])+'_plxcom.fits')\n",
    "\n",
    "    print(1,dates[0])\n",
    "    for ind,date in enumerate(dates[1:]):\n",
    "        print(ind+2,date)\n",
    "        try:\n",
    "            data_next = Table.read('daily/galah_dr4_allspec_not_validated_'+str(date)+'_plxcom.fits')\n",
    "            data_allstar = vstack([data_allstar, data_next])\n",
    "        except:\n",
    "            print('Could not read single: '+str(date))\n",
    "        \n",
    "    return(data_allstar)\n",
    "\n",
    "data_allstar = combine_allstar()\n",
    "\n",
    "for label in ['logg']:\n",
    "    data_allstar[label] -= parameter_biases[label]\n",
    "\n",
    "data_allstar.write('galah_dr4_allstar_not_validated.fits',overwrite=True)\n",
    "print(len(data_allstar['sobject_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conventional-polls",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574 ['131216', '131217', '131220', '140111', '140112', '140113', '140114', '140115', '140116', '140117', '140118', '140207', '140208', '140209', '140210', '140211', '140212', '140303', '140304', '140305', '140307', '140308', '140309', '140310', '140312', '140313', '140314', '140315', '140316', '140409', '140412', '140413', '140414', '140607', '140608', '140609', '140610', '140611', '140707', '140708', '140709', '140710', '140711', '140713', '140805', '140806', '140807', '140808', '140809', '140810', '140811', '140812', '140813', '140814', '140822', '140823', '140824', '141031', '141102', '141103', '141104', '141202', '141231', '150101', '150102', '150103', '150105', '150106', '150107', '150108', '150109', '150112', '150204', '150205', '150206', '150207', '150208', '150209', '150210', '150211', '150330', '150401', '150405', '150406', '150407', '150408', '150409', '150410', '150411', '150412', '150413', '150426', '150427', '150428', '150429', '150430', '150504', '150531', '150601', '150602', '150603', '150604', '150605', '150606', '150607', '150703', '150704', '150705', '150706', '150718', '150719', '150824', '150826', '150827', '150828', '150829', '150830', '150831', '150901', '150902', '150903', '151008', '151009', '151109', '151110', '151111', '151219', '151220', '151223', '151225', '151226', '151227', '151228', '151229', '151230', '151231', '160106', '160107', '160108', '160109', '160110', '160111', '160112', '160113', '160123', '160124', '160125', '160126', '160129', '160130', '160325', '160326', '160327', '160328', '160330', '160331', '160401', '160402', '160403', '160415', '160417', '160418', '160419', '160420', '160422', '160423', '160424', '160425', '160426', '160512', '160513', '160514', '160515', '160517', '160518', '160519', '160520', '160521', '160522', '160523', '160524', '160525', '160527', '160529', '160530', '160531', '160602', '160610', '160611', '160612', '160613', '160723', '160724', '160811', '160813', '160814', '160815', '160816', '160817', '160916', '160919', '160921', '160923', '161006', '161007', '161008', '161009', '161011', '161012', '161013', '161104', '161105', '161106', '161107', '161108', '161109', '161115', '161116', '161117', '161118', '161119', '161209', '161210', '161211', '161212', '161213', '161216', '161217', '161218', '161219', '161228', '161229', '161231', '170102', '170103', '170104', '170105', '170106', '170107', '170108', '170109', '170110', '170111', '170112', '170113', '170114', '170115', '170116', '170117', '170118', '170119', '170121', '170122', '170127', '170128', '170129', '170130', '170131', '170202', '170203', '170205', '170206', '170207', '170215', '170216', '170217', '170218', '170219', '170220', '170312', '170313', '170314', '170403', '170404', '170406', '170407', '170408', '170409', '170410', '170411', '170412', '170413', '170414', '170415', '170416', '170417', '170418', '170506', '170507', '170508', '170509', '170510', '170511', '170512', '170513', '170514', '170515', '170516', '170517', '170530', '170531', '170601', '170602', '170603', '170604', '170614', '170615', '170710', '170711', '170712', '170713', '170723', '170724', '170725', '170801', '170802', '170805', '170806', '170828', '170829', '170830', '170904', '170905', '170906', '170907', '170908', '170909', '170910', '170911', '170912', '170928', '171001', '171003', '171027', '171029', '171031', '171101', '171102', '171104', '171106', '171205', '171206', '171207', '171208', '171227', '171228', '171230', '180101', '180102', '180103', '180125', '180126', '180129', '180130', '180131', '180602', '180603', '180604', '180620', '180621', '180622', '180623', '180625', '180628', '181220', '181221', '181222', '181223', '181224', '181225', '181226', '190204', '190205', '190206', '190207', '190209', '190210', '190211', '190212', '190223', '190224', '190225', '190614', '190615', '191106', '191107', '191108', '191109', '191114', '191115', '191116', '200213', '200214', '200215', '200216', '200511', '200512', '200513', '200514', '200515', '200516', '200517', '200519', '200521', '200528', '200529', '200530', '200531', '200708', '200709', '200712', '200714', '200724', '200728', '200801', '200802', '200803', '200804', '200805', '200810', '200824', '200825', '200826', '200831', '200901', '200902', '200903', '200904', '200905', '200906', '200907', '200926', '200927', '201001', '201002', '201003', '201004', '201005', '201006', '201007', '201130', '201201', '201202', '201204', '201207', '210113', '210114', '210115', '210116', '210117', '210122', '210123', '210124', '210125', '210126', '210202', '210203', '210324', '210325', '210327', '210328', '210329', '210330', '210331', '210401', '210402', '210403', '210404', '210405', '210422', '210513', '210514', '210515', '210516', '210517', '210518', '210519', '210520', '210521', '210522', '210523', '210524', '210531', '210602', '210605', '210606', '210607', '210608', '210612', '210614', '210706', '210707', '210710', '210711', '210715', '210716', '210718', '210719', '210721', '210725', '210726', '210727', '210728', '210729', '210730', '210731', '210802', '210803', '210914', '210915', '210916', '210918', '210919', '210920', '210921', '210922', '210923', '210925', '210926', '210927', '211113', '211114', '211213', '211214', '211215', '211216', '211217', '211219', '211220', '211221', '220120', '220121', '220122', '220123', '220124', '220125', '220214', '220215', '220216', '220217', '220218', '220219', '220220', '220221', '220322', '220324', '220413', '220414', '220415', '220416', '220417', '220418', '220420', '220421', '220422', '220515', '220516', '220517', '220518', '220520', '220522', '220622', '220706', '220708', '220709', '220711', '220712', '220714', '220806', '220807', '220808', '220809', '220815', '220816']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39d83084d085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     return(data_single, 1, data_plxlogg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_plxlogg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_allspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def combine_allspec():\n",
    "    \n",
    "    dates = glob.glob('daily/*single.fits')\n",
    "    dates = [date[38:38+6] for date in dates]\n",
    "    dates = np.unique(dates)\n",
    "    dates.sort()\n",
    "    try:\n",
    "        dates = np.delete(dates,np.where('plxlog' == dates)[0][0])\n",
    "    except:\n",
    "        pass\n",
    "    print(len(dates),list(dates))\n",
    "    \n",
    "#     data_single  = Table.read('daily/galah_dr4_allspec_not_validated_'+str(dates[0])+'_single.fits')\n",
    "# #     data_binary  = Table.read('daily/galah_dr4_allspec_not_validated_'+str(dates[0])+'_binary.fits')\n",
    "#     data_plxlogg = Table.read('daily/galah_dr4_allspec_not_validated_plxlogg_'+str(dates[0])+'.fits')\n",
    "\n",
    "#     for ind,date in enumerate(dates[1:]):\n",
    "#         print(ind,date)\n",
    "#         try:\n",
    "#             data_next = Table.read('daily/galah_dr4_allspec_not_validated_'+str(date)+'_single.fits')\n",
    "#             data_single = vstack([data_single, data_next])\n",
    "#         except:\n",
    "#             print('Could not read single: '+str(date))\n",
    "        \n",
    "# #         try:\n",
    "# #             data_next = Table.read('daily/galah_dr4_allspec_not_validated_'+str(date)+'_binary.fits')\n",
    "# #             data_binary = vstack([data_binary, data_next])\n",
    "# #         except:\n",
    "# #             pass\n",
    "\n",
    "#         try:\n",
    "#             data_next = Table.read('daily/galah_dr4_allspec_not_validated_plxlogg_'+str(date)+'.fits')\n",
    "#         except:\n",
    "#             data_next = data_next['sobject_id']\n",
    "#             print('Could not read plxcom: '+str(date))\n",
    "#         data_plxlogg = vstack([data_plxlogg, data_next])\n",
    "#     return(data_single, 1, data_plxlogg)\n",
    "\n",
    "data, data_binary, data_plxlogg = combine_allspec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data['sobject_id']) == len(data_plxlogg['sobject_id']):\n",
    "    for key in data_plxlogg.keys():\n",
    "        \n",
    "        if key in ['sobject_id','gaiadr3_source_id','teff','fe_h','logg_plx']:\n",
    "            pass\n",
    "        elif key == 'logg':\n",
    "            if 'logg_spec' not in data.keys():\n",
    "                data['logg_spec'] = data['logg']\n",
    "                data['e_logg_spec'] = data['e_logg']\n",
    "                data['logg'] = data_plxlogg['logg_plx']\n",
    "                data['e_logg'][:] = np.NaN\n",
    "        elif key in ['r_med','r_lo','r_hi']:\n",
    "            data_plxlogg[key][data_plxlogg[key] > 1000000.] = np.NaN\n",
    "            data[key] = np.array(data_plxlogg[key],dtype=np.float32)\n",
    "        else:\n",
    "            data[key] = np.array(data_plxlogg[key],dtype=np.float32)\n",
    "else:\n",
    "    print('Not same length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plxlogg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-dollar",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use = np.isfinite(data['logg_spec']) #& (~data['logg'].mask)\n",
    "\n",
    "f, gs = plt.subplots(1,3,figsize=(10,3))\n",
    "ax = gs[0]\n",
    "ax.hist2d(\n",
    "    data['teff'][use],\n",
    "    data['logg_spec'][use],\n",
    "    bins=(np.arange(2900,8100,25),np.arange(-0.5,5.5,0.05)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(8100,2900)\n",
    "ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('Teff(spec)',fontsize=15)\n",
    "ax.set_ylabel('logg(spec)',fontsize=15)\n",
    "\n",
    "ax = gs[1]\n",
    "ax.hist2d(\n",
    "    data['teff'][use],\n",
    "    data['logg'][use],\n",
    "    bins=(np.arange(2900,8100,25),np.arange(-0.5,5.5,0.05)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(8100,2900)\n",
    "ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('Teff(spec)',fontsize=15)\n",
    "ax.set_ylabel('logg(plx)',fontsize=15)\n",
    "\n",
    "ax = gs[2]\n",
    "s = ax.hist2d(\n",
    "    data['logg'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.linspace(-0.5,5.5,100),np.linspace(-2,2,100)), cmin=1, norm=LogNorm(),label='100,000 stars'\n",
    ")\n",
    "\n",
    "steps = 0.5\n",
    "loggs = np.arange(-0.5,5.51,steps)\n",
    "dloggs = []\n",
    "sloggs = []\n",
    "for logg in loggs:\n",
    "    in_bin = abs(data['logg'][use] - logg) < 0.5*steps\n",
    "    if len(data['logg'][use][in_bin]) > 100:\n",
    "        dloggs.append(np.nanmedian(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "        sloggs.append(np.nanstd(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "    else:\n",
    "        dloggs.append(np.NaN)\n",
    "        sloggs.append(np.NaN)\n",
    "        \n",
    "print(dloggs)\n",
    "plt.errorbar(\n",
    "    loggs,dloggs,yerr=sloggs,fmt='o',c='r',ms=3,capsize=5,lw=1,label='median/std'\n",
    ")\n",
    "plt.axhline(0,c='lightblue',lw=2,label='dlogg = 0')\n",
    "ax.set_xlabel('logg(plx)',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout(w_pad=0)\n",
    "plt.savefig('figures/dlogg_spec_plx.png',dpi=200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "use = np.isfinite(data['logg_spec']) & (data['logg'] < 3) #& (~data['logg'].mask)\n",
    "\n",
    "f, gs = plt.subplots(1,3,figsize=(10,3))\n",
    "ax = gs[0]\n",
    "ax.hist2d(\n",
    "    data['teff'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.arange(2900,8100,25),np.linspace(-2,2,100)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(8100,2900)\n",
    "# ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('Teff(spec)',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "\n",
    "ax = gs[1]\n",
    "s = ax.hist2d(\n",
    "    data['logg'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.linspace(-0.5,5.5,100),np.linspace(-2,2,100)), cmin=1, norm=LogNorm(),label='100,000 stars'\n",
    ")\n",
    "\n",
    "steps = 0.5\n",
    "loggs = np.arange(-0.5,5.51,steps)\n",
    "dloggs = []\n",
    "sloggs = []\n",
    "for logg in loggs:\n",
    "    in_bin = abs(data['logg'][use] - logg) < 0.5*steps\n",
    "    if len(data['logg'][use][in_bin]) > 100:\n",
    "        dloggs.append(np.nanmedian(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "        sloggs.append(np.nanstd(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "    else:\n",
    "        dloggs.append(np.NaN)\n",
    "        sloggs.append(np.NaN)\n",
    "        \n",
    "#print(dloggs)\n",
    "ax.errorbar(\n",
    "    loggs,dloggs,yerr=sloggs,fmt='o',c='r',ms=3,capsize=5,lw=1,label='median/std'\n",
    ")\n",
    "ax.axhline(0,c='lightblue',lw=2,label='dlogg = 0')\n",
    "ax.set_xlabel('logg(plx)',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "ax = gs[2]\n",
    "ax.hist2d(\n",
    "    data['fe_h'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.linspace(-3.0,0.75,100),np.linspace(-2,2,100)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(-3.0,0.75)\n",
    "# ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('[Fe/H]',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "\n",
    "\n",
    "plt.tight_layout(w_pad=0)\n",
    "\n",
    "use = np.isfinite(data['logg_spec']) & (data['logg'] >= 3) #& (~data['logg'].mask)\n",
    "\n",
    "f, gs = plt.subplots(1,3,figsize=(10,3))\n",
    "ax = gs[0]\n",
    "ax.hist2d(\n",
    "    data['teff'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.arange(2900,8100,25),np.linspace(-2,2,100)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(8100,2900)\n",
    "# ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('Teff(spec)',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "\n",
    "ax = gs[1]\n",
    "s = ax.hist2d(\n",
    "    data['logg'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.linspace(-0.5,5.5,100),np.linspace(-2,2,100)), cmin=1, norm=LogNorm(),label='100,000 stars'\n",
    ")\n",
    "\n",
    "steps = 0.5\n",
    "loggs = np.arange(-0.5,5.51,steps)\n",
    "dloggs = []\n",
    "sloggs = []\n",
    "for logg in loggs:\n",
    "    in_bin = abs(data['logg'][use] - logg) < 0.5*steps\n",
    "    if len(data['logg'][use][in_bin]) > 100:\n",
    "        dloggs.append(np.nanmedian(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "        sloggs.append(np.nanstd(data['logg_spec'][use][in_bin] - data['logg'][use][in_bin]))\n",
    "    else:\n",
    "        dloggs.append(np.NaN)\n",
    "        sloggs.append(np.NaN)\n",
    "        \n",
    "#print(dloggs)\n",
    "ax.errorbar(\n",
    "    loggs,dloggs,yerr=sloggs,fmt='o',c='r',ms=3,capsize=5,lw=1,label='median/std'\n",
    ")\n",
    "ax.axhline(0,c='lightblue',lw=2,label='dlogg = 0')\n",
    "ax.set_xlabel('logg(plx)',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "ax = gs[2]\n",
    "ax.hist2d(\n",
    "    data['fe_h'][use],\n",
    "    data['logg_spec'][use] - data['logg'][use],\n",
    "    bins=(np.linspace(-3.0,0.75,100),np.linspace(-2,2,100)), cmin = 1, norm=LogNorm()\n",
    ")\n",
    "ax.set_xlim(-3.0,0.75)\n",
    "# ax.set_ylim(5.5,-0.5)\n",
    "ax.set_xlabel('[Fe/H]',fontsize=15)\n",
    "ax.set_ylabel('dlogg(plx-spec)',fontsize=15)\n",
    "\n",
    "plt.tight_layout(w_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = Table.read('../spectrum_analysis/galah_dr4_initial_parameters_230101_lite.fits')\n",
    "init['date'] = np.array([str(x)[:6] for x in init['sobject_id']])\n",
    "\n",
    "bad_ccd3 = init[init['cdelt_flag']==4]\n",
    "bad_ccd3['date'] = np.array([str(x)[:6] for x in bad_ccd3['sobject_id']])\n",
    "\n",
    "bad_ccd3_dates = np.unique(bad_ccd3['date'])\n",
    "\n",
    "obs6p1 = np.unique(np.array([x[20:20+6] for x in glob.glob('../observations_6p1/*')]))\n",
    "\n",
    "# rerun_input = []\n",
    "# for unique_date in np.unique(init['date']):\n",
    "#     print(len(init['date'][init['date'] == unique_date]))\n",
    "#     if (unique_date in obs6p1) & (unique_date in bad_ccd3_dates):\n",
    "#         print('X')\n",
    "#         print(len(bad_ccd3['sobject_id'][bad_ccd3['date'] == unique_date]))\n",
    "#     else:\n",
    "#         print('')#Cannot rerun '+unique_date)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"final_flag_sp_dictionary.pkl\", \"rb\")\n",
    "flag_sp_dictionary = pickle.load(a_file)\n",
    "a_file.close()\n",
    "\n",
    "binary_setup = data['setup']=='binary'\n",
    "\n",
    "triple = binary_setup & ((data['flag_sp'] & flag_sp_dictionary['is_sb2'][0]) > 0)\n",
    "double = binary_setup & ((data['flag_sp'] & flag_sp_dictionary['is_sb2'][0]) == 0)\n",
    "\n",
    "data['flag_sp'][double] += flag_sp_dictionary['is_sb2'][0]\n",
    "\n",
    "data['flag_sp'][triple] += flag_sp_dictionary['sb_triple_warn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for element in [\n",
    "#         'Li','C','N','O',\n",
    "#         'Na','Mg','Al','Si',\n",
    "#         'K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn',\n",
    "#         'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "#         'Ba','La','Ce','Nd','Sm','Eu'\n",
    "# ]:\n",
    "#     data[element.lower()+'_fe'][np.where(data['flag_'+element.lower()+'_fe'] == -1)[0]] = np.NaN\n",
    "#     data['e_'+element.lower()+'_fe'][np.where(data['flag_'+element.lower()+'_fe'] == -1)[0]] = np.NaN\n",
    "#     data[element.lower()+'_fe'][np.where(data['flag_'+element.lower()+'_fe'] == 2)[0]] = np.NaN\n",
    "#     data['flag_'+element.lower()+'_fe'][np.where(data['flag_'+element.lower()+'_fe'] == -1)[0]] = 2\n",
    "# data['fe_h'][np.where(data['flag_fe_h'] == -1)[0]] = np.NaN\n",
    "# data['e_fe_h'][np.where(data['flag_fe_h'] == -1)[0]] = np.NaN\n",
    "# data['fe_h'][np.where(data['flag_fe_h'] == 2)[0]] = np.NaN\n",
    "# data['flag_fe_h'][np.where(data['flag_fe_h'] == -1)[0]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f71bec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[[0,1,2,3,4,-5,-4,-3,-2,-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-likelihood",
   "metadata": {},
   "source": [
    "# Populate best_spec4star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-characteristic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# debug = False\n",
    "# # debug = True\n",
    "\n",
    "# for tmass_id in np.unique(data['tmass_id']):\n",
    "    \n",
    "#     if tmass_id != 'None':\n",
    "\n",
    "#         # First let's see how many matches we actually have\n",
    "#         find_matches = np.where(tmass_id == data['tmass_id'])[0]\n",
    "\n",
    "#         # If only 1 entry found, we most likely can simply take that\n",
    "#         if len(find_matches) == 1:\n",
    "\n",
    "#             # Sanity check: is it really a single one?\n",
    "#             if data['setup'][find_matches[0]] == 'single':\n",
    "#                 data['best_spec4star'][find_matches[0]] = True\n",
    "#             # If not: you better run the single setup as well!\n",
    "#             else:\n",
    "#                 raise ValueError('1 entry found, but not single?!')\n",
    "\n",
    "#         # If we have multiple matches: check out which ones are single/binary/coadds\n",
    "#         else:\n",
    "\n",
    "#             single_matches = find_matches[np.where(data['setup'][find_matches]=='single')[0]]\n",
    "#             binary_matches = find_matches[np.where(data['setup'][find_matches]=='binary')[0]]\n",
    "#             coadds_matches = find_matches[np.where(data['setup'][find_matches]=='coadds')[0]]\n",
    "#             if len(coadds_matches) > 1:\n",
    "#                 raise ValueError('Multiple coadded measurements available?!')\n",
    "#             elif len(coadds_matches) == 1:\n",
    "#                 coadds_match = coadds_matches[0]\n",
    "#             nonbin_matches = find_matches[np.where(data['setup'][find_matches]!='binary')[0]]\n",
    "\n",
    "#             if debug:\n",
    "#                 print(tmass_id,':',list(data['sobject_id'][find_matches]))\n",
    "#                 print('    flag_sp:',list(find_matches),list(data['setup'][find_matches]))\n",
    "#                 print('    flag_sp:',list(data['flag_sp'][find_matches]))\n",
    "\n",
    "#             # The rundown of our preference:\n",
    "#             # if it is clearly better fit with a binary analysis: prefer that!\n",
    "#             #     Double check if there are mutiple binary analyses and take \n",
    "#             #       1) the highest unflagged one\n",
    "#             #       2) the unflagged one\n",
    "#             #       3) the highest snr one\n",
    "#             # elif there is a coadds run, prefer that\n",
    "#             # else: there should be a coadds run!\n",
    "\n",
    "#             # When do we believe it is a binary?\n",
    "#             it_is_a_binary = False\n",
    "\n",
    "#             # Let's test that, if binary setup(s) avaliable\n",
    "#             if len(binary_matches) > 0:\n",
    "\n",
    "#                 if debug:\n",
    "#                     print('--> binary found!')\n",
    "\n",
    "#                 # If only 1 binary measurement is available\n",
    "#                 if len(binary_matches) == 1:\n",
    "#                     binary_match = binary_matches[0]\n",
    "\n",
    "#                     # chi2 has to be better by at least 5% in more than 50% of the cases\n",
    "#                     binary_criterion_a = (np.nanmedian(data['chi2_sp'][binary_match] / list(data['chi2_sp'][nonbin_matches])) < 0.95)\n",
    "#                     # ΔRV of the 2 binary components has to be beyond 10 km/s\n",
    "#                     binary_criterion_b = (np.abs(data['rv_comp_1'][binary_match] - data['rv_comp_2'][binary_match]) > 10)\n",
    "#                     # ΔRV of the 2 binary components has to be within 300 km/s\n",
    "#                     binary_criterion_c = (np.abs(data['rv_comp_1'][binary_match] - data['rv_comp_2'][binary_match]) < 300)\n",
    "\n",
    "#                     if (\n",
    "#                         binary_criterion_a &\n",
    "#                         binary_criterion_b &\n",
    "#                         binary_criterion_c\n",
    "#                     ):\n",
    "#                         it_is_a_binary = True\n",
    "\n",
    "#                     if debug:\n",
    "#                         print('    a) chi2 ratios:',binary_criterion_a,data['chi2_sp'][binary_match] / list(data['chi2_sp'][nonbin_matches]))\n",
    "#                         print('    delta rv1/2:',binary_criterion_b,data['rv_comp_1'][binary_match],data['rv_comp_2'][binary_match],np.abs(data['rv_comp_1'][binary_match] - data['rv_comp_2'][binary_match]))\n",
    "#                         print('    b) delta rv1/2  >  10?: ',binary_criterion_b)\n",
    "#                         print('    c) delta rv1/2  < 300?: ',binary_criterion_c)\n",
    "#                         print('    Vertict?:',it_is_a_binary)\n",
    "\n",
    "#                     if it_is_a_binary:\n",
    "#                         data['best_spec4star'][binary_match] = True\n",
    "\n",
    "#                 # If multiple binary measurements are available\n",
    "#                 else:\n",
    "\n",
    "#                     single_matches_to_binary_matches = []\n",
    "#                     for sobject_id in data['sobject_id'][binary_matches]:\n",
    "\n",
    "#                         single_matches_to_binary_matches.append(single_matches[np.where(data['sobject_id'][single_matches] == sobject_id)[0][0]])\n",
    "\n",
    "#                     if np.all(data['sobject_id'][single_matches_to_binary_matches] == data['sobject_id'][binary_matches]):\n",
    "\n",
    "#                         # Let's first check if these binaries hold up the basic tests from criterions a,b,c\n",
    "#                         valid_binary_matches = (\n",
    "#                             (abs(data['chi2_sp'][binary_matches]/data['chi2_sp'][single_matches_to_binary_matches]) < 0.95) &\n",
    "#                             (abs(data['rv_comp_1'][binary_matches]-data['rv_comp_2'][binary_matches]) > 10) &\n",
    "#                             (abs(data['rv_comp_1'][binary_matches]-data['rv_comp_2'][binary_matches]) < 300)\n",
    "#                         )\n",
    "#                         # If there is a coadds match, let's also check agains that measurement\n",
    "#                         if len(coadds_matches) == 1:\n",
    "#                             valid_binary_matches = (\n",
    "#                                 (abs(data['chi2_sp'][binary_matches]/data['chi2_sp'][single_matches_to_binary_matches]) < 0.95) &\n",
    "#                                 (abs(data['chi2_sp'][binary_matches]/data['chi2_sp'][coadds_match]) < 0.95) &\n",
    "#                                 (abs(data['rv_comp_1'][binary_matches]-data['rv_comp_2'][binary_matches]) > 10) &\n",
    "#                                 (abs(data['rv_comp_1'][binary_matches]-data['rv_comp_2'][binary_matches]) < 300)\n",
    "#                             )\n",
    "\n",
    "#                         # If we have at least 1 good measurement\n",
    "#                         if len(binary_matches[valid_binary_matches]) > 0:\n",
    "\n",
    "#                             it_is_a_binary = True\n",
    "\n",
    "#                             # Then find the one with the largest separation and use it as best_spec4star\n",
    "#                             larger_rv_separation = np.argmax(abs(data['rv_comp_1'][binary_matches[valid_binary_matches]]-data['rv_comp_2'][binary_matches[valid_binary_matches]]))\n",
    "#                             data['best_spec4star'][binary_matches[valid_binary_matches][larger_rv_separation]] = True\n",
    "#                     else:\n",
    "\n",
    "#                         raise ValueError('sobject_id of single and binary not in the same order...')\n",
    "\n",
    "#             if not it_is_a_binary:\n",
    "\n",
    "#                 # If it is not a binary, prefer the coadds\n",
    "#                 if len(coadds_matches) == 1:\n",
    "\n",
    "#                     if debug:\n",
    "#                         print('--> coadds found (but not binary)!')\n",
    "\n",
    "#                         print(tmass_id,':',list(data['sobject_id'][find_matches]))\n",
    "#                         print('    flag_sp:',list(find_matches),list(data['setup'][find_matches]))\n",
    "#                         print('    flag_sp:',list(data['flag_sp'][find_matches]))\n",
    "\n",
    "#                         print('    SNRs:')\n",
    "#                         print('         ',data['snr_px_ccd2'][coadds_match])\n",
    "#                         print('         ',list(data['snr_px_ccd2'][single_matches]))\n",
    "\n",
    "#                     data['best_spec4star'][coadds_match] = True\n",
    "\n",
    "#                 # If there is no coadds: take the single measurement\n",
    "#                 elif len(single_matches) == 1:\n",
    "#                     data['best_spec4star'][single_matches[0]] = True\n",
    "\n",
    "#         if len(np.where(data['best_spec4star'][find_matches] == True)[0]) != 1:\n",
    "\n",
    "#             # This should only be activated, if we have not run coadds for all stars!\n",
    "\n",
    "#             best_single_match = single_matches[np.argmin(data['flag_sp'][single_matches])]\n",
    "#             data['best_spec4star'][best_single_match] = True\n",
    "\n",
    "# no_tmass_id = data['tmass_id'] == 'None'\n",
    "# entries = np.arange(len(data['sobject_id']))\n",
    "\n",
    "# for object_index, sobject_id in enumerate(data['sobject_id'][no_tmass_id]):\n",
    "        \n",
    "#     index = entries[no_tmass_id][object_index]\n",
    "#     same_coordinates = np.where((data['ra'][index]==data['ra']) & (data['dec'][index]==data['dec']))[0]\n",
    "\n",
    "#     single_matches = np.where(data['setup'][same_coordinates] == 'single')[0]\n",
    "#     binary_matches = np.where(data['setup'][same_coordinates] == 'binary')[0]\n",
    "#     coadds_matches = np.where(data['setup'][same_coordinates] == 'coadds')[0]\n",
    "#     nonbin_matches = np.where(data['setup'][same_coordinates] != 'binary')[0]\n",
    "    \n",
    "#     already_best = np.where(data['best_spec4star'][same_coordinates]==True)[0]\n",
    "\n",
    "#     it_is_a_binary = False\n",
    "    \n",
    "#     if len(same_coordinates) == 1:\n",
    "#         data['best_spec4star'][index] = True\n",
    "        \n",
    "#     elif len(already_best) > 0:\n",
    "#         pass\n",
    "    \n",
    "#     elif len(binary_matches) > 0:\n",
    "        \n",
    "#         if len(binary_matches) == 1:\n",
    "#             binary_match = binary_matches[0]\n",
    "        \n",
    "#             # chi2 has to be better by at least 5% in more than 50% of the cases\n",
    "#             binary_criterion_a = (np.nanmedian(data['chi2_sp'][same_coordinates[binary_match]] / list(data['chi2_sp'][same_coordinates[nonbin_matches]])) < 0.95)\n",
    "#             # ΔRV of the 2 binary components has to be beyond 10 km/s\n",
    "#             binary_criterion_b = (np.abs(data['rv_comp_1'][same_coordinates[binary_match]] - data['rv_comp_2'][same_coordinates[binary_match]]) > 10)\n",
    "#             # ΔRV of the 2 binary components has to be within 300 km/s\n",
    "#             binary_criterion_c = (np.abs(data['rv_comp_1'][same_coordinates[binary_match]] - data['rv_comp_2'][same_coordinates[binary_match]]) < 300)\n",
    "            \n",
    "#             if debug:\n",
    "#                 print('    a) chi2 ratios:',binary_criterion_a,data['chi2_sp'][same_coordinates[binary_match]] / list(data['chi2_sp'][same_coordinates[nonbin_matches]]))\n",
    "#                 print('    delta rv1/2:',binary_criterion_b,data['rv_comp_1'][same_coordinates[binary_match]],data['rv_comp_2'][same_coordinates[binary_match]],np.abs(data['rv_comp_1'][same_coordinates[binary_match]] - data['rv_comp_2'][same_coordinates[binary_match]]))\n",
    "#                 print('    b) delta rv1/2  >  10?: ',binary_criterion_b)\n",
    "#                 print('    c) delta rv1/2  < 300?: ',binary_criterion_c)\n",
    "#                 print('    Vertict?:',it_is_a_binary)\n",
    "\n",
    "#             if (\n",
    "#                 binary_criterion_a &\n",
    "#                 binary_criterion_b &\n",
    "#                 binary_criterion_c\n",
    "#             ):\n",
    "#                 it_is_a_binary = True\n",
    "#         else:\n",
    "            \n",
    "#             single_matches_to_binary_matches = []\n",
    "#             for sobject_id in data['sobject_id'][same_coordinates[binary_matches]]:\n",
    "\n",
    "#                 single_matches_to_binary_matches.append(single_matches[np.where(data['sobject_id'][same_coordinates][single_matches] == sobject_id)[0][0]])\n",
    "                \n",
    "#             if np.all(data['sobject_id'][same_coordinates][single_matches_to_binary_matches] == data['sobject_id'][same_coordinates[binary_matches]]):\n",
    "\n",
    "#                 # Let's first check if these binaries hold up the basic tests from criterions a,b,c\n",
    "#                 valid_binary_matches = (\n",
    "#                     (abs(data['chi2_sp'][same_coordinates[binary_matches]]/data['chi2_sp'][same_coordinates][single_matches_to_binary_matches]) < 0.95) &\n",
    "#                     (abs(data['rv_comp_1'][same_coordinates[binary_matches]]-data['rv_comp_2'][same_coordinates[binary_matches]]) > 10) &\n",
    "#                     (abs(data['rv_comp_1'][same_coordinates[binary_matches]]-data['rv_comp_2'][same_coordinates[binary_matches]]) < 300)\n",
    "#                 )\n",
    "#                 # If there is a coadds match, let's also check agains that measurement\n",
    "#                 if len(coadds_matches) == 1:\n",
    "#                     valid_binary_matches = (\n",
    "#                         (abs(data['chi2_sp'][same_coordinates[binary_matches]]/data['chi2_sp'][same_coordinates][single_matches_to_binary_matches]) < 0.95) &\n",
    "#                         (abs(data['chi2_sp'][same_coordinates[binary_matches]]/data['chi2_sp'][same_coordinates][coadds_match]) < 0.95) &\n",
    "#                         (abs(data['rv_comp_1'][same_coordinates[binary_matches]]-data['rv_comp_2'][same_coordinates[binary_matches]]) > 10) &\n",
    "#                         (abs(data['rv_comp_1'][same_coordinates[binary_matches]]-data['rv_comp_2'][same_coordinates[binary_matches]]) < 300)\n",
    "#                     )\n",
    "                    \n",
    "#                 # If we have at least 1 good measurement\n",
    "#                 if len(binary_matches[valid_binary_matches]) > 0:\n",
    "\n",
    "#                     it_is_a_binary = True\n",
    "\n",
    "#                     # Then find the one with the largest separation and use it as best_spec4star\n",
    "#                     larger_rv_separation = np.argmax(abs(data['rv_comp_1'][same_coordinates[binary_matches[valid_binary_matches]]]-data['rv_comp_2'][same_coordinates[binary_matches[valid_binary_matches]]]))\n",
    "#                     data['best_spec4star'][same_coordinates][binary_matches[valid_binary_matches][larger_rv_separation]] = True\n",
    "#             else:\n",
    "\n",
    "#                 raise ValueError('sobject_id of single and binary not in the same order...')\n",
    "\n",
    "#     if not it_is_a_binary:\n",
    "\n",
    "#         if len(coadds_matches) > 0:\n",
    "            \n",
    "#             if len(coadds_matches) == 1:\n",
    "#                 data['setup'][same_coordinates][coadds_matches[0]] = True\n",
    "            \n",
    "#             else:\n",
    "#                 raise ValueError('Multiple coadds measurements for this non-2MASS star')\n",
    "#         elif len(single_matches) == 1:\n",
    "#             data['best_spec4star'][same_coordinates][single_matches[0]] = True\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             # This should only be activated, if we have not run coadds for all stars!\n",
    "\n",
    "#             if debug:\n",
    "#                 print('There should be a coadds spectrum here too!')\n",
    "\n",
    "#             best_single_match = single_matches[np.argmin(data['flag_sp'][same_coordinates][single_matches])]\n",
    "#             data['best_spec4star'][same_coordinates[best_single_match]] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-attachment",
   "metadata": {},
   "source": [
    "# Save FITS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write('galah_dr4_allspec_not_validated.fits',overwrite=True)\n",
    "print(len(data['sobject_id']))\n",
    "\n",
    "# data_allstar = data[data['best_spec4star']==True]\n",
    "# data_allstar.sort(keys='ra')\n",
    "# data_allstar.write('galah_dr4_allstar_not_validated.fits',overwrite=True)\n",
    "# data_allstar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-furniture",
   "metadata": {},
   "source": [
    "# Flag dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231600",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"final_flag_sp_dictionary.pkl\", \"rb\")\n",
    "flag_sp_dictionary = pickle.load(a_file)\n",
    "a_file.close()\n",
    "print(flag_sp_dictionary)\n",
    "\n",
    "entries = []\n",
    "for flag in np.unique(data['flag_sp']):\n",
    "    \n",
    "    flag_text = []\n",
    "    for flag_key in flag_sp_dictionary.keys():\n",
    "        if((flag & flag_sp_dictionary[flag_key][0]) == flag_sp_dictionary[flag_key][0]):\n",
    "            flag_text.append(flag_key)\n",
    "    entries.append([flag,len(data['flag_sp'][data['flag_sp']==flag]),\", \".join(flag_text)])\n",
    "entries = np.array(entries)\n",
    "\n",
    "a = Table()\n",
    "a['flag_sp'] = entries[:,0]\n",
    "a['nr_spectra'] = entries[:,1]\n",
    "a['flag_sp_keys'] = entries[:,2]\n",
    "for s in flag_sp_dictionary.keys():\n",
    "    print(flag_sp_dictionary[s][0],flag_sp_dictionary[s][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c281f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bad = data[(data['flag_sp'] >= 64) & (data['flag_sp'] < np.max(data['flag_sp'])) & (data['teff'] > 6500)]\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "grids = Table.read('../spectrum_grids/galah_dr4_model_trainingset_gridpoints.fits')\n",
    "grid_index_tree = cKDTree(np.c_[grids['teff_subgrid'],grids['logg_subgrid'],grids['fe_h_subgrid']])\n",
    "\n",
    "model_needed = []\n",
    "for i in data[(data['flag_sp'] >= 64) & (data['flag_sp'] < np.max(data['flag_sp']))]:\n",
    "    model_needed.append(grid_index_tree.query([i['teff'],i['logg'],i['fe_h']],k=1)[1])\n",
    "model_needed = np.array(model_needed)\n",
    "models_needed = Table()\n",
    "models_needed['models'], models_needed['counts'] = np.unique(model_needed,return_counts=True)\n",
    "models_needed.sort(keys = 'counts', reverse=True)\n",
    "models_needed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27f470",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number = Table()\n",
    "number['closest_model'],number['count'] = np.unique(data['closest_model'][(data['flag_sp_fit'] == 1)],return_counts=True)\n",
    "number.sort(keys='count',reverse=True)\n",
    "number[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-length",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# date = [str(x)[:6] for x in data['sobject_id'][(data['flag_sp_fit'] == 1) & (data['closest_model']=='5000_2.00_-0.75')]]\n",
    "data['sobject_id'][(data['flag_sp_fit'] == 1) & (data['closest_model']=='5000_2.00_-0.75')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['n_fe'] > 1) & (data['flag_n_fe'] == 0) & (data['flag_sp'] == 0)][['sobject_id','model_name','closest_model','flag_sp','c_fe','flag_c_fe','n_fe','flag_n_fe','o_fe','flag_o_fe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ab15d",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot HRD & abundances \n",
    "\n",
    "flag_sp_0 = data['flag_sp'] == 0\n",
    "flag_sp_above0_but_results = (data['flag_sp'] > 0) & (data['flag_sp'] < np.max(data['flag_sp']))\n",
    "flag_sp_results = data['flag_sp'] < np.max(data['flag_sp'])\n",
    "\n",
    "for label in [\n",
    "    'Li',\n",
    "    'C',\n",
    "    'N',\n",
    "    'O',\n",
    "    'Na',\n",
    "    'Mg',\n",
    "    'Al',\n",
    "    'Si',\n",
    "    'K',\n",
    "    'Ca',\n",
    "    'Sc',\n",
    "    'Ti',\n",
    "    'V',\n",
    "    'Cr',\n",
    "    'Mn',\n",
    "    'Co',\n",
    "    'Ni',\n",
    "    'Cu',\n",
    "    'Zn',\n",
    "    'Rb',\n",
    "    'Sr',\n",
    "    'Y',\n",
    "    'Zr',\n",
    "    'Mo',\n",
    "    'Ru',\n",
    "    'Ba',\n",
    "    'La',\n",
    "    'Ce',\n",
    "    'Nd',\n",
    "    'Sm',\n",
    "    'Eu'\n",
    "    ]:\n",
    "    \n",
    "    flag0 = flag_sp_0 & (data['flag_'+label.lower()+'_fe'] == 0) #& (data['fe_h'] > -1)\n",
    "    flag1 = flag_sp_0 & (data['flag_'+label.lower()+'_fe'] == 1) #& (data['fe_h'] > -1)\n",
    "    flag_rest = flag_sp_above0_but_results & (data['flag_'+label.lower()+'_fe'] <= 1)\n",
    "\n",
    "    f, gs = plt.subplots(1,3,figsize=(10,3),sharey=True)\n",
    "\n",
    "    xbins = np.linspace(-2.5,0.75,50)\n",
    "    if label == 'Li':\n",
    "        ybins = np.linspace(0,4,50)\n",
    "    elif label in ['C','N','O','Y','Ba','La','Ce','Nd']:\n",
    "        ybins = np.linspace(-1,2,50)\n",
    "    elif label in ['Mg','Si','Ti']:\n",
    "        ybins = np.linspace(-0.5,1,50)\n",
    "    else:\n",
    "        ybins = np.linspace(-1,1,50)\n",
    "    \n",
    "    if label == 'Li':\n",
    "        ydata = data[label.lower()+'_fe'] + data['fe_h'] + 1.05\n",
    "    else:\n",
    "        ydata = data[label.lower()+'_fe']\n",
    "    \n",
    "    # First panel: Detections for GALAH DR4 [Fe/H] vs. [X/Fe]\n",
    "    ax = gs[0]\n",
    "    ax.text(0.05,0.9,'a) Detections for ['+label+'/Fe]',ha='left',transform=ax.transAxes,fontsize=12,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75))\n",
    "    ax.text(0.05,0.785,str(len(data['fe_h'][flag0]))+' ('+\"{:.0f}\".format(100*len(data['fe_h'][flag0])/len(data['fe_h'][flag_sp_results]))+r'%)',ha='left',transform=ax.transAxes,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75,lw=0))\n",
    "    ax.set_xlabel('[Fe/H] (GALAH DR4)')\n",
    "    if label == 'Li':\n",
    "        ax.set_ylabel('A('+label+') (GALAH DR4)')\n",
    "    else:\n",
    "        ax.set_ylabel('['+label+'/Fe] (GALAH DR4)')\n",
    "\n",
    "    if len(data['fe_h'][flag0]) > 10:\n",
    "        corner.hist2d(\n",
    "            data['fe_h'][flag0],\n",
    "            ydata[flag0],\n",
    "            bins = (xbins,ybins),\n",
    "            range=[(xbins[0],xbins[-1]),(ybins[0],ybins[-1])],\n",
    "            ax = ax\n",
    "        )\n",
    "    ax.set_xlim(xbins[0],xbins[-1])\n",
    "    ax.set_ylim(ybins[0],ybins[-1])\n",
    "    ax.errorbar(\n",
    "        0.9*xbins[0]+0.1*xbins[-1],\n",
    "        0.9*ybins[0]+0.1*ybins[-1],\n",
    "        xerr=np.ma.median(data['e_fe_h'][flag0]),\n",
    "        yerr=np.ma.median(data['e_'+label.lower()+'_fe'][flag0]),\n",
    "        capsize=2,color='k'\n",
    "    )\n",
    "    \n",
    "    # Second panel: Upper Limits for GALAH DR4 [Fe/H] vs. [X/Fe]\n",
    "    ax = gs[1]\n",
    "    ax.text(0.05,0.9,'b) Upper limits',ha='left',transform=ax.transAxes,fontsize=12,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75))\n",
    "    ax.text(0.05,0.785,str(len(data['fe_h'][flag1]))+' ('+\"{:.0f}\".format(100*len(data['fe_h'][flag1])/len(data['fe_h'][flag_sp_results]))+r'%)',ha='left',transform=ax.transAxes,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75,lw=0))\n",
    "    ax.set_xlabel('[Fe/H] (GALAH DR4)')\n",
    "    \n",
    "    if len(data['fe_h'][flag1]) > 10:\n",
    "        corner.hist2d(\n",
    "            data['fe_h'][flag1],\n",
    "            ydata[flag1],\n",
    "            bins = (xbins,ybins),\n",
    "            range=[(xbins[0],xbins[-1]),(ybins[0],ybins[-1])],\n",
    "            ax = ax\n",
    "        )\n",
    "    ax.set_xlim(xbins[0],xbins[-1])\n",
    "    ax.set_ylim(ybins[0],ybins[-1])\n",
    "    ax.errorbar(\n",
    "        0.9*xbins[0]+0.1*xbins[-1],\n",
    "        0.9*ybins[0]+0.1*ybins[-1],\n",
    "        xerr=np.ma.median(data['e_fe_h'][flag1]),\n",
    "        yerr=np.ma.median(data['e_'+label.lower()+'_fe'][flag1]),\n",
    "        capsize=2,color='k'\n",
    "    )\n",
    "\n",
    "    # Second panel: Upper Limits for GALAH DR4 [Fe/H] vs. [X/Fe]\n",
    "    ax = gs[2]\n",
    "    ax.text(0.05,0.9,'c) Flagged Spectra',ha='left',transform=ax.transAxes,fontsize=12,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75))\n",
    "    ax.text(0.05,0.785,str(len(data['fe_h'][flag_rest]))+' ('+\"{:.0f}\".format(100*len(data['fe_h'][flag_rest])/len(data['fe_h'][flag_sp_results]))+r'%)',ha='left',transform=ax.transAxes,bbox=dict(boxstyle='round', facecolor='w', alpha=0.75,lw=0))\n",
    "    ax.set_xlabel('[Fe/H] (GALAH DR4)')\n",
    "\n",
    "    if len(data['fe_h'][flag_rest]) > 10:\n",
    "        corner.hist2d(\n",
    "            data['fe_h'][flag_rest],\n",
    "            ydata[flag_rest],\n",
    "            bins = (xbins,ybins),\n",
    "            range=[(xbins[0],xbins[-1]),(ybins[0],ybins[-1])],\n",
    "            ax = ax\n",
    "        )\n",
    "    ax.set_xlim(xbins[0],xbins[-1])\n",
    "    ax.set_ylim(ybins[0],ybins[-1])\n",
    "    ax.errorbar(\n",
    "        0.9*xbins[0]+0.1*xbins[-1],\n",
    "        0.9*ybins[0]+0.1*ybins[-1],\n",
    "        xerr=np.ma.median(data['e_fe_h'][flag_rest]),\n",
    "        yerr=np.ma.median(data['e_'+label.lower()+'_fe'][flag_rest]),\n",
    "        capsize=2,color='k'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "#     plt.savefig('figures/galah_dr4_validation_overview_'+label.lower()+'_fe_density.png',dpi=200,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_table = Table()\n",
    "flag_table['flag_sp'],flag_table['counts'] = np.unique(data['flag_sp'],return_counts=True)\n",
    "flag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(flag_table['flag_sp'],flag_table['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ce8b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot CNO\n",
    "\n",
    "f, gs = plt.subplots(1,3,sharex=True,sharey=True,figsize=(2.5*3,2.5))\n",
    "\n",
    "x_low  = -2.50\n",
    "x_high =  0.75\n",
    "y_low  = -1.00\n",
    "y_high =  1.75\n",
    "\n",
    "panels = ['a)','b)','c)','d)']\n",
    "\n",
    "for i,label in enumerate(['C','N','O']):\n",
    "#for i,label in enumerate(['C','N','CN','O']):\n",
    "    ax = gs[i]\n",
    "\n",
    "    if label in ['C','N','O']:\n",
    "        flag0 = (data['flag_'+label.lower()+'_fe'] == 0) #& (data['fe_h'] > -1)\n",
    "        flag1 = (data['flag_'+label.lower()+'_fe'] == 1) #& (data['fe_h'] > -1)\n",
    "        corner.hist2d(\n",
    "            data['fe_h'][flag0],\n",
    "            data[label.lower()+'_fe'][flag0],\n",
    "            ax = ax,bins=(np.linspace(x_low,x_high,50),np.linspace(y_low,y_high,50))\n",
    "        )\n",
    "#         ax.hist2d(\n",
    "#             data['fe_h'][flag0],\n",
    "#             data[label.lower()+'_fe'][flag0],\n",
    "#             cmin = 1, bins=50\n",
    "#             #s=1,label='Detection'\n",
    "#         )\n",
    "#         ax.scatter(\n",
    "#             data['fe_h'][flag1],\n",
    "#             data[label.lower()+'_fe'][flag1],\n",
    "#             marker='v',label='Upper limit',\n",
    "#             s=0.5\n",
    "#         )\n",
    "        \n",
    "    if label == 'CN':\n",
    "        flag0 = (data['flag_c_fe'] == 0) & (data['flag_n_fe'] == 0) #& (data['fe_h'] > -1)\n",
    "        flag1 = ((data['flag_c_fe'] == 1) | (data['flag_n_fe'] == 1)) #& (data['fe_h'] > -1)\n",
    "        corner.hist2d(\n",
    "            data['fe_h'][flag0],\n",
    "            data['c_fe'][flag0]-data['n_fe'][flag0],\n",
    "            ax = ax,bins=(np.linspace(x_low,x_high,50),np.linspace(y_low,y_high,50))\n",
    "        )\n",
    "        #         ax.hist2d(\n",
    "#             data['fe_h'][flag0],\n",
    "#             data['c_fe'][flag0]-data['n_fe'][flag0],\n",
    "#             cmin = 1, bins=50\n",
    "# #             s=1,label='Detection'\n",
    "#         )\n",
    "#         ax.scatter(\n",
    "#             data['fe_h'][flag1],\n",
    "#             data['c_fe'][flag1]-data['n_fe'][flag1],\n",
    "#             marker='v',label='Upper limit',\n",
    "#             s=0.5\n",
    "#         )\n",
    "    \n",
    "    ax.set_xlim(x_low,x_high)\n",
    "    ax.set_ylim(y_low,y_high)\n",
    "    \n",
    "#     if i==0:\n",
    "#         ax.legend()\n",
    "    ax.set_xlabel('[Fe/H]')\n",
    "    if label in ['C','N','O']:\n",
    "        ax.set_ylabel('['+label+'/Fe]')\n",
    "    if label == 'CN':\n",
    "        ax.set_ylabel('[C/N]')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/overview_CNO_incl_upper_limits.png',dpi=200,bbox_inches='tight')\n",
    "plt.savefig('figures/overview_CNO.png',dpi=200,bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [X/Fe] = [X/H] - [M/H]\n",
    "# # [X/H] + (A_X - 12) = log(N_X/N_H) \n",
    "# # [X/Fe] = log(N_X / N_H) - log(N_X / N_H)_Sun - [Fe/H]\n",
    "# # [C + N / Fe] = log((N_C + N_N) / N_H) - log((N_C_Sun + (N_N_Sun / N_H)_Sun - [Fe/H]\n",
    "# # [C + N / Fe] = log(N_C/N_H + N_N/N_H) - log(N_C_Sun/H_Sun + N_N_Sun/N_H_Sun) - [Fe/H]\n",
    "\n",
    "# A_N_Sun = 7.78+0.15\n",
    "# A_C_Sun = 8.39+0.037\n",
    "\n",
    "# A_C = data['c_fe'] + data['fe_h'] + (A_C_Sun)\n",
    "# A_N = data['n_fe'] + data['fe_h'] + (A_N_Sun)\n",
    "\n",
    "# N_C_N_H = 10**(A_C + 12)\n",
    "# N_N_N_H = 10**(A_N + 12)\n",
    "# N_C_N_H_sun = 10**(A_C_Sun + 12)\n",
    "# N_N_N_H_sun = 10**(A_N_Sun + 12)\n",
    "\n",
    "# data['cn_fe'] = np.log10(N_C_N_H + N_N_N_H) - np.log10(N_C_N_H_sun + N_N_N_H_sun) - data['fe_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aea44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cn_masses(fe_h, c_fe, n_fe, cn_fe):\n",
    "#     return(\n",
    "#         1.08 - 0.18 * fe_h + 4.30 * c_fe +1.43 * n_fe - 7.55 * cn_fe\n",
    "#         - 1.05 * (fe_h)**2 - 1.12 * (fe_h * c_fe) - 0.67 * (fe_h * n_fe) - 1.30 * (fe_h * cn_fe)\n",
    "#         - 49.92 * (c_fe)**2 - 41.04 * (c_fe * n_fe) + 139.92 * (c_fe * cn_fe)\n",
    "#         - 0.63 * (n_fe)**2 + 47.33 * (n_fe * cn_fe)\n",
    "#         - 86.62 * (cn_fe)**2\n",
    "#     )\n",
    "# data['mass'] = cn_masses(fe_h=data['fe_h'], c_fe=data['c_fe'], n_fe=data['n_fe'], cn_fe=(data['cn_fe']-0.1)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a831327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cn_ages(fe_h, c_fe, n_fe, cn_fe, teff, logg):\n",
    "#     return(\n",
    "#         -54.35 + 6.53*fe_h -19.02 *c_fe -12.18*n_fe +37.22*cn_fe +59.58*teff +16.14*logg\n",
    "#         +0.74*fe_h*fe_h +4.04*fe_h*c_fe +0.76*fe_h*n_fe -4.94*fe_h*cn_fe -1.46*fe_h*teff -1.56*fe_h*logg\n",
    "#         +26.90*c_fe*c_fe +13.33*c_fe*n_fe -77.84*c_fe*cn_fe +48.29*c_fe*teff -13.12*c_fe*logg\n",
    "#         -1.04*n_fe*n_fe -17.60*n_fe*cn_fe +13.99*n_fe*teff -1.77*n_fe*logg\n",
    "#         +51.24*cn_fe*cn_fe -65.67*cn_fe*teff +14.24*cn_fe*logg\n",
    "#         +15.54*teff*teff -34.68*teff*logg\n",
    "#         +4.17*logg*logg\n",
    "#     )\n",
    "# data['age'] = 10**(cn_ages(fe_h=data['fe_h'], c_fe=data['c_fe'], n_fe=data['n_fe'], cn_fe=data['cn_fe'], teff=data['teff']/4000., logg=data['logg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
