{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GALAH DR4 Stellar Label Optimisation with spectroscopic and non-spectroscopic information\n",
    "\n",
    "This script is used to find the optimal set of stellar labels for GALAH spectra in combination with non-spectroscopic information\n",
    "\n",
    "The code is maintained at\n",
    "https://github.com/svenbuder/GALAH_DR4\n",
    "and described at\n",
    "https://github.com/svenbuder/galah_dr4_paper\n",
    "\n",
    "Author(s): Sven Buder (ANU, ASTRO 3D)\n",
    "\n",
    "History:  \n",
    "220811: Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TAP+ (v1.2.1) - Connection:\n",
      "\tHost: gea.esac.esa.int\n",
      "\tUse HTTPS: True\n",
      "\tPort: 443\n",
      "\tSSL Port: 443\n",
      "Created TAP+ (v1.2.1) - Connection:\n",
      "\tHost: geadata.esac.esa.int\n",
      "\tUse HTTPS: True\n",
      "\tPort: 443\n",
      "\tSSL Port: 443\n"
     ]
    }
   ],
   "source": [
    "# Preamble \n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from galah_dr4_functions.galah_dr4_functions import correct_gband\n",
    "from galah_dr4_functions.galah_dr4_functions import get_spectroscopic_information_for_sobject_id\n",
    "from galah_dr4_functions.galah_dr4_functions import combine_spectroscopic_information\n",
    "from galah_dr4_functions.galah_dr4_functions import get_astrometric_and_photometric_information\n",
    "from galah_dr4_functions.galah_dr4_functions import get_asteroseismic_information\n",
    "from galah_dr4_functions.galah_dr4_functions import get_interferometric_information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Star:\n",
    "    def __init__(self, sobject_id, combine_spectra, spectroscopy_only, debug_mode):\n",
    "        \n",
    "        print('Initialising... \\n')\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if isinstance(sobject_id, int):\n",
    "            if len(str(sobject_id)) == 15:\n",
    "                self.sobject_id = int(sobject_id)\n",
    "                print('sobject_id = '+str(sobject_id))\n",
    "            else:\n",
    "                raise ValueError('sobject_id has to be 15-digit integer')\n",
    "        else:\n",
    "            raise ValueError('sobject_id has to be 15-digit integer')\n",
    "\n",
    "        if isinstance(combine_spectra, bool):\n",
    "            self.combine_spectra = bool(combine_spectra)\n",
    "            print('combine_spectra = '+str(bool(combine_spectra)))\n",
    "        else:\n",
    "            raise ValueError('combine_spectra has to be True/False bool')\n",
    "        \n",
    "        if isinstance(spectroscopic_fit_only, bool):\n",
    "            self.spectroscopic_fit_only = bool(spectroscopic_fit_only)\n",
    "            print('spectroscopic_fit_only = '+str(bool(spectroscopic_fit_only)))\n",
    "        else:\n",
    "            raise ValueError('spectroscopic_fit_only has to be True/False bool')\n",
    "            \n",
    "        if isinstance(debug_mode, bool):\n",
    "            self.debug_mode = bool(debug_mode)\n",
    "            print('debug_mode = '+str(bool(debug_mode)))\n",
    "        else:\n",
    "            raise ValueError('debug_mode has to be True/False bool')\n",
    "            \n",
    "        self.tmass_id = 'hhmmssss+hhmmsss'\n",
    "        self.gaiadr3_source_id = int(-1)\n",
    "            \n",
    "        print('Done after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "\n",
    "    def load_spectroscopic_information(self):\n",
    "        \"\"\"\n",
    "        This function reads the reduction products (dr6.0.fits).\n",
    "        It identifies all spectra with the same tmass_id as the chosen star\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Loading spectroscopic information')\n",
    "        \n",
    "        print('-> Loading information from dr6.0.fits')\n",
    "        # First we have to read in the reduction products\n",
    "        dr60 = Table.read('../observations/dr6.0.fits')\n",
    "        dr60.sort(keys='sobject_id')\n",
    "        match_in_dr60 = np.where(self.sobject_id == dr60['sobject_id'])[0]\n",
    "        if len(match_in_dr60) > 0:\n",
    "            if len(match_in_dr60) > 1:\n",
    "                print('   Warning: Multiple matches in dr6.0.fits found. Using first entry')\n",
    "            else:\n",
    "                match_in_dr60 = match_in_dr60[0]\n",
    "                self.raj2000 = dr60['ra'][match_in_dr60]\n",
    "                self.dej2000 = dr60['dec'][match_in_dr60]\n",
    "                # pivot\n",
    "                # plate\n",
    "                self.tmass_id = str(dr60['2mass'][match_in_dr60])\n",
    "                self.gaiadr3_source_id = int(dr60['gaia_id'][match_in_dr60])\n",
    "                self.snr = dr60['snr'][match_in_dr60]\n",
    "                self.v_bary_eff = dr60['v_bary_eff'][match_in_dr60]\n",
    "                self.ebv = dr60['e_b-v'][match_in_dr60]\n",
    "                # wav_rms\n",
    "                # wav_n_lines\n",
    "                self.rv = dr60['rv'][match_in_dr60]\n",
    "                self.e_rv = dr60['e_rv'][match_in_dr60]\n",
    "                self.rv_com = dr60['rv_com'][match_in_dr60]\n",
    "                self.e_rv_com = dr60['e_rv_com'][match_in_dr60]\n",
    "                \n",
    "                self.red_teff = dr60['teff_r'][match_in_dr60]\n",
    "                self.red_logg = dr60['logg_r'][match_in_dr60]\n",
    "                self.red_fe_h = dr60['fe_h_r'][match_in_dr60]\n",
    "                self.red_vmic = dr60['vmic_r'][match_in_dr60]\n",
    "                self.red_vsini = dr60['vbroad_r'][match_in_dr60]\n",
    "                self.red_flag = dr60['reduction_flags'][match_in_dr60]\n",
    "        else:\n",
    "            raise IndexError('No match in dr6.0.fits found for '+str(self.sobject_id))\n",
    "        \n",
    "        print('-> combine_spectra == '+str(self.combine_spectra))\n",
    "        if combine_spectra:\n",
    "            find_all_spectra_with_same_tmass_id = np.where(dr60['2mass'] == self.tmass_id)[0]\n",
    "            if len(find_all_spectra_with_same_tmass_id) > 1:\n",
    "                if dr60['sobject_id'][find_all_spectra_with_same_tmass_id[0]] != self.sobject_id:\n",
    "                    raise ValueError('This sobject_id is not the first observation of this star. Returning.')\n",
    "                else:\n",
    "                    print('-> Found '+str(find_all_spectra_with_same_tmass_id)+' spectra, combining now.')\n",
    "            else:\n",
    "                print('-> Found only one spectrum.')\n",
    "            self.spectra_to_combine = dr60['sobject_id'][find_all_spectra_with_same_tmass_id]\n",
    "        else:\n",
    "            self.spectra_to_combine = [self.sobject_id]\n",
    "            \n",
    "        for sobject_id_index, sobject_id_to_combine in enumerate(self.spectra_to_combine):\n",
    "            print('-> Reading in '+str(sobject_id_to_combine))\n",
    "            \n",
    "            if sobject_id_index == 0:\n",
    "                self.spectrum = get_spectroscopic_information_for_sobject_id(sobject_id_to_combine, dr60)\n",
    "            else:\n",
    "                raise Exception('ToDo: Code the combining of multiple spectra')\n",
    "                \n",
    "        print('Done loading spectroscopic information after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def load_photoastrometric_information(self):\n",
    "        print('Loading Photo- and Astrometric Information')\n",
    "\n",
    "        self.non_spec_info = get_astrometric_and_photometric_information(self.gaiadr3_source_id, self.tmass_id)\n",
    "        \n",
    "        print('Done Loading Photo- and Astrometric Information after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "\n",
    "    def load_asteroseismic_information(self):\n",
    "        print('Loading Asteroseismic Information')\n",
    "        \n",
    "        try:\n",
    "            astero_info = get_asteroseismic_information(self.gaiadr3_source_id)\n",
    "            for keys in astero_info.keys():\n",
    "                self.non_spec_info[key] = astero_info[key]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print('Done Loading Asteroseismic Information after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def load_interferometric_information(self):\n",
    "        print('Loading Interferometric Information')\n",
    "\n",
    "        try:\n",
    "            interfero_info = get_interferometric_information(self.tmass_id)\n",
    "            for keys in interfero_info.keys():\n",
    "                self.non_spec_info[key] = interfero_info[key]\n",
    "        except:\n",
    "            pass        \n",
    "        \n",
    "        print('Done Loading Interferometric Information after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "\n",
    "    def select_initial_parameters(self):\n",
    "        print('Selecting Initial Parameters')\n",
    "        \n",
    "        self.init_params = dict()\n",
    "        self.init_params['init_teff'] = 5.0\n",
    "        self.init_params['init_logg'] = 3.5\n",
    "        self.init_params['init_fe_h'] = 0.0\n",
    "        self.init_params['init_vmic'] = 1.5\n",
    "        self.init_params['init_vsini'] = 5.0\n",
    "                \n",
    "        galah_dr3 = Table.read('../auxiliary_information/GALAH_DR3_main_allspec_v2_lite.fits')\n",
    "        galah_dr3_match = np.where(galah_dr3['sobject_id'] == self.sobject_id)[0]\n",
    "        if len(galah_dr3_match) > 0:\n",
    "            galah_dr3_entry = galah_dr3[galah_dr3_match[0]]\n",
    "            print('   Found match in GALAH+ DR3 with flag_sp = '+str(galah_dr3_entry['flag_sp']))\n",
    "\n",
    "            if galah_dr3_entry['flag_sp'] == 0:\n",
    "                self.init_params['init_teff'] = 0.001*galah_dr3_entry['teff']\n",
    "\n",
    "                for label in ['logg','vmic','vsini']:\n",
    "                    label2 = label\n",
    "                    if label=='vsini':\n",
    "                        label2 = 'vbroad'\n",
    "                    self.init_params['init_'+label] = galah_dr3_entry[label2]\n",
    "\n",
    "                if galah_dr3_entry['flag_fe_h'] == 0:\n",
    "                    self.init_params['init_fe_h'] = galah_dr3_entry['fe_h']\n",
    "\n",
    "        print(self.init_params)\n",
    "        \n",
    "        print('           TEFF, LOGG, FE_H, VMIC, VSINI')\n",
    "        print('Default  ',\"{:.3f}\".format(5.0), \"{:.3f}\".format(3.5), \"{:.3f}\".format(0.0), \"{:.3f}\".format(1.5), \"{:.3f}\".format(5.0))\n",
    "        print('dr6.0    ',\"{:.3f}\".format(0.001*self.red_teff), \"{:.3f}\".format(3.5), \"{:.3f}\".format(0.0), \"{:.3f}\".format(1.5), \"{:.3f}\".format(5.0))\n",
    "        print('GALAH_DR3',\"{:.3f}\".format(0.001*galah_dr3_entry['teff']), \"{:.3f}\".format(galah_dr3_entry['logg']), \"{:.3f}\".format(galah_dr3_entry['fe_h']), \"{:.3f}\".format(galah_dr3_entry['vmic']), \"{:.3f}\".format(galah_dr3_entry['vbroad']))\n",
    "\n",
    "\n",
    "                            \n",
    "#         irfm_teff_array = calculate_irfm_teffs(\n",
    "#             self.logg,\n",
    "#             self.feh,\n",
    "#             self.non_spec_info['phot_g_mean_mag'],\n",
    "#             self.non_spec_info['phot_bp_mean_mag'],\n",
    "#             self.non_spec_info['phot_rp_mean_mag'],\n",
    "#             self.non_spec_info['j_mag'],\n",
    "#             self.non_spec_info['h_mag'],\n",
    "#             self.non_spec_info['ks_mag'],\n",
    "#             self.ebv\n",
    "#         )\n",
    "        \n",
    "#         p16,p50,p84 = np.nanpercentile(irfm_teff_array,q=[16,50,84])\n",
    "\n",
    "#         initial_irfm_teff = p50.clip(min=3050,max=7950)\n",
    "\n",
    "#         bcs = bcstar(\n",
    "#             sid = self.sobject_id,\n",
    "#             teff = self.teff,\n",
    "#             logg = self.logg,\n",
    "#             feh = self.fe_h,\n",
    "#             ebv = ebv.clip(min=0.0,max=0.71),\n",
    "#             filters='G3_BP3_RP3_J2M_H2M_K2M',\n",
    "#         )\n",
    "\n",
    "        print('Done Selecting Initial Parameters after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def fit_spectroscopic_data(self):\n",
    "        print('Fitting Spectroscopic Data')\n",
    "\n",
    "        print('Done after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def fit_astrophysical_data(self):\n",
    "        print('Fitting Astrophysical Data')\n",
    "\n",
    "        print('Done after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def fit_spectroscopic_data_with_astrophysical_priors(self):\n",
    "        print('Fitting Spectroscopic Data with Astrophysical Priors')\n",
    "\n",
    "        print('Done after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')\n",
    "        \n",
    "    def save_results(self):\n",
    "        print('Saving Results')\n",
    "\n",
    "        print('Done after '+\"{:.1f}\".format(time.time()-self.start_time)+' sec \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising... \n",
      "\n",
      "sobject_id = 140805001501040\n",
      "combine_spectra = True\n",
      "spectroscopic_fit_only = True\n",
      "debug_mode = True\n",
      "Done after 0.0 sec \n",
      "\n",
      "Loading spectroscopic information\n",
      "-> Loading information from dr6.0.fits\n",
      "-> combine_spectra == True\n",
      "-> Found only one spectrum.\n",
      "-> Reading in 140805001501040\n",
      "   Warning: Count uncertainties <= 0 for 6 pixels in CCD1 of 140805001501040, setting to 0.1 (SNR~10)\n",
      "Done loading spectroscopic information after 3.1 sec \n",
      "\n",
      "Selecting Initial Parameters\n",
      "   Found match in GALAH+ DR3 with flag_sp = 0\n",
      "{'init_teff': 4.42793603515625, 'init_logg': 2.4850726, 'init_fe_h': -0.01624918, 'init_vmic': 1.452522, 'init_vsini': 5.828994}\n",
      "           TEFF, LOGG, FE_H, VMIC, VSINI\n",
      "Default   5.000 3.500 0.000 1.500 5.000\n",
      "dr6.0     4.516 3.500 0.000 1.500 5.000\n",
      "GALAH_DR3 4.428 2.485 -0.016 1.453 5.829\n",
      "Done Selecting Initial Parameters after 3.7 sec \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sobject_id = 140805001501040\n",
    "combine_spectra = True\n",
    "spectroscopic_fit_only = True\n",
    "debug_mode = True\n",
    "\n",
    "star = Star(sobject_id, combine_spectra, spectroscopic_fit_only, debug_mode)\n",
    "\n",
    "# try:\n",
    "star.load_spectroscopic_information()\n",
    "# star.load_photoastrometric_information()\n",
    "# star.load_asteroseismic_information()\n",
    "# star.load_interferometric_information()\n",
    "\n",
    "star.select_initial_parameters()\n",
    "\n",
    "# star.fit_spectroscopic_data()\n",
    "\n",
    "# if not star.spectroscopic_fit_only:\n",
    "#     star.fit_astrophysical_data()\n",
    "#     star.fit_spectroscopic_data_with_astrophysical_priors()\n",
    "# else:\n",
    "#     print('spectroscopic_fit_only == True')\n",
    "#     print('Skipping astrophysical fit and spectroscopic fit with astrophysical priors \\n')\n",
    "# # except:\n",
    "# #     print('Could not finish analysis. Saving progress.')\n",
    "    \n",
    "# star.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in isochrone grid and trained nearest neighbor search machinery 'kdtree'\n",
    "parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_6p19_0p01_10p17_mh_m2p75_0p25_m0p75_mh_m0p60_0p10_0p70_GaiaEDR3_2MASS.fits')\n",
    "# parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_6p19_0p01_10p17_mh_m2p75_0p25_1p00_mh_m0p75_0p05_0p75_GaiaEDR3_2MASS.fits')\n",
    "file = open('../auxiliary_information/parsec_isochrones/isochrone_kdtree_Teff_logg_M_H.pickle','rb')\n",
    "parsec_kdtree = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_irfm_teffs(\n",
    "    initial_logg[index],\n",
    "    initial_fe_h[index],\n",
    "    g[index],\n",
    "    bp[index],\n",
    "    rp[index],\n",
    "    j[index],\n",
    "    h[index],\n",
    "    ks[index],\n",
    "    ebv[index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kd_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_logg_from_photoastrometru(star):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_initial_parameters(star):\n",
    "    \"\"\"\n",
    "    Based on the information at hand, we will estimate the following starting parameters:\n",
    "    teff\n",
    "    logg\n",
    "    fe_h\n",
    "    vmic\n",
    "    vsini\n",
    "    \"\"\"\n",
    "    \n",
    "    galah_dr3 = Table.read('../auxiliary_information/GALAH_DR3_main_allspec_v2_lite.fits')\n",
    "    galah_dr3_match = np.where(galah_dr3['sobject_id'] == star['sobject_id'])[0]\n",
    "    if len(galah_dr3_match) > 0:\n",
    "        \n",
    "        galah_dr3_entry = galah_dr3[galah_dr3_match[0]]\n",
    "        \n",
    "        print('Found match in GALAH+ DR3 with flag_sp = '+str(galah_dr3_entry['flag_sp']))\n",
    "        \n",
    "        if galah_dr3_entry['flag_sp'] == 0:\n",
    "            star['init_teff'] = 0.001*galah_dr3_entry['teff']\n",
    "            \n",
    "            for label in ['logg','vmic','vsini']:\n",
    "                label2 = label\n",
    "                if label=='vsini':\n",
    "                    label2 = 'vbroad'\n",
    "                star['init_'+label] = galah_dr3_entry[label2]\n",
    "\n",
    "            if galah_dr3_entry['flag_fe_h'] == 0:\n",
    "                star['init_fe_h'] = galah_dr3_entry['fe_h']\n",
    "\n",
    "            for element in ['Li','C','O','Na','Mg','Al','Si','K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','La','Ce','Nd','Sm','Eu']:\n",
    "                if galah_dr3_entry['flag_'+element+'_fe'] == 0:\n",
    "                    star['init_'+element.lower()+'_fe'] = galah_dr3_entry[element+'_fe']\n",
    "\n",
    "    \n",
    "                    \n",
    "    return(star)\n",
    "    \n",
    "star = estimate_initial_parameters(star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_reduction_products(sobject_id, neglect_ir_beginning=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function collects the spectrum and other reduction products.\n",
    "    \n",
    "    INPUT:\n",
    "    sobject_id: identifier for spectra\n",
    "    neglect_ir_beginning: Cut away CCD4 information below 7700Ã…?\n",
    "\n",
    "    OUTPUT:\n",
    "    spectrum - dictionary with the following keywords:\n",
    "    - sobject_id: identifier for GALAH\n",
    "    - tmass_id: identifier for 2MASS\n",
    "    - gaia_edr3_source_id: identifier for GaiaEDR3\n",
    "    - init_teff: Teff value suggested by reduction\n",
    "    - init_logg: logg value suggested by reduction\n",
    "    - init_fe_h: [Fe/H] value suggested by reduction\n",
    "    - init_x_fe: [X/Fe] for alpha-process elements C, O, Mg, Si, Ca, Ti\n",
    "    - init_vmic: vmic value suggested by reduction\n",
    "    - init_vsini: vsini value suggested by reduction\n",
    "    - init_vrad: vrad value suggested by reduction\n",
    "\n",
    "    \"\"\"\n",
    "    spectrum = dict()\n",
    "\n",
    "    spectrum['sobject_id'] = sobject_id\n",
    "\n",
    "    fits_file = fits.open(spectra_directory+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+'1.fits')\n",
    "\n",
    "    spectrum['galah_id'] = fits_file[0].header['GALAH_ID']\n",
    "    spectrum['tmass_id'] = fits_file[0].header['2MASS_ID']\n",
    "    spectrum['gaia_edr3_source_id'] = int(fits_file[0].header['GAIA_ID'])\n",
    "\n",
    "    if fits_file[0].header['SLITMASK'] in ['IN','IN      ']:\n",
    "        spectrum['resolution'] = 'high-res'\n",
    "        print('Warning: Spectrum is high-resolution!')\n",
    "    else:\n",
    "        spectrum['resolution'] = 'low-res'\n",
    "    \n",
    "    if fits_file[0].header['PAR_OK']==1:\n",
    "        \n",
    "        # TEFF\n",
    "        spectrum['init_teff'] = 0.001*fits_file[0].header['TEFF_R']\n",
    "        if spectrum['init_teff'] < 3:\n",
    "            raise ValueError('TEFF < 3000')\n",
    "        if spectrum['init_teff'] > 8:\n",
    "            raise ValueError('TEFF > 8000')\n",
    "        \n",
    "        # LOGG\n",
    "        spectrum['init_logg'] = fits_file[0].header['LOGG_R']\n",
    "        if spectrum['init_logg'] < -0.5:\n",
    "            raise ValueError('LOGG < -0.5')\n",
    "        if spectrum['init_logg'] > 5.5:\n",
    "            raise ValueError('LOGG > 5.5')\n",
    "            \n",
    "        # FE_H\n",
    "        spectrum['init_fe_h'] = fits_file[0].header['FE_H_R']\n",
    "        if spectrum['init_fe_h'] < -4.0:\n",
    "            raise ValueError('LOGG < -4.0')\n",
    "        if spectrum['init_fe_h'] > 1.0:\n",
    "            raise ValueError('LOGG > 1.0')\n",
    "        \n",
    "        for element in ['Li','C','N','O','Na','Mg','Al','Si','K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','La','Ce','Nd','Sm','Eu']:\n",
    "            spectrum['init_'+element.lower()+'_fe'] = 0.0\n",
    "\n",
    "        # ALPHA_FE -> C, O, Si, Mg, Ca, Ti\n",
    "        for each_alpha in ['c','o','mg','si','ca','ti']:\n",
    "            spectrum['init_'+each_alpha+'_fe'] = fits_file[0].header['A_FE_R']\n",
    "            if spectrum['init_'+each_alpha+'_fe'] < -1.0:\n",
    "                raise ValueError('['+each_alpha+'/Fe] < -1.0')\n",
    "            if spectrum['init_'+each_alpha+'_fe'] > 1.0:\n",
    "                raise ValueError('['+each_alpha+'/Fe] > 1.0')\n",
    "            \n",
    "        # VMIC\n",
    "        spectrum['init_vmic'] = fits_file[0].header['VMIC_R']\n",
    "        \n",
    "        # VBROAD\n",
    "        spectrum['init_vsini'] = fits_file[0].header['VBROAD_R']\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Reduction parameters not trustworthy!')\n",
    "\n",
    "    if fits_file[0].header['RVCOM_OK']==1:\n",
    "        spectrum['init_vrad'] = fits_file[0].header['RVCOM']\n",
    "    else:\n",
    "        raise ValueError('RV pipeline did not converge!')\n",
    "\n",
    "    if fits_file[0].header['WAV_OK']==0:\n",
    "        raise ValueError('Wavelength solution not ok!')\n",
    "\n",
    "    if fits_file[0].header['CROSS_OK']==0:\n",
    "        raise ValueError('Cross-talk not calculated reliably!')\n",
    "\n",
    "    for ccd in [1,2,3,4]:\n",
    "\n",
    "        if ccd != 1:\n",
    "            fits_file = fits.open(spectra_directory+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+str(ccd)+'.fits')\n",
    "\n",
    "        spectrum['crval_ccd'+str(ccd)] = fits_file[0].header['CRVAL1']\n",
    "        spectrum['cdelt_ccd'+str(ccd)] = fits_file[0].header['CDELT1']\n",
    "\n",
    "        spectrum['counts_ccd'+str(ccd)]   = fits_file[0].data\n",
    "        spectrum['counts_unc_ccd'+str(ccd)] = fits_file[0].data * fits_file[2].data\n",
    "\n",
    "        spectrum['sky_ccd'+str(ccd)]   = fits_file[3].data\n",
    "        spectrum['telluric_ccd'+str(ccd)]   = fits_file[4].data\n",
    "\n",
    "        spectrum['lsf_b_ccd'+str(ccd)] = fits_file[0].header['B']\n",
    "        spectrum['lsf_ccd'+str(ccd)]   = fits_file[7].data\n",
    "        \n",
    "        fits_file.close()\n",
    "\n",
    "        if (ccd == 4) & neglect_ir_beginning:\n",
    "            wave_ccd4 = spectrum['crval_ccd4'] + spectrum['cdelt_ccd4'] * np.arange(len(spectrum['counts_ccd4']))\n",
    "            bad_ir = wave_ccd4 > 7680\n",
    "            spectrum['crval_ccd4'] = wave_ccd4[bad_ir][0]\n",
    "            spectrum['counts_ccd4'] = spectrum['counts_ccd4'][bad_ir]\n",
    "            spectrum['counts_unc_ccd4'] = spectrum['counts_unc_ccd4'][bad_ir]\n",
    "            spectrum['lsf_ccd4'] = spectrum['lsf_ccd4'][bad_ir]\n",
    "\n",
    "    return(spectrum)\n",
    "\n",
    "spectrum = get_reduction_products(sobject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def exchange_with_reliable_galah_dr3_values(spectrum):\n",
    "\n",
    "    galah_dr3 = Table.read('../auxiliary_information/GALAH_DR3_main_allspec_v2_lite.fits')\n",
    "    galah_dr3_match = np.where(galah_dr3['sobject_id'] == spectrum['sobject_id'])[0]\n",
    "    if len(galah_dr3_match) > 0:\n",
    "        print('Found match in GALAH+ DR3')\n",
    "        galah_dr3_entry = galah_dr3[galah_dr3_match[0]]\n",
    "\n",
    "        if galah_dr3_entry['flag_sp'] == 0:\n",
    "            print('Replacing initial value for Teff, logg, vmic, vsini with GALAH+ DR3 ones.')\n",
    "            print('Teff IRAF 6.0: ',\"{:.2f}\".format(1000.*spectrum['init_teff']), 'GALAH+ DR3: ',\"{:.2f}\".format(galah_dr3_entry['teff']))\n",
    "            spectrum['init_teff'] = 0.001*galah_dr3_entry['teff']\n",
    "            \n",
    "            for label in ['logg','vmic','vsini']:\n",
    "                label2 = label\n",
    "                if label=='vsini':\n",
    "                    label2 = 'vbroad'\n",
    "                print(label+' IRAF 6.0: ',\"{:.2f}\".format(spectrum['init_'+label]), 'GALAH+ DR3: ',\"{:.2f}\".format(galah_dr3_entry[label2]))\n",
    "                spectrum['init_'+label] = galah_dr3_entry[label2]\n",
    "\n",
    "            if galah_dr3_entry['flag_fe_h'] == 0:\n",
    "                print('Replacing initial value for [Fe/H] with GALAH+ DR3 ones.')\n",
    "                print('[Fe/H] IRAF 6.0: ',\"{:.2f}\".format(spectrum['init_fe_h']), 'GALAH+ DR3: ',\"{:.2f}\".format(galah_dr3_entry['fe_h']))\n",
    "                spectrum['init_fe_h'] = galah_dr3_entry['fe_h']\n",
    "\n",
    "            for element in ['Li','C','O','Na','Mg','Al','Si','K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','La','Ce','Nd','Sm','Eu']:\n",
    "                if galah_dr3_entry['flag_'+element+'_fe'] == 0:\n",
    "                    print('['+element+'/Fe] initial value from GALAH+ DR3: '+\"{:.2f}\".format(galah_dr3_entry[element+'_fe']))\n",
    "\n",
    "                    spectrum['init_'+element.lower()+'_fe'] = galah_dr3_entry[element+'_fe']\n",
    "\n",
    "exchange_with_reliable_galah_dr3_values(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_non_spec_info(spectrum, use_photoastrometry, use_asteroseismology):\n",
    "    \"\"\"\n",
    "    Get the non-spectroscopic information from the auxiliary information.\n",
    "    X-matches to 2MASS, Gaia eDR3, and K2\n",
    "    \n",
    "    INPUT:\n",
    "    spectrum dictionary with sobject_id and gaia_edr_source_id keywords\n",
    "    use_photoastrometry : True/False statement to use Gaia+2MASS+WISE photometry (g_bp, ks_m, W2, etc.) + Gaia astrometry (parallax)\n",
    "    use_asteroseismology : True/False statement to use asteroseismology (nu_max and delta_nu)\n",
    "    \n",
    "    OUTPUT:\n",
    "    dictionary with the important information\n",
    "    from Gaia eDR3, 2MASS, WISE, and K2 (if available)\n",
    "    \"\"\"\n",
    "\n",
    "    non_spec_info = dict()\n",
    "    non_spec_info['sobject_id'] = spectrum['sobject_id']\n",
    "    non_spec_info['tmass_id'] = spectrum['tmass_id']\n",
    "    non_spec_info['gaia_edr3_source_id'] = spectrum['gaia_edr3_source_id']\n",
    "\n",
    "    print('Matching with Gaia eDR3 and 2MASS via TAP query')\n",
    "    if use_photoastrometry:\n",
    "        \n",
    "        # Query to get Gaia eDR3 and 2MASS information.\n",
    "        # At the moment, we do not match with other surveys, but we easily could, as indicated below\n",
    "        \n",
    "        adql = [\n",
    "            'SELECT TOP 2',\n",
    "            'gaia_edr3.ruwe, gaia_edr3.astrometric_params_solved, gaia_edr3.nu_eff_used_in_astrometry, gaia_edr3.pseudocolour, gaia_edr3.ecl_lat,',\n",
    "            'gaia_edr3.bp_rp, gaia_edr3.phot_g_mean_mag, gaia_edr3.phot_bp_mean_mag, gaia_edr3.phot_rp_mean_mag,',\n",
    "            'gaia_edr3.phot_g_mean_flux, gaia_edr3.phot_bp_mean_flux, gaia_edr3.phot_rp_mean_flux,',\n",
    "            'gaia_edr3.phot_g_mean_flux_error, gaia_edr3.phot_bp_mean_flux_error, gaia_edr3.phot_rp_mean_flux_error,',\n",
    "            'gaia_edr3.parallax, gaia_edr3.parallax_error,',\n",
    "            'gaia_edr3.dr2_radial_velocity, gaia_edr3.dr2_radial_velocity_error,',\n",
    "            'tmass.designation as tmass_id,',\n",
    "            'tmass.ph_qual as ph_qual_tmass,',\n",
    "            'tmass.j_m, tmass.j_msigcom,',\n",
    "            'tmass.h_m, tmass.h_msigcom,',\n",
    "            'tmass.ks_m, tmass.ks_msigcom',\n",
    "            'FROM gaiaedr3.gaia_source as gaia_edr3',\n",
    "            'JOIN gaiaedr3.tmass_psc_xsc_best_neighbour AS xmatch USING (source_id)',\n",
    "            'JOIN gaiaedr3.tmass_psc_xsc_join AS xjoin USING (clean_tmass_psc_xsc_oid)',\n",
    "            'JOIN gaiadr1.tmass_original_valid AS tmass',\n",
    "            'ON xjoin.original_psc_source_id = tmass.designation',\n",
    "            # We could extend to use of Allwise (but check multiple keywords like ph_qual)\n",
    "            #'INNER JOIN gaiaedr3.allwise_best_neighbour as allwise_xmatch',\n",
    "            #'ON gaia_edr3.source_id = allwise_xmatch.source_id',\n",
    "            #'INNER JOIN gaiadr1.allwise_original_valid as allwise',\n",
    "            #'ON allwise.allwise_oid = allwise_xmatch.allwise_oid',\n",
    "            'WHERE gaia_edr3.source_id = '+str(spectrum['gaia_edr3_source_id'])\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            job = Gaia.launch_job(' '.join(adql))\n",
    "            adql_result = job.get_results()\n",
    "        except:\n",
    "            print(' '.join(adql))\n",
    "            raise ValueError('No match with via gaiaedr3.source_id found')\n",
    "\n",
    "        if np.shape(adql_result)[0] > 1:\n",
    "            print(' '.join(adql))\n",
    "            raise ValueError('More than 1 match with via gaiaedr3.source_id found')\n",
    "\n",
    "        # Prepare the photometric information from Gaia eDR3\n",
    "        Gmag, FG = correct_gband(\n",
    "            bp_rp = adql_result['bp_rp'][0], \n",
    "            astrometric_params_solved = adql_result['astrometric_params_solved'][0], \n",
    "            phot_g_mean_mag = adql_result['phot_g_mean_mag'][0], \n",
    "            phot_g_mean_flux = adql_result['phot_g_mean_flux'][0]\n",
    "        )\n",
    "        non_spec_info['gmag_gaia_edr3'] = Gmag * u.mag\n",
    "        non_spec_info['gbpmag_gaia_edr3'] = adql_result['phot_bp_mean_mag'][0] * u.mag\n",
    "        non_spec_info['grpmag_gaia_edr3'] = adql_result['phot_rp_mean_mag'][0] * u.mag\n",
    "\n",
    "        # Calculation of Gaia eDR3 G/GBP/GRP uncertainties:\n",
    "        # See both https://www.cosmos.esa.int/web/gaia/edr3-passbands\n",
    "        # and https://cdsarc.unistra.fr/viz-bin/ReadMe/I/350?format=html&tex=true#sRM3.63\n",
    "        sigmaG_0 = 0.0027553202\n",
    "        sigmaGBP_0 = 0.0027901700\n",
    "        sigmaGRP_0 = 0.0037793818\n",
    "        non_spec_info['e_gmag_gaia_edr3']   = np.sqrt((-2.5/np.log(10)*adql_result['phot_g_mean_flux_error'][0]/FG)**2 + sigmaG_0**2) * u.mag\n",
    "        non_spec_info['e_gbpmag_gaia_edr3'] = np.sqrt((-2.5/np.log(10)*adql_result['phot_bp_mean_flux_error'][0]/adql_result['phot_bp_mean_flux'][0])**2 + sigmaGBP_0**2) * u.mag\n",
    "        non_spec_info['e_grpmag_gaia_edr3'] = np.sqrt((-2.5/np.log(10)*adql_result['phot_rp_mean_flux_error'][0]/adql_result['phot_rp_mean_flux'][0])**2 + sigmaGRP_0**2) * u.mag\n",
    "\n",
    "        # Calculations of Parallax Zeropoint Corrections:\n",
    "        # Following script by Lindegren et al. (2021b)\n",
    "        # https://ui.adsabs.harvard.edu/abs/2021A&A...649A...4L\n",
    "        if adql_result['astrometric_params_solved'][0] in [31,95]:\n",
    "            # Parallax zeropoint corretion for \n",
    "            parallax_zeropoint_correction = zpt.get_zpt(\n",
    "                phot_g_mean_mag = adql_result['phot_g_mean_mag'][0],\n",
    "                nu_eff_used_in_astrometry = adql_result['nu_eff_used_in_astrometry'][0],\n",
    "                pseudocolour = adql_result['pseudocolour'][0],\n",
    "                ecl_lat = adql_result['ecl_lat'][0],\n",
    "                astrometric_params_solved = adql_result['astrometric_params_solved'][0]\n",
    "            )\n",
    "        else:\n",
    "            parallax_zeropoint_correction = 0.\n",
    "\n",
    "        if np.isfinite(adql_result['dr2_radial_velocity_error'][0]):\n",
    "            non_spec_info['rv_gaia_edr3'] = adql_result['dr2_radial_velocity'][0] * u.km/u.s\n",
    "            non_spec_info['e_rv_gaia_edr3'] = adql_result['dr2_radial_velocity_error'][0] * u.km/u.s\n",
    "        else:\n",
    "            non_spec_info['rv_gaia_edr3'] = np.nan\n",
    "            non_spec_info['e_rv_gaia_edr3'] = np.nan\n",
    "\n",
    "        non_spec_info['ruwe_gaia_edr3'] = adql_result['ruwe'][0]\n",
    "        \n",
    "        non_spec_info['parallax_raw_gaia_edr3'] = adql_result['parallax'][0] * u.mas\n",
    "        non_spec_info['parallax_gaia_edr3'] = (adql_result['parallax'][0] - parallax_zeropoint_correction) * u.mas\n",
    "        non_spec_info['e_parallax_gaia_edr3'] = adql_result['parallax_error'][0] * u.mas\n",
    "        \n",
    "        non_spec_info['parallax'] = non_spec_info['parallax_gaia_edr3']\n",
    "        non_spec_info['e_parallax'] = non_spec_info['e_parallax_gaia_edr3']\n",
    "\n",
    "        # Check entries in open cluster catalog by Cantat-Gaudin et al., 2020, A&A 640, 1\n",
    "        cantatgaudin2020_parallaxes = Table.read('../auxiliary_information/CantatGaudin_2020_AandA_640_1.fits')\n",
    "        cantatgaudin2020_match = np.where(spectrum['gaia_edr3_source_id'] == cantatgaudin2020_parallaxes['GaiaDR2'])[0]\n",
    "        # If there is an entry in this catalog\n",
    "        if len(cantatgaudin2020_match) > 0:\n",
    "            non_spec_info['parallax_cg2020'] = cantatgaudin2020_parallaxes['plx'][cantatgaudin2020_match[0]] * u.mas\n",
    "            non_spec_info['e_parallax_cg2020'] = cantatgaudin2020_parallaxes['e_plx'][cantatgaudin2020_match[0]] * u.mas\n",
    "            # replace parallax to be used, if Cantat-Gaudin et al. parallax has smaller uncertainty\n",
    "            if non_spec_info['e_parallax_cg2020'] < non_spec_info['e_parallax']:\n",
    "                print('Open cluster entry by Cantat-Gaudin et al. (2020) more precise')\n",
    "                non_spec_info['parallax'] = non_spec_info['parallax_cg2020']\n",
    "                non_spec_info['e_parallax'] = non_spec_info['e_parallax_cg2020']\n",
    "            else:\n",
    "                print('Open cluster entry by Cantat-Gaudin et al. (2020) less precise')\n",
    "                print(r'Gaia EDR3:                   $'+\"{:.3f}\".format(non_spec_info['parallax'])+' \\pm '+\"{:.3f}\".format(non_spec_info['e_parallax'])+'$')\n",
    "                print(r'Cantat-Gaudin et al. (2020): $'+\"{:.3f}\".format(non_spec_info['parallax_cg2020'])+' \\pm '+\"{:.3f}\".format(non_spec_info['e_parallax_cg2020'])+'$')\n",
    "        else:\n",
    "            print('No entry in Cantat-Gaudin et al. (2020) found')\n",
    "\n",
    "        # Check entries in open cluster catalog by Cantat-Gaudin et al., 2020, A&A 640, 1\n",
    "        vasiliev2021_parallaxes = Table.read('../auxiliary_information/VasilievBaumgardt_2021_MNRAS_505_5978_8GCs_70percent.fits')\n",
    "        vasiliev2021_match = np.where(spectrum['gaia_edr3_source_id'] == vasiliev2021_parallaxes['source_id'])[0]\n",
    "        # If there is an entry in this catalog\n",
    "        if len(vasiliev2021_match) > 0:\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_104_47Tuc':\n",
    "                non_spec_info['parallax_vb21'] = 0.232 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.009 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_288':\n",
    "                non_spec_info['parallax_vb21'] = 0.141 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.011 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_362':\n",
    "                non_spec_info['parallax_vb21'] = 0.114 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.011 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_1851':\n",
    "                non_spec_info['parallax_vb21'] = 0.088 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.011 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_5139_oCen':\n",
    "                non_spec_info['parallax_vb21'] = 0.193 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.009 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_6362':\n",
    "                non_spec_info['parallax_vb21'] = 0.136 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.010 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_6397':\n",
    "                non_spec_info['parallax_vb21'] = 0.416 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.010 * u.mas\n",
    "            if vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]] == 'NGC_7099_M_30':\n",
    "                non_spec_info['parallax_vb21'] = 0.136 * u.mas\n",
    "                non_spec_info['e_parallax_vb21'] = 0.011 * u.mas\n",
    "            \n",
    "            # replace parallax to be used, if Cantat-Gaudin et al. parallax has smaller uncertainty\n",
    "            if non_spec_info['e_parallax_vb21'] < non_spec_info['e_parallax']:\n",
    "                print('Globular cluster entry by Vasiliev & Baumgardt (2021) more precise')\n",
    "                non_spec_info['parallax'] = non_spec_info['parallax_vb21']\n",
    "                non_spec_info['e_parallax'] = non_spec_info['e_parallax_vb21']\n",
    "            else:\n",
    "                print('Globular cluster entry by Vasiliev & Baumgardt (2021) less precise:')\n",
    "                print(r'Gaia EDR3:                   $'+\"{:.3f}\".format(non_spec_info['parallax'])+' \\pm '+\"{:.3f}\".format(non_spec_info['e_parallax'])+'$')\n",
    "                print(r'Vasiliev & Baumgardt (2021): $'+\"{:.3f}\".format(non_spec_info['parallax_vb21'])+' \\pm '+\"{:.3f}\".format(non_spec_info['e_parallax_vb21'])+'$')\n",
    "        else:\n",
    "            print('No entry in Vasiliev & Baumgardt (2021) found')\n",
    "                \n",
    "        if spectrum['tmass_id'] == '16103957-2602249':\n",
    "            non_spec_info['ph_qual_tmass'] = 'AAA'\n",
    "            non_spec_info['jmag_tmass'] = 7.996 * u.mag\n",
    "            non_spec_info['e_jmag_tmass'] = 0.026 * u.mag\n",
    "            non_spec_info['hmag_tmass'] = 7.431 * u.mag\n",
    "            non_spec_info['e_hmag_tmass'] = 0.038 * u.mag\n",
    "            non_spec_info['ksmag_tmass'] = 7.265 * u.mag\n",
    "            non_spec_info['e_ksmag_tmass'] = 0.016 * u.mag\n",
    "\n",
    "        else:\n",
    "            #Prepare the photometric information from 2MASS\n",
    "            if spectrum['tmass_id'] != str(adql_result['tmass_id'][0])[2:-1]:\n",
    "                print(' '.join(adql))\n",
    "                print('GALAH internal 2MASS ID', spectrum['tmass_id'])\n",
    "                print('Gaia eDR3 matched  2MASS ID', str(adql_result['tmass_id'][0])[2:-1])\n",
    "                raise ValueError('Gaia eDR3 crossmatch to 2MASS is not consistent with our 2MASS ID')\n",
    "\n",
    "            non_spec_info['ph_qual_tmass'] = str(adql_result['ph_qual_tmass'][0])[2:-1]\n",
    "            non_spec_info['jmag_tmass'] = adql_result['j_m'][0] * u.mag\n",
    "            non_spec_info['e_jmag_tmass'] = adql_result['j_msigcom'][0] * u.mag\n",
    "            non_spec_info['hmag_tmass'] = adql_result['h_m'][0] * u.mag\n",
    "            non_spec_info['e_hmag_tmass'] = adql_result['h_msigcom'][0] * u.mag\n",
    "            non_spec_info['ksmag_tmass'] = adql_result['ks_m'][0] * u.mag\n",
    "            non_spec_info['e_ksmag_tmass'] = adql_result['ks_msigcom'][0] * u.mag\n",
    "\n",
    "    if use_asteroseismology:\n",
    "        # information from K2\n",
    "        K2 = Table.read('../auxiliary_information/Zinn_Table2_eDR3_xmatch.fits')\n",
    "        match = np.where(K2['source_id'] == non_spec_info['gaia_edr3_source_id'])[0]\n",
    "    \n",
    "        if len(match) > 0:\n",
    "            if len(match) > 1:\n",
    "                print('Multiple matches between K2 and Gaia eDR3 found!')\n",
    "\n",
    "            non_spec_info['epic_id'] = K2['epic_id'][match[0]]\n",
    "\n",
    "            non_spec_info['nu_max'] = K2['numax_mean'][match[0]] * u.microHertz\n",
    "            if np.isfinite(K2['numax_scatter'][match[0]]):\n",
    "                non_spec_info['e_nu_max'] = K2['numax_scatter'][match[0]] * u.microHertz\n",
    "            else:\n",
    "                print('No uncertainty for nu_max value')\n",
    "                non_spec_info['e_nu_max'] = np.nan\n",
    "\n",
    "            non_spec_info['delta_nu'] = K2['dnu_mean_corr'][match[0]] * u.microHertz\n",
    "            if np.isfinite(K2['dnu_mean_corr'][match[0]]):\n",
    "                non_spec_info['e_delta_nu'] = K2['dnu_mean_corr'][match[0]] * u.microHertz\n",
    "            else:\n",
    "                print('No uncertainty for delta_nu value')\n",
    "                non_spec_info['e_delta_nu'] = np.nan\n",
    "        else:\n",
    "            raise ValueError('No match with K2 asteroseismic table found!')\n",
    "\n",
    "    return(non_spec_info)\n",
    "\n",
    "non_spec_info = get_non_spec_info(spectrum, use_photoastrometry=True, use_asteroseismology=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spectrum['sobject_id'] == 160107004101260:\n",
    "    non_spec_info['nu_max'] = 1300 * u.microHertz\n",
    "    non_spec_info['e_nu_max'] = 13 * u.microHertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in isochrone grid and trained nearest neighbor search machinery 'kdtree'\n",
    "parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_6p19_0p01_10p17_mh_m2p75_0p25_m0p75_mh_m0p60_0p10_0p70_GaiaEDR3_2MASS.fits')\n",
    "# parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_6p19_0p01_10p17_mh_m2p75_0p25_1p00_mh_m0p75_0p05_0p75_GaiaEDR3_2MASS.fits')\n",
    "file = open('../auxiliary_information/parsec_isochrones/isochrone_kdtree_Teff_logg_M_H.pickle','rb')\n",
    "parsec_kdtree = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CANNON Models\n",
    "cannon_models = Table.read(cannon_model_directory+'Cannon_subgrid.fits')\n",
    "cannon_index_tree = cKDTree(np.c_[cannon_models['teff'],cannon_models['logg'],cannon_models['fe_h']])\n",
    "\n",
    "default_model_name = '../spectrum_interpolation/TheCannon/models/galah_dr4_thecannon_model_5750_4.50_0.00_order2_36labels.model'\n",
    "default_model = tc.CannonModel.read(default_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spectrum masks\n",
    "masks = Table.read('spectrum_masks/solar_spectrum_mask.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustment functions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def galah_kern(fwhm, b):\n",
    "    \"\"\" Returns a normalized 1D kernel as is used for GALAH resolution profile \"\"\"\n",
    "    size=2*(fwhm/2.355)**2\n",
    "    size_grid = int(size) # we limit the size of kernel, so it is as small as possible (or minimal size) for faster calculations\n",
    "    if size_grid<7: size_grid=7\n",
    "    x= scipy.mgrid[-size_grid:size_grid+1]\n",
    "    g = scipy.exp(-0.693147*np.power(abs(2*x/fwhm), b))\n",
    "    return g / np.sum(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cubic_spline_interpolate(old_wavelength, old_flux, new_wavelength):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    old_wavelength, old_flux: Input spectrum that has to be interpolated\n",
    "    new_wavelength: Wavelength array onto which we want to interpolate\n",
    "    \n",
    "    OUTPUT:\n",
    "    flux interpolated on new_wavelength array\n",
    "    \"\"\"\n",
    "    return scipy.interpolate.CubicSpline(old_wavelength, old_flux)(new_wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_default_degrading_wavelength_grid(default_model_wave, default_model_flux, synth_res=300000.):\n",
    "    initial_l = dict()\n",
    "    \n",
    "    for ccd in [1,2,3,4]:\n",
    "\n",
    "        wave_model_ccd = (default_model_wave > (3+ccd)*1000) & (default_model_wave < (4+ccd)*1000)\n",
    "\n",
    "        synth = np.array([default_model_wave[wave_model_ccd],default_model_flux[wave_model_ccd]]).T\n",
    "\n",
    "        l_original=synth[:,0]\n",
    "        #check if the shape of the synthetic spectrum is correct\n",
    "        if synth.shape[1]!=2: logging.error('Syntehtic spectrum must have shape m x 2.')\n",
    "\n",
    "        #check if the resolving power is high enough\n",
    "        sigma_synth=synth[:,0]/synth_res\n",
    "        if max(sigma_synth)>=min(spectrum['lsf_ccd'+str(ccd)])*0.95: logging.error('Resolving power of the synthetic spectrum must be higher.')\n",
    "\n",
    "        #check if wavelength calibration of the synthetic spectrum is linear:\n",
    "        if not (synth[:,0][1]-synth[:,0][0])==(synth[:,0][-1]-synth[:,0][-2]):\n",
    "            logging.error('Synthetic spectrum must have linear (equidistant) sampling.')\t\t\n",
    "\n",
    "        #current sampling:\n",
    "        sampl=synth[:,0][1]-synth[:,0][0]\n",
    "        galah_sampl=spectrum['cdelt_ccd'+str(ccd)]\n",
    "\n",
    "        #original sigma\n",
    "        s_original=sigma_synth\n",
    "\n",
    "        #required sigma (resample the resolution map into the wavelength range of the synthetic spectrum)\n",
    "        s_out=np.interp(synth[:,0], spectrum['crval_ccd'+str(ccd)]+spectrum['cdelt_ccd'+str(ccd)]*np.arange(len(spectrum['counts_ccd'+str(ccd)])), spectrum['lsf_ccd'+str(ccd)])\n",
    "        \n",
    "        #the sigma of the kernel is:\n",
    "        s=np.sqrt(s_out**2-s_original**2)\n",
    "        \n",
    "        #fit it with the polynomial, so we have a function instead of sampled values:\n",
    "        map_fit=np.poly1d(np.polyfit(synth[:,0], s, deg=6))\n",
    "\n",
    "        #create an array with new sampling. The first point is the same as in the spectrum:\n",
    "        l_new=[synth[:,0][0]]\n",
    "\n",
    "        #oversampling. If synthetic spectrum sampling is much finer than the size of the kernel, the code would work, but would return badly sampled spectrum. this is because from here on the needed sampling is measured in units of sigma.\n",
    "        oversample=galah_sampl/sampl*10.0\n",
    "\n",
    "        #minimal needed sampling\n",
    "        min_sampl=max(s_original)/sampl/sampl*oversample\n",
    "        \n",
    "        #keep adding samples until end of the wavelength range is reached\n",
    "        while l_new[-1]<synth[:,0][-1]+sampl:\n",
    "            # THIS IS THE BOTTLENECK OF THE COMPUTATION\n",
    "            l_new.append(l_new[-1]+map_fit(l_new[-1])/sampl/min_sampl)\n",
    "\n",
    "        initial_l['ccd'+str(ccd)] = np.array(l_new)\n",
    "    return(initial_l)\n",
    "\n",
    "default_model_flux = default_model.__call__(default_model._fiducials)\n",
    "default_model_wave = default_model._dispersion\n",
    "\n",
    "initial_l = calculate_default_degrading_wavelength_grid(default_model_wave,default_model_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def synth_resolution_degradation(l, res_map, res_b, synth, synth_res=300000.0, reuse_initial_res_wave_grid=True, initial_l=initial_l):\n",
    "    \"\"\"\n",
    "    Take a synthetic spectrum with a very high  resolution and degrade its resolution to the resolution profile of the observed spectrum. The synthetic spectrum should not be undersampled, or the result of the convolution might be wrong.\n",
    "    Parameters:\n",
    "        synth np array or similar: an array representing the synthetic spectrum. Must have size m x 2. First column is the wavelength array, second column is the flux array. Resolution of the synthetic spectrum must be constant and higher than that of the observed spectrum.\n",
    "        synth_res (float): resolving power of the synthetic spectrum\n",
    "    Returns:\n",
    "        Convolved syntehtic spectrum as a np array of size m x 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    synth=np.array(synth)\n",
    "    l_original=synth[:,0]\n",
    "    #check if the shape of the synthetic spectrum is correct\n",
    "    if synth.shape[1]!=3: logging.error('Syntehtic spectrum must have shape m x 2.')\n",
    "\n",
    "    #check if the resolving power is high enough\n",
    "    sigma_synth=synth[:,0]/synth_res\n",
    "    if max(sigma_synth)>=min(res_map)*0.95: logging.error('Resolving power of the synthetic spectrum must be higher.')\n",
    "        \n",
    "    #check if wavelength calibration of the synthetic spectrum is linear:\n",
    "    if not (synth[:,0][1]-synth[:,0][0])==(synth[:,0][-1]-synth[:,0][-2]):\n",
    "        logging.error('Synthetic spectrum must have linear (equidistant) sampling.')\t\t\n",
    "\n",
    "    #current sampling:\n",
    "    sampl=synth[:,0][1]-synth[:,0][0]\n",
    "    galah_sampl=l[1]-l[0]\n",
    "\n",
    "    #original sigma\n",
    "    s_original=sigma_synth\n",
    "\n",
    "    #oversampling. If synthetic spectrum sampling is much finer than the size of the kernel, the code would work, but would return badly sampled spectrum. this is because from here on the needed sampling is measured in units of sigma.\n",
    "    oversample=galah_sampl/sampl*10.0\n",
    "\n",
    "    if reuse_initial_res_wave_grid == False:        \n",
    "\n",
    "        #required sigma (resample the resolution map into the wavelength range of the synthetic spectrum)\n",
    "        s_out=np.interp(synth[:,0], l, res_map)\n",
    "\n",
    "        #the sigma of the kernel is:\n",
    "        s=np.sqrt(s_out**2-s_original**2)\n",
    "\n",
    "        #fit it with the polynomial, so we have a function instead of sampled values:\n",
    "        map_fit=np.poly1d(np.polyfit(synth[:,0], s, deg=6))\n",
    "\n",
    "        #create an array with new sampling. The first point is the same as in the spectrum:\n",
    "        l_new=[synth[:,0][0]]\n",
    "\n",
    "        #minimal needed sampling\n",
    "        min_sampl=max(s_original)/sampl/sampl*oversample\n",
    "\n",
    "        #keep adding samples until end of the wavelength range is reached\n",
    "        while l_new[-1]<synth[:,0][-1]+sampl:\n",
    "            # THIS IS THE BOTTLENECK OF THE COMPUTATION\n",
    "            l_new.append(l_new[-1]+map_fit(l_new[-1])/sampl/min_sampl)\n",
    "        \n",
    "        l_new = np.array(l_new)\n",
    "    else:\n",
    "        l_new = initial_l\n",
    "        \n",
    "    #interpolate the spectrum to the new sampling:\n",
    "    new_f=np.interp(l_new,synth[:,0],synth[:,1])\n",
    "    new_s=np.interp(l_new,synth[:,0],synth[:,2])\n",
    "\n",
    "    kernel_=galah_kern(max(s_original)/sampl*oversample, res_b)\n",
    "\n",
    "    con_f=signal.fftconvolve(new_f,kernel_,mode='same')\n",
    "    con_s=signal.fftconvolve(new_s,kernel_,mode='same')\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.scatter(synth[:,0],synth[:,1],label='original')\n",
    "    #inverse the warping:\n",
    "    #synth = \n",
    "    #synth[:,1]=cubic_spline_interpolate(np.array(l_new),con_f,l_original)\n",
    "    #synth[:,2]=cubic_spline_interpolate(np.array(l_new),con_s,l_original)\n",
    "    #plt.plot(synth[:,0],synth[:,1],label='interpolated')\n",
    "    #plt.plot(np.array(l_new),con_f,label='conf')\n",
    "    #plt.xlim(synth[200,0],synth[220,0])\n",
    "    #plt.legend()\n",
    "\n",
    "    return np.array([np.array(l_new),con_f,con_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gaussbroad(w, s, hwhm):\n",
    "    \"\"\"\n",
    "    Smooths a spectrum by convolution with a gaussian of specified hwhm.\n",
    "    Parameters\n",
    "    -------\n",
    "    w : array[n]\n",
    "        wavelength scale of spectrum to be smoothed\n",
    "    s : array[n]\n",
    "        spectrum to be smoothed\n",
    "    hwhm : float\n",
    "        half width at half maximum of smoothing gaussian.\n",
    "    Returns\n",
    "    -------\n",
    "    sout: array[n]\n",
    "        the gaussian-smoothed spectrum.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    --------\n",
    "        Dec-90 GB,GM\n",
    "            Rewrote with fourier convolution algorithm.\n",
    "        Jul-91 AL\n",
    "            Translated from ANA to IDL.\n",
    "        22-Sep-91 JAV\n",
    "            Relaxed constant dispersion check# vectorized, 50% faster.\n",
    "        05-Jul-92 JAV\n",
    "            Converted to function, handle nonpositive hwhm.\n",
    "        Oct-18 AW\n",
    "            Python version\n",
    "    \"\"\"\n",
    "\n",
    "    # Warn user if hwhm is negative.\n",
    "    if hwhm < 0:\n",
    "        logger.warning(\"Forcing negative smoothing width to zero.\")\n",
    "\n",
    "    # Return input argument if half-width is nonpositive.\n",
    "    if hwhm <= 0:\n",
    "        return s  # true: no broadening\n",
    "\n",
    "    # Calculate (uniform) dispersion.\n",
    "    nw = len(w)  ## points in spectrum\n",
    "    wrange = w[-1] - w[0]\n",
    "    dw = wrange / (nw - 1)  # wavelength change per pixel\n",
    "\n",
    "    # Make smoothing gaussian# extend to 4 sigma.\n",
    "    # 4.0 / sqrt(2.0*alog(2.0)) = 3.3972872 and sqrt(alog(2.0))=0.83255461\n",
    "    # sqrt(alog(2.0)/pi)=0.46971864 (*1.0000632 to correct for >4 sigma wings)\n",
    "    if hwhm >= 5 * wrange:\n",
    "        return np.full(nw, np.sum(s) / nw)\n",
    "    nhalf = int(3.3972872 * hwhm / dw)  ## points in half gaussian\n",
    "    ng = 2 * nhalf + 1  ## points in gaussian (odd!)\n",
    "    wg = dw * (\n",
    "        np.arange(ng, dtype=float) - (ng - 1) / 2\n",
    "    )  # wavelength scale of gaussian\n",
    "    xg = (0.83255461 / hwhm) * wg  # convenient absisca\n",
    "    gpro = (0.46974832 * dw / hwhm) * np.exp(-xg * xg)  # unit area gaussian w/ FWHM\n",
    "    gpro = gpro / np.sum(gpro)\n",
    "\n",
    "    # Pad spectrum ends to minimize impact of Fourier ringing.\n",
    "    sout = convolve(s, gpro, mode=\"nearest\")\n",
    "\n",
    "    return sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def apply_gauss_broad(wave, smod, ipres=30000, debug=True):\n",
    "    # Apply Gaussian Instrument Broadening\n",
    "    if ipres == 0.0:\n",
    "        hwhm = 0\n",
    "    else:\n",
    "        hwhm = 0.5 * wave[0] / ipres\n",
    "    if hwhm > 0: smod = gaussbroad(wave, smod, hwhm)\n",
    "\n",
    "    return(smod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sclip(p,fit,n,ye=[],sl=99999,su=99999,min=0,max=0,min_data=1,grow=0,global_mask=None,verbose=True):\n",
    "    \"\"\"\n",
    "    p: array of coordinate vectors. Last line in the array must be values that are fitted. The rest are coordinates.\n",
    "    fit: name of the fitting function. It must have arguments x,y,ye,and mask and return an array of values of the fitted function at coordinates x\n",
    "    n: number of iterations\n",
    "    ye: array of errors for each point\n",
    "    sl: lower limit in sigma units\n",
    "    su: upper limit in sigma units\n",
    "    min: number or fraction of rejected points below the fitted curve\n",
    "    max: number or fraction of rejected points above the fitted curve\n",
    "    min_data: minimal number of points that can still be used to make a constrained fit\n",
    "    global_mask: if initial mask is given it will be used throughout the whole fitting process, but the final fit will be evaluated also in the masked points\n",
    "    grow: number of points to reject around the rejected point.\n",
    "    verbose: print the results or not\n",
    "    \"\"\"\n",
    "\n",
    "    nv,dim=np.shape(p)\n",
    "\n",
    "    #if error vector is not given, assume errors are equal to 0:\n",
    "    if ye==[]: ye=np.zeros(dim)\n",
    "    #if a single number is given for y errors, assume it means the same error is for all points:\n",
    "    if isinstance(ye, (int, float)): ye=np.ones(dim)*ye\n",
    "\n",
    "    if global_mask==None: global_mask=np.ones(dim, dtype=bool)\n",
    "    else: pass\n",
    "\n",
    "    f_initial=fit(p,ye,global_mask)\n",
    "    s_initial=np.std(p[-1]-f_initial)\n",
    "\n",
    "    f=f_initial\n",
    "    s=s_initial\n",
    "\n",
    "    tmp_results=[]\n",
    "\n",
    "    b_old=np.ones(dim, dtype=bool)\n",
    "\n",
    "    for step in range(n):\n",
    "        #check that only sigmas or only min/max are given:\n",
    "        if (sl!=99999 or su!=99999) and (min!=0 or max!=0):\n",
    "            raise RuntimeError('Sigmas and min/max are given. Only one can be used.')\n",
    "\n",
    "        #if sigmas are given:\n",
    "        if sl!=99999 or su!=99999:\n",
    "            b=np.zeros(dim, dtype=bool)\n",
    "            if sl>=99999 and su!=sl: sl=su#check if only one is given. In this case set the other to the same value\n",
    "            if su>=99999 and sl!=su: su=sl\n",
    "\n",
    "            good_values=np.where(((f-p[-1])<(sl*(s+ye))) & ((f-p[-1])>-(su*(s+ye))))#find points that pass the sigma test\n",
    "            b[good_values]=True\n",
    "\n",
    "        #if min/max are given\n",
    "        if min!=0 or max!=0:\n",
    "            b=np.ones(dim, dtype=bool)\n",
    "            if min<1: min=dim*min#detect if min is in number of points or percentage\n",
    "            if max<1: max=dim*max#detect if max is in number of points or percentage\n",
    "\n",
    "            bad_values=np.concatenate(((p[-1]-f).argsort()[-int(max):], (p[-1]-f).argsort()[:int(min)]))\n",
    "            b[bad_values]=False\n",
    "\n",
    "        #check the grow parameter:\n",
    "        if grow>=1 and nv==2:\n",
    "            b_grown=np.ones(dim, dtype=bool)\n",
    "            for ind,val in enumerate(b):\n",
    "                if val==False:\n",
    "                    ind_l=ind-int(grow)\n",
    "                    ind_u=ind+int(grow)+1\n",
    "                    if ind_l<0: ind_l=0\n",
    "                    b_grown[ind_l:ind_u]=False\n",
    "\n",
    "            b=b_grown\n",
    "\n",
    "        tmp_results.append(f)\n",
    "\n",
    "        #check that the minimal number of good points is not too low:\n",
    "        if len(b[b])<min_data:\n",
    "            step=step-1\n",
    "            b=b_old\n",
    "            break\n",
    "\n",
    "        #check if the new b is the same as old one and break if yes:\n",
    "        if np.array_equal(b,b_old):\n",
    "            step=step-1\n",
    "            break\n",
    "\n",
    "        #fit again\n",
    "        f=fit(p,ye,b&global_mask)\n",
    "        s=np.std(p[-1][b]-f[b])\n",
    "        b_old=b\n",
    "\n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('FITTING RESULTS:')\n",
    "        print('Number of iterations requested:    ',n)\n",
    "        print('Number of iterations performed:    ', step+1)\n",
    "        print('Initial standard deviation:        ', s_initial)\n",
    "        print('Final standard deviation:          ', s)\n",
    "        print('Number of rejected points:         ',len(np.invert(b[np.invert(b)])))\n",
    "        print('')\n",
    "\n",
    "    return f,tmp_results,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def chebyshev(p,ye,mask):\n",
    "    coef=np.polynomial.chebyshev.chebfit(p[0][mask], p[1][mask], 4)\n",
    "    cont=np.polynomial.chebyshev.chebval(p[0],coef)\n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rv_shift(rv_value, wavelength):\n",
    "    '''\n",
    "    Shifts observed wavelengths to account for radial velocity measurements\n",
    "    \n",
    "    INPUT:\n",
    "    rv_value = radial velocity in km/s (negative if moving towards earth)\n",
    "    wavelengths = array of observed wavelengths\n",
    "    \n",
    "    OUTPUT:\n",
    "    array of shifted wavelengths\n",
    "    '''\n",
    "    return wavelength / (1.+rv_value/c.c.to(u.km/u.s).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_closest_cannon_model(cannon_index_tree, teff, logg, feh, labels, order, debug=True):\n",
    "\n",
    "    cannon_index = cannon_index_tree.query(np.c_[teff, logg, feh])[1]\n",
    "    \n",
    "    if debug:\n",
    "        print(cannon_index)\n",
    "        print('input: ',teff,logg,feh)\n",
    "        print('Cannon: ',cannon_models['teff'][cannon_index][0],cannon_models['logg'][cannon_index][0],cannon_models['fe_h'][cannon_index][0])\n",
    "\n",
    "    model_file = 'subgrid_'+str(cannon_models['index'][cannon_index][0])+'_order'+str(order)+'_'+str(len(labels))+'labels'\n",
    "    if debug:\n",
    "        print(model_file)\n",
    "    \n",
    "    return(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_synthetic_spectrum(model_parameters, model_labels, default_model=None, default_model_name=None, debug=True):\n",
    "    \n",
    "    model_parameters = np.array(model_parameters)\n",
    "    \n",
    "    if 'teff' in model_labels:\n",
    "        teff = 1000.*model_parameters[model_labels=='teff'][0]\n",
    "    else:\n",
    "        raise ValueError('You have to define Teff as input parameter')\n",
    "    if 'logg' in model_labels:\n",
    "        logg = model_parameters[model_labels=='logg'][0]\n",
    "    else:\n",
    "        raise ValueError('You have to define logg as input parameter')\n",
    "    if 'fe_h' in model_labels:\n",
    "        fe_h = model_parameters[model_labels=='fe_h'][0]\n",
    "    else:\n",
    "        raise ValueError('You have to define fe_h as input parameter')\n",
    "\n",
    "    if 'vmic' in model_labels:\n",
    "        vmic = model_parameters[model_labels=='vmic'][0]\n",
    "    else:\n",
    "        vmic = 1.0\n",
    "\n",
    "    if 'vsini' in model_labels:\n",
    "        vsini = model_parameters[model_labels=='vsini'][0]\n",
    "    else:\n",
    "        vsini = 0.0\n",
    "\n",
    "    if 'li_fe' in model_labels:\n",
    "        li_fe = model_parameters[model_labels=='li_fe'][0]\n",
    "    else:\n",
    "        li_fe = 0.0\n",
    "    \n",
    "    if 'c_fe' in model_labels:\n",
    "        c_fe = model_parameters[model_labels=='c_fe'][0]\n",
    "    else:\n",
    "        c_fe = 0.0\n",
    "\n",
    "    if 'n_fe' in model_labels:\n",
    "        n_fe = model_parameters[model_labels=='n_fe'][0]\n",
    "    else:\n",
    "        n_fe = 0.0\n",
    "\n",
    "    if 'o_fe' in model_labels:\n",
    "        o_fe = model_parameters[model_labels=='o_fe'][0]\n",
    "    else:\n",
    "        o_fe = 0.0\n",
    "\n",
    "    if 'na_fe' in model_labels:\n",
    "        na_fe = model_parameters[model_labels=='na_fe'][0]\n",
    "    else:\n",
    "        na_fe = 0.0\n",
    "\n",
    "    if 'mg_fe' in model_labels:\n",
    "        mg_fe = model_parameters[model_labels=='mg_fe'][0]\n",
    "    else:\n",
    "        mg_fe = 0.0\n",
    "\n",
    "    if 'al_fe' in model_labels:\n",
    "        al_fe = model_parameters[model_labels=='al_fe'][0]\n",
    "    else:\n",
    "        al_fe = 0.0\n",
    "\n",
    "    if 'si_fe' in model_labels:\n",
    "        si_fe = model_parameters[model_labels=='si_fe'][0]\n",
    "    else:\n",
    "        si_fe = 0.0\n",
    "\n",
    "    if 'k_fe' in model_labels:\n",
    "        k_fe = model_parameters[model_labels=='k_fe'][0]\n",
    "    else:\n",
    "        k_fe = 0.0\n",
    "\n",
    "    if 'ca_fe' in model_labels:\n",
    "        ca_fe = model_parameters[model_labels=='ca_fe'][0]\n",
    "    else:\n",
    "        ca_fe = 0.0\n",
    "\n",
    "    if 'sc_fe' in model_labels:\n",
    "        sc_fe = model_parameters[model_labels=='sc_fe'][0]\n",
    "    else:\n",
    "        sc_fe = 0.0\n",
    "\n",
    "    if 'ti_fe' in model_labels:\n",
    "        ti_fe = model_parameters[model_labels=='ti_fe'][0]\n",
    "    else:\n",
    "        ti_fe = 0.0\n",
    "\n",
    "    if 'v_fe' in model_labels:\n",
    "        v_fe = model_parameters[model_labels=='v_fe'][0]\n",
    "    else:\n",
    "        v_fe = 0.0\n",
    "\n",
    "    if 'cr_fe' in model_labels:\n",
    "        cr_fe = model_parameters[model_labels=='cr_fe'][0]\n",
    "    else:\n",
    "        cr_fe = 0.0\n",
    "\n",
    "    if 'mn_fe' in model_labels:\n",
    "        mn_fe = model_parameters[model_labels=='mn_fe'][0]\n",
    "    else:\n",
    "        mn_fe = 0.0\n",
    "\n",
    "    if 'co_fe' in model_labels:\n",
    "        co_fe = model_parameters[model_labels=='co_fe'][0]\n",
    "    else:\n",
    "        co_fe = 0.0\n",
    "\n",
    "    if 'ni_fe' in model_labels:\n",
    "        ni_fe = model_parameters[model_labels=='ni_fe'][0]\n",
    "    else:\n",
    "        ni_fe = 0.0\n",
    "\n",
    "    if 'cu_fe' in model_labels:\n",
    "        cu_fe = model_parameters[model_labels=='cu_fe'][0]\n",
    "    else:\n",
    "        cu_fe = 0.0\n",
    "\n",
    "    if 'zn_fe' in model_labels:\n",
    "        zn_fe = model_parameters[model_labels=='zn_fe'][0]\n",
    "    else:\n",
    "        zn_fe = 0.0\n",
    "\n",
    "    if 'rb_fe' in model_labels:\n",
    "        rb_fe = model_parameters[model_labels=='rb_fe'][0]\n",
    "    else:\n",
    "        rb_fe = 0.0\n",
    "\n",
    "    if 'sr_fe' in model_labels:\n",
    "        sr_fe = model_parameters[model_labels=='sr_fe'][0]\n",
    "    else:\n",
    "        sr_fe = 0.0\n",
    "\n",
    "    if 'y_fe' in model_labels:\n",
    "        y_fe = model_parameters[model_labels=='y_fe'][0]\n",
    "    else:\n",
    "        y_fe = 0.0\n",
    "\n",
    "    if 'zr_fe' in model_labels:\n",
    "        zr_fe = model_parameters[model_labels=='zr_fe'][0]\n",
    "    else:\n",
    "        zr_fe = 0.0\n",
    "\n",
    "    if 'mo_fe' in model_labels:\n",
    "        mo_fe = model_parameters[model_labels=='mo_fe'][0]\n",
    "    else:\n",
    "        mo_fe = 0.0\n",
    "\n",
    "    if 'ru_fe' in model_labels:\n",
    "        ru_fe = model_parameters[model_labels=='ru_fe'][0]\n",
    "    else:\n",
    "        ru_fe = 0.0\n",
    "\n",
    "    if 'ba_fe' in model_labels:\n",
    "        ba_fe = model_parameters[model_labels=='ba_fe'][0]\n",
    "    else:\n",
    "        ba_fe = 0.0\n",
    "\n",
    "    if 'la_fe' in model_labels:\n",
    "        la_fe = model_parameters[model_labels=='la_fe'][0]\n",
    "    else:\n",
    "        la_fe = 0.0\n",
    "\n",
    "    if 'ce_fe' in model_labels:\n",
    "        ce_fe = model_parameters[model_labels=='ce_fe'][0]\n",
    "    else:\n",
    "        ce_fe = 0.0\n",
    "\n",
    "    if 'nd_fe' in model_labels:\n",
    "        nd_fe = model_parameters[model_labels=='nd_fe'][0]\n",
    "    else:\n",
    "        nd_fe = 0.0\n",
    "\n",
    "    if 'sm_fe' in model_labels:\n",
    "        sm_fe = model_parameters[model_labels=='sm_fe'][0]\n",
    "    else:\n",
    "        sm_fe = 0.0\n",
    "\n",
    "    if 'eu_fe' in model_labels:\n",
    "        eu_fe = model_parameters[model_labels=='eu_fe'][0]\n",
    "    else:\n",
    "        eu_fe = 0.0\n",
    "    \n",
    "    #model_file = find_closest_cannon_model(cannon_index_tree, teff, logg, fe_h, labels=['teff','logg','feh'], order=2, debug=debug)\n",
    "    #model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/'+model_file+'.model')\n",
    "    #model_spectrum = model.__call__([teff, logg, fe_h])\n",
    "\n",
    "    # Below we have other Cannon Model Choices with 35 and 10 labels for the Solar Twin Cannon label\n",
    "    \n",
    "#     model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/rc_rgb_grid_pm1dex_25_labels.model')\n",
    "#     model_spectrum = model.__call__([\n",
    "#         teff, logg, fe_h, vmic, li_fe,\n",
    "#         c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "#         al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "#         ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "#         ni_fe, cu_fe, zn_fe, #rb_fe, sr_fe,\n",
    "#         y_fe, #zr_fe, mo_fe, ru_fe, \n",
    "#         ba_fe\n",
    "#         #la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "#     ])\n",
    "    \n",
    "#     model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/rc_rgb_grid_pm1dex_vsini_25_labels.model')\n",
    "#     model_spectrum = model.__call__([\n",
    "#         teff, logg, fe_h, vmic, vsini, li_fe,\n",
    "#         c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "#         al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "#         ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "#         ni_fe, cu_fe, zn_fe, #rb_fe, sr_fe,\n",
    "#         y_fe, #zr_fe, mo_fe, ru_fe, \n",
    "#         ba_fe\n",
    "#         #la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "#     ])\n",
    "    \n",
    "    # model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/rc_rgb_grid_pm1dex_vsini_5_labels.model')\n",
    "    # model_spectrum = model.__call__([\n",
    "    #     teff, logg, fe_h, vmic, vsini\n",
    "    # ])\n",
    "\n",
    "    # model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/solar_twin_grid_pm1dex_35_labels.model')\n",
    "    # model_spectrum = model.__call__([\n",
    "    #     teff, logg, fe_h, vmic, li_fe,\n",
    "    #     c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "    #     al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "    #     ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "    #     ni_fe, cu_fe, zn_fe, rb_fe, sr_fe,\n",
    "    #     y_fe, zr_fe, mo_fe, ru_fe, ba_fe,\n",
    "    #     la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "    # ])\n",
    "    \n",
    "    if default_model_name == None:\n",
    "        model = tc.CannonModel.read('../spectrum_interpolation/TheCannon/models/galah_dr4_thecannon_model_5750_4.50_0.00_order2_36labels.model')\n",
    "        model_spectrum = model.__call__([\n",
    "            teff, logg, fe_h, vmic, vsini, li_fe,\n",
    "            c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "            al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "            ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "            ni_fe, cu_fe, zn_fe, rb_fe, sr_fe,\n",
    "            y_fe, zr_fe, mo_fe, ru_fe, ba_fe,\n",
    "            la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "        ])\n",
    "        return(model.dispersion, model_spectrum, model.s2)\n",
    "    else:\n",
    "        model_spectrum = default_model.__call__([\n",
    "            teff, logg, fe_h, vmic, vsini, li_fe,\n",
    "            c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "            al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "            ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "            ni_fe, cu_fe, zn_fe, rb_fe, sr_fe,\n",
    "            y_fe, zr_fe, mo_fe, ru_fe, ba_fe,\n",
    "            la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "        ])\n",
    "        \n",
    "        return(default_model.dispersion, model_spectrum, default_model.s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def match_observation_and_model(model_parameters, model_labels, spectrum, masks, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=False, debug=True):\n",
    "    \n",
    "    model_parameters = np.array(model_parameters)\n",
    "    vrad = model_parameters[model_labels=='vrad'][0]\n",
    "    cdelt = dict()\n",
    "    crval = dict()\n",
    "        \n",
    "    if debug:\n",
    "        start = time.time()\n",
    "        print(start)\n",
    "    \n",
    "    wave_model, flux_model, s2_model = create_synthetic_spectrum(model_parameters, model_labels, default_model, default_model_name, debug)\n",
    "    \n",
    "    if debug:\n",
    "        time_step = time.time()-start\n",
    "        print('reading in cannon model',time_step)\n",
    "    \n",
    "    # at the moment, let's assume cdelt and crval are correct\n",
    "    \n",
    "    for ccd in [1,2,3,4]:\n",
    "        \n",
    "        if 'cdelt'+str(ccd) in model_labels:\n",
    "            cdelt['ccd'+str(ccd)] = model_parameters[model_labels=='cdelt'+str(ccd)][0]\n",
    "        else:\n",
    "            cdelt['ccd'+str(ccd)] = 1000*spectrum['cdelt_ccd'+str(ccd)]\n",
    "\n",
    "        if 'crval'+str(ccd) in model_labels:\n",
    "            crval['ccd'+str(ccd)] = model_parameters[model_labels=='crval'+str(ccd)][0]\n",
    "        else:\n",
    "            crval['ccd'+str(ccd)] = spectrum['crval_ccd'+str(ccd)]\n",
    "        \n",
    "        spectrum['wave_ccd'+str(ccd)] = rv_shift(vrad,crval['ccd'+str(ccd)] + cdelt['ccd'+str(ccd)]/1000.*np.arange(len(spectrum['counts_ccd'+str(ccd)])))\n",
    "        \n",
    "        wave_model_ccd = (wave_model > (3+ccd)*1000) & (wave_model < (4+ccd)*1000)\n",
    "        \n",
    "        # Degrade synthetic spectrum onto LSF\n",
    "        # Note: Synthetic spectra have to be on equidistant wavelength scale!\n",
    "        wave_model_ccd_lsf, flux_model_ccd_lsf, s2_model_ccd_lsf = synth_resolution_degradation(\n",
    "            l = spectrum['crval_ccd'+str(ccd)] + spectrum['cdelt_ccd'+str(ccd)]*np.arange(len(spectrum['counts_ccd'+str(ccd)])), \n",
    "            res_map = spectrum['lsf_ccd'+str(ccd)], \n",
    "            res_b = spectrum['lsf_b_ccd'+str(ccd)], \n",
    "            synth = np.array([wave_model[wave_model_ccd], flux_model[wave_model_ccd], s2_model[wave_model_ccd]]).T,\n",
    "            synth_res=300000.0,\n",
    "            reuse_initial_res_wave_grid = reuse_initial_res_wave_grid,\n",
    "            initial_l = initial_l['ccd'+str(ccd)]\n",
    "        )\n",
    "        if debug:\n",
    "            time_step_old = time_step\n",
    "            time_step = time.time()-start\n",
    "            print('degrade flux and sigma ccd'+str(ccd),time_step,time_step-time_step_old)\n",
    "        \n",
    "        # Interpolate model onto right wavelength grid\n",
    "        spectrum['flux_model_ccd'+str(ccd)] = cubic_spline_interpolate(\n",
    "            wave_model_ccd_lsf,\n",
    "            flux_model_ccd_lsf,\n",
    "            spectrum['wave_ccd'+str(ccd)]\n",
    "        )\n",
    "\n",
    "        spectrum['s2_model_ccd'+str(ccd)] = cubic_spline_interpolate(\n",
    "            wave_model_ccd_lsf,\n",
    "            s2_model_ccd_lsf,\n",
    "            spectrum['wave_ccd'+str(ccd)]\n",
    "        )\n",
    "\n",
    "        renormalisation_fit = sclip((spectrum['wave_ccd'+str(ccd)],spectrum['counts_ccd'+str(ccd)]/spectrum['flux_model_ccd'+str(ccd)]),chebyshev,int(3),ye=spectrum['counts_unc_ccd'+str(ccd)],su=5,sl=5,min_data=100,verbose=False)\n",
    "        spectrum['flux_obs_ccd'+str(ccd)] = spectrum['counts_ccd'+str(ccd)]/renormalisation_fit[0]\n",
    "        spectrum['flux_obs_unc_ccd'+str(ccd)] = spectrum['counts_unc_ccd'+str(ccd)]/renormalisation_fit[0]\n",
    "            \n",
    "    # prepare input for likelihood (we will combine sigma2 and s2 later):\n",
    "    # -0.5 * sum((data-model))**2/sigma) + log(sigma)\n",
    "    wave = np.concatenate([spectrum['wave_ccd'+str(ccd)] for ccd in [1,2,3,4]])\n",
    "    data = np.concatenate([spectrum['flux_obs_ccd'+str(ccd)] for ccd in [1,2,3,4]])\n",
    "    sigma2 = np.concatenate([spectrum['flux_obs_unc_ccd'+str(ccd)] for ccd in [1,2,3,4]])**2\n",
    "    model = np.concatenate([spectrum['flux_model_ccd'+str(ccd)] for ccd in [1,2,3,4]])\n",
    "    s2 = np.concatenate([spectrum['s2_model_ccd'+str(ccd)] for ccd in [1,2,3,4]])\n",
    "\n",
    "    return(wave,data,sigma2,model,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_observation_and_model(model_parameters, model_labels, spectrum, masks, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=False, debug=False):\n",
    "    \n",
    "    wave,data,sigma2,model,s2 = match_observation_and_model(model_parameters, model_labels, spectrum, masks, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=reuse_initial_res_wave_grid, debug=debug)\n",
    "    \n",
    "    for ccd in [1,2,3,4]:\n",
    "        plt.figure(figsize=(15,3))\n",
    "        plt.fill_between(\n",
    "            spectrum['wave_ccd'+str(ccd)],\n",
    "            spectrum['flux_obs_ccd'+str(ccd)]-spectrum['flux_obs_unc_ccd'+str(ccd)],\n",
    "            spectrum['flux_obs_ccd'+str(ccd)]+spectrum['flux_obs_unc_ccd'+str(ccd)],\n",
    "            color = 'grey',alpha=0.5,label='Uncertainty'\n",
    "        )\n",
    "        plt.plot(spectrum['wave_ccd'+str(ccd)],spectrum['flux_obs_ccd'+str(ccd)],c='k',label='Observation',lw=0.5)\n",
    "        plt.plot(spectrum['wave_ccd'+str(ccd)],spectrum['flux_model_ccd'+str(ccd)],c='C0',label='Synthesis',lw=0.5)\n",
    "        plt.plot(spectrum['wave_ccd'+str(ccd)],spectrum['flux_obs_ccd'+str(ccd)]-spectrum['flux_model_ccd'+str(ccd)],c='C1',label='Residuals',lw=0.5)\n",
    "        maski = 0\n",
    "        for (mask_beginning, mask_end) in zip(masks['mask_begin'],masks['mask_end']):\n",
    "            if (mask_beginning > spectrum['wave_ccd'+str(ccd)][0]) & (mask_end < spectrum['wave_ccd'+str(ccd)][-1]):\n",
    "                if maski == 0:\n",
    "                    plt.axvspan(mask_beginning,mask_end,color='C0',alpha=0.2,label='Mask')\n",
    "                    maski += 1\n",
    "                else:\n",
    "                    plt.axvspan(mask_beginning,mask_end,color='C0',alpha=0.2)\n",
    "        plt.xlabel('Wavelength [$\\mathrm{\\AA}$]')\n",
    "        plt.ylabel('Flux [norm.]')\n",
    "        plt.ylim(-0.2,1.2)\n",
    "        if ccd == 3:\n",
    "            plt.legend(ncol=5)\n",
    "        \n",
    "    plot_output = dict()\n",
    "    plot_output['wave'] = wave\n",
    "    plot_output['data'] = data\n",
    "    plot_output['sigma2'] = sigma2\n",
    "    plot_output['model'] = model\n",
    "    plot_output['s2'] = s2\n",
    "    \n",
    "    return(plot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def spectroscopic_log_likelihood(model_parameters, model_labels, spectrum, masks, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=False, debug=False):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    model_parameters: an array with each parameter that we need to create an array \"flux_modelled\"\n",
    "    wavelength_observed, flux_observed, flux_error_observed\n",
    "    \n",
    "    OUTPUT:\n",
    "    log(Likelihood) = -0.5 * sum((data-model))**2/sigma2) + log(sigma2)\n",
    "    \"\"\"\n",
    "        \n",
    "    (wave,data,data_sigma2,model,model_sigma2) = match_observation_and_model(model_parameters, model_labels, spectrum, masks, default_model, default_model_name, reuse_initial_res_wave_grid, debug)\n",
    "        \n",
    "    # you can also add \"debug\" keywords. They come in handy, when debugging code.\n",
    "    # Here I simply test, how rv_value is changing during the likelihood maximisation\n",
    "    if debug:\n",
    "        print('Current LogP Values:', model_parameters)\n",
    "\n",
    "    sigma2 = data_sigma2 + model_sigma2\n",
    "\n",
    "    # Mask pixels that differ more than 0.25 units and 5 sigma of normalised flux or less than\n",
    "    # Mask pixels within the predefined masks\n",
    "    unmasked = (\n",
    "        (~((np.abs(data-model)/np.sqrt(sigma2) > 5) & (np.abs(data-model) > 0.25))) & \n",
    "        (~np.any(np.array([((wave >= mask_beginning) & (wave <= mask_end)) for (mask_beginning, mask_end) in zip(masks['mask_begin'],masks['mask_end'])]),axis=0))\n",
    "    )\n",
    "    \n",
    "    return - 1/2 * ((data[unmasked] - model[unmasked])**2/(sigma2[unmasked]) + np.log(2*np.pi*np.sqrt(sigma2[unmasked])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def asteroseismic_log_likelihood(model_parameters, model_labels, non_spec_info):\n",
    "\n",
    "    model_parameters = np.array(model_parameters)\n",
    "\n",
    "    teff = 1000.*model_parameters[model_labels=='teff'][0]\n",
    "    logg = model_parameters[model_labels=='logg'][0]\n",
    "    \n",
    "    nu_max_model = (10**logg) / (10**4.438) * np.sqrt(teff / 5772.) * 3090. * u.microHertz\n",
    "    \n",
    "    return - 1/2 * ((non_spec_info['nu_max'] - nu_max_model)**2/(non_spec_info['e_nu_max'])**2 + np.log(2*np.pi*non_spec_info['e_nu_max']/u.microHertz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def photoastrometric_log_likelihood(model_parameters, model_labels, non_spec_info, use_gaia_mags = False):\n",
    "    \n",
    "    model_parameters = np.array(model_parameters)\n",
    "    teff = 1000.*model_parameters[model_labels=='teff'][0]\n",
    "    logg = model_parameters[model_labels=='logg'][0]\n",
    "    fe_h = model_parameters[model_labels=='fe_h'][0]\n",
    "    distance = model_parameters[model_labels=='distance'][0] * u.pc\n",
    "    A_V = model_parameters[model_labels=='a_v'][0] * u.mag\n",
    "    \n",
    "    # taken as truth value for the purpose of finding the best isochrone:\n",
    "    abs_j_m = non_spec_info['jmag_tmass'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 0.243 # Â±0.004 Wang & Chen 2019 ApJ 877:116\n",
    "    abs_h_m = non_spec_info['hmag_tmass'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 0.131 # Â±0.006 Wang & Chen 2019 ApJ 877:116\n",
    "    abs_ks_m = non_spec_info['ksmag_tmass'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 0.078 # Â±0.004 Wang & Chen 2019 ApJ 877:116\n",
    "    if use_gaia_mags:\n",
    "        abs_gaia_g = non_spec_info['gmag_gaia_edr3'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 1.002 # Â±0.005 Wang & Chen 2019 ApJ 877:116\n",
    "        abs_gaia_bp = non_spec_info['gbpmag_gaia_edr3'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 1.002 # Â±0.007 Wang & Chen 2019 ApJ 877:116\n",
    "        abs_gaia_rp = non_spec_info['grpmag_gaia_edr3'] - 5*np.log10(distance/(10. * u.pc))*u.mag - A_V * 0.589 # Â±0.004 Wang & Chen 2019 ApJ 877:116\n",
    "    \n",
    "    # find the nearest neighbours\n",
    "    distance_matches, closest_matches = parsec_kdtree.query([[np.log10(teff),logg,fe_h]],k=100)\n",
    "    \n",
    "    # Calculate weighted average values based on squared KDTree distances\n",
    "    weights = 1/distance_matches**2\n",
    "    mass = np.average(parsec['mass'][closest_matches],weights=weights) * u.M_sun\n",
    "    age = 10**np.average(parsec['logAge'][closest_matches],weights=weights) * u.Gyr\n",
    "    abs_j_m = np.average(parsec['Jmag'][closest_matches],weights=weights) * u.mag\n",
    "    abs_h_m = np.average(parsec['Hmag'][closest_matches],weights=weights) * u.mag\n",
    "    abs_ks_m = np.average(parsec['Ksmag'][closest_matches],weights=weights) * u.mag \n",
    "    if use_gaia_mags:\n",
    "        abs_gaia_g = np.average(parsec['Gmag'][closest_matches],weights=weights) * u.mag \n",
    "        abs_gaia_bp = np.average(parsec['GBPmag'][closest_matches],weights=weights) * u.mag \n",
    "        abs_gaia_rp = np.average(parsec['GRPmag'][closest_matches],weights=weights) * u.mag \n",
    "\n",
    "    # Calculate observables for comparison\n",
    "    parallax_model = ((1/distance) * u.arcsec * u.pc).to(u.mas)\n",
    "    \n",
    "    jmag_tmass_model = abs_j_m + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 0.28688 # PARSEC website\n",
    "    hmag_tmass_model = abs_h_m + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 0.18103 # PARSEC website\n",
    "    ksmag_tmass_model = abs_ks_m + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 0.11265 # PARSEC website\n",
    "    if use_gaia_mags:\n",
    "        gmag_gaia_edr3_model = abs_gaia_g + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 0.83627 # PARSEC website\n",
    "        gbpmag_gaia_edr3_model = abs_gaia_bp + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 1.08337 # PARSEC website\n",
    "        grpmag_gaia_edr3_model = abs_gaia_rp + 5*np.log10(distance/(10. * u.pc)) * u.mag  + A_V * 0.63439 # PARSEC website\n",
    "\n",
    "    logL_parallax = - 1/2 * ((parallax_model - non_spec_info['parallax'])**2/(non_spec_info['e_parallax']**2) + np.log(2*np.pi*non_spec_info['e_parallax']/u.mas))\n",
    "    logL_jmag = - 1/2 * ((jmag_tmass_model - non_spec_info['jmag_tmass'])**2/(non_spec_info['e_jmag_tmass']**2) + np.log(2*np.pi*non_spec_info['e_jmag_tmass']/u.mag))\n",
    "    logL_hmag = - 1/2 * ((hmag_tmass_model - non_spec_info['hmag_tmass'])**2/(non_spec_info['e_hmag_tmass']**2) + np.log(2*np.pi*non_spec_info['e_hmag_tmass']/u.mag))\n",
    "    logL_ksmag = - 1/2 * ((ksmag_tmass_model - non_spec_info['ksmag_tmass'])**2/(non_spec_info['e_ksmag_tmass']**2) + np.log(2*np.pi*non_spec_info['e_ksmag_tmass']/u.mag))\n",
    "    if use_gaia_mags:\n",
    "        logL_gmag = - 1/2 * ((gmag_gaia_edr3_model - non_spec_info['gmag_gaia_edr3'])**2/(non_spec_info['e_gmag_gaia_edr3']**2) + np.log(2*np.pi*non_spec_info['e_gmag_gaia_edr3']/u.mag))\n",
    "        logL_gbpmag = - 1/2 * ((gbpmag_gaia_edr3_model - non_spec_info['gbpmag_gaia_edr3'])**2/(non_spec_info['e_gbpmag_gaia_edr3']**2) + np.log(2*np.pi*non_spec_info['e_gbpmag_gaia_edr3']/u.mag))\n",
    "        logL_grpmag = - 1/2 * ((grpmag_gaia_edr3_model - non_spec_info['grpmag_gaia_edr3'])**2/(non_spec_info['e_grpmag_gaia_edr3']**2) + np.log(2*np.pi*non_spec_info['e_grpmag_gaia_edr3']/u.mag))\n",
    "\n",
    "    if 'theta_ld' in non_spec_info.keys():\n",
    "        theta_ld_model = (2*np.arctan((np.sqrt(c.G * c.M_sun / ((10**logg*u.cm).to(u.m)/u.s**2)) / (distance).to(u.m)))).to(u.mas)\n",
    "        logL_theta_ld = - 1/2 * ((theta_ld_model - non_spec_info['theta_ld'])**2/(non_spec_info['e_theta_ld']**2) + np.log(2*np.pi*non_spec_info['e_theta_ld']/u.mas))\n",
    "        if use_gaia_mags:\n",
    "            return(logL_theta_ld + logL_parallax + logL_jmag + logL_hmag + logL_ksmag + logL_gmag + logL_gbpmag + logL_grpmag)\n",
    "        else:\n",
    "            return(logL_theta_ld + logL_parallax + logL_jmag + logL_hmag + logL_ksmag + logL_gmag + logL_gbpmag + logL_grpmag)\n",
    "    else:\n",
    "        if use_gaia_mags:\n",
    "            return(logL_parallax + logL_jmag + logL_hmag + logL_ksmag + logL_gmag + logL_gbpmag + logL_grpmag)\n",
    "        else:\n",
    "            return(logL_parallax + logL_jmag + logL_hmag + logL_ksmag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log_likelihood(model_parameters, model_labels, spectrum, masks, non_spec_info, use_spectroscopy=True, use_photoastrometry=True, use_asteroseismology=True, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=False, debug=False):\n",
    "\n",
    "    combined_log_likelihood = 0.0\n",
    "    \n",
    "    if use_spectroscopy:\n",
    "        combined_log_likelihood += np.sum(spectroscopic_log_likelihood(model_parameters, model_labels, spectrum, masks, default_model, default_model_name, reuse_initial_res_wave_grid, debug=debug))\n",
    "\n",
    "    if use_photoastrometry:\n",
    "        combined_log_likelihood += photoastrometric_log_likelihood(model_parameters, model_labels, non_spec_info)\n",
    "        \n",
    "    if use_asteroseismology:\n",
    "        combined_log_likelihood += asteroseismic_log_likelihood(model_parameters, model_labels, non_spec_info)\n",
    "    \n",
    "    #print(combined_log_likelihood, model_parameters)\n",
    "    \n",
    "    return(combined_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log_prior(model_parameters, model_labels, use_spectroscopy=True, use_photoastrometry=True, use_asteroseismology=True):\n",
    "\n",
    "    model_parameters = np.array(model_parameters)\n",
    "    \n",
    "    teff = 1000.*model_parameters[model_labels=='teff'][0]\n",
    "    logg = model_parameters[model_labels=='logg'][0]\n",
    "    fe_h = model_parameters[model_labels=='fe_h'][0]\n",
    "    \n",
    "    if not (teff >= 3000.0 and teff <= 8000.0 and logg >= -0.5 and logg <= 5.5 and fe_h <= 1.0):# and vsini >= 0.0):\n",
    "        return -np.inf\n",
    "    \n",
    "    if 'vmic' in model_labels:\n",
    "        vmic = model_parameters[model_labels=='vmic'][0]\n",
    "        if not (vmic >= 0.0):\n",
    "            return -np.inf\n",
    "\n",
    "    if 'vsini' in model_labels:\n",
    "        vsini = model_parameters[model_labels=='vsini'][0]\n",
    "        if not (vsini >= 0.0):\n",
    "            return -np.inf\n",
    "        \n",
    "    if 'distance' in model_labels and use_photoastrometry:\n",
    "        distance = model_parameters[model_labels=='distance'][0]\n",
    "        if not (distance >= 0.0):\n",
    "            return -np.inf\n",
    "\n",
    "    if 'a_v' in model_labels and use_photoastrometry:\n",
    "        a_v = model_parameters[model_labels=='a_v'][0]\n",
    "        if not (a_v >= 0.0):\n",
    "            return -np.inf\n",
    "\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log_probability(model_parameters, model_labels, spectrum, masks, non_spec_info, use_spectroscopy = True, use_photoastrometry=True, use_asteroseismology=True, default_model=None, default_model_name=None, reuse_initial_res_wave_grid=False, debug=False):\n",
    "    \n",
    "    # Estimate Prior\n",
    "    lp = log_prior(model_parameters, model_labels, use_spectroscopy, use_photoastrometry, use_asteroseismology)\n",
    "    \n",
    "    # If prior kicks in\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    # Otherwise combine prior and likelihood\n",
    "    return lp + log_likelihood(model_parameters, model_labels, spectrum, masks, non_spec_info, use_spectroscopy, use_photoastrometry, use_asteroseismology, default_model, default_model_name, reuse_initial_res_wave_grid, debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Maximum Likelihood optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     47
    ]
   },
   "outputs": [],
   "source": [
    "initial_model_parameters = [\n",
    "    spectrum['init_vrad'],\n",
    "    spectrum['init_teff'],\n",
    "    spectrum['init_logg'],\n",
    "    spectrum['init_fe_h'],\n",
    "    spectrum['init_vmic'],\n",
    "    spectrum['init_vsini'],\n",
    "#     spectrum['init_li_fe'],\n",
    "#     spectrum['init_c_fe'],\n",
    "#     spectrum['init_n_fe'],\n",
    "#     spectrum['init_o_fe'],\n",
    "#     spectrum['init_na_fe'],\n",
    "#     spectrum['init_mg_fe'],\n",
    "#     spectrum['init_al_fe'],\n",
    "#     spectrum['init_si_fe'],\n",
    "#     spectrum['init_k_fe'],\n",
    "#     spectrum['init_ca_fe'],\n",
    "#     spectrum['init_sc_fe'],\n",
    "#     spectrum['init_ti_fe'],\n",
    "#     spectrum['init_v_fe'],\n",
    "#     spectrum['init_cr_fe'],\n",
    "#     spectrum['init_mn_fe'],\n",
    "#     spectrum['init_co_fe'],\n",
    "#     spectrum['init_ni_fe'],\n",
    "#     spectrum['init_cu_fe'],\n",
    "#     spectrum['init_zn_fe'],\n",
    "#     spectrum['init_rb_fe'],\n",
    "#     spectrum['init_sr_fe'],\n",
    "#     spectrum['init_y_fe'],\n",
    "#     spectrum['init_zr_fe'],\n",
    "#     spectrum['init_mo_fe'],\n",
    "#     spectrum['init_ru_fe'],\n",
    "#     spectrum['init_ba_fe'],\n",
    "#     spectrum['init_la_fe'],\n",
    "#     spectrum['init_ce_fe'],\n",
    "#     spectrum['init_nd_fe'],\n",
    "#     spectrum['init_sm_fe'],\n",
    "#     spectrum['init_eu_fe'],\n",
    "#     1000*spectrum['cdelt_ccd1'],\n",
    "#     1000*spectrum['cdelt_ccd2'],\n",
    "#     1000*spectrum['cdelt_ccd3'],\n",
    "#     1000*spectrum['cdelt_ccd4'],\n",
    "#     spectrum['crval_ccd1'],\n",
    "#     spectrum['crval_ccd2'],\n",
    "#     spectrum['crval_ccd3'],\n",
    "    spectrum['crval_ccd4']\n",
    "]\n",
    "model_labels = np.array([\n",
    "    'vrad',\n",
    "    'teff',\n",
    "    'logg',\n",
    "    'fe_h',\n",
    "    'vmic',\n",
    "    'vsini',\n",
    "#     'li_fe',\n",
    "#     'c_fe', 'n_fe', 'o_fe', 'na_fe', 'mg_fe',\n",
    "#     'al_fe', 'si_fe', 'k_fe', 'ca_fe', 'sc_fe',\n",
    "#     'ti_fe', 'v_fe', 'cr_fe', 'mn_fe', 'co_fe',\n",
    "#     'ni_fe', 'cu_fe', 'zn_fe', 'rb_fe', 'sr_fe',\n",
    "#     'y_fe', 'zr_fe', 'mo_fe', 'ru_fe', \n",
    "#     'ba_fe','la_fe','ce_fe','nd_fe','sm_fe','eu_fe',\n",
    "#     'cdelt1',\n",
    "#     'cdelt2',\n",
    "#     'cdelt3',\n",
    "#     'cdelt4',\n",
    "#     'crval1',\n",
    "#     'crval2',\n",
    "#     'crval3',\n",
    "    'crval4'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = []\n",
    "for s in range(100):\n",
    "    start = time.time()\n",
    "    log_p_s = np.sum(spectroscopic_log_likelihood(\n",
    "        initial_model_parameters, \n",
    "        model_labels,\n",
    "        spectrum, masks, default_model, default_model_name, reuse_initial_res_wave_grid=True, debug=False)\n",
    "        )\n",
    "    sample.append(time.time()-start)\n",
    "print('Computation time:')\n",
    "print(np.round(((np.median(np.array(sample))*u.s)).to(u.ms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_initial = plot_observation_and_model(\n",
    "    initial_model_parameters, \n",
    "    model_labels, spectrum, masks, default_model, default_model_name, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -log_likelihood(*args)\n",
    "\n",
    "start = time.time()\n",
    "Maximum_logL = op.minimize(\n",
    "    nll, \n",
    "    initial_model_parameters, \n",
    "    args=(\n",
    "        model_labels,\n",
    "        spectrum,\n",
    "        masks,\n",
    "        non_spec_info,\n",
    "        use_spectroscopy, #use_spectroscopy\n",
    "        use_photoastrometry, #use_photoastrometry\n",
    "        use_asteroseismology, #use_asteroseismology\n",
    "        default_model,# default_model\n",
    "        default_model_name, # default_model_name\n",
    "        True, # reuse_initial_res_wave_grid\n",
    "        False #debug=False\n",
    "    ),\n",
    "    options = dict(maxiter = 50)\n",
    ")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Table()\n",
    "\n",
    "file_directory = working_directory+'/analysis_products/fitting_output/'+str(spectrum['sobject_id'])[:6]+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for label in ['sobject_id','tmass_id','gaia_edr3_source_id']:\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = [spectrum[label]],\n",
    "        description=description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "\n",
    "for label_index, label in enumerate(model_labels):\n",
    "    if label in ['teff']:\n",
    "        out_data = 1000*np.float32(Maximum_logL.x[label_index])\n",
    "    elif label in ['cdelt1','cdelt2','cdelt3','cdelt4']:\n",
    "        out_data = 0.001*np.float32(Maximum_logL.x[label_index])\n",
    "    else:\n",
    "        out_data = np.float32(Maximum_logL.x[label_index])\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = [out_data],\n",
    "        description=description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "\n",
    "output.write(file_directory+str(spectrum['sobject_id'])+'_Maximum_logL_results.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = working_directory+'/analysis_products/fitting_output/'+str(spectrum['sobject_id'])[:6]+'/'\n",
    "output = Table.read(file_directory+str(spectrum['sobject_id'])+'_Maximum_logL_results.fits')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New let's update the best wavelength grid for the resolution interpolation\n",
    "\n",
    "(default_model_wave, default_model_flux, default_model_s2) = create_synthetic_spectrum(\n",
    "    Maximum_logL.x,\n",
    "    model_labels,\n",
    "    default_model,\n",
    "    default_model_name\n",
    ")\n",
    "default_model_wave = default_model._dispersion\n",
    "initial_l = calculate_default_degrading_wavelength_grid(default_model_wave,default_model_flux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maximum_logL.x - initial_model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(np.sum(spectroscopic_log_likelihood(\n",
    "    Maximum_logL.x, \n",
    "    model_labels,\n",
    "    spectrum, masks, default_model, default_model_name, debug=False)\n",
    "    ))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_initial = plot_observation_and_model(\n",
    "    initial_model_parameters, \n",
    "    model_labels, spectrum, masks, default_model, default_model_name, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "dict_minise = plot_observation_and_model(\n",
    "    Maximum_logL.x, \n",
    "    model_labels, spectrum, masks, default_model, default_model_name, True\n",
    ")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(\n",
    "    dict_initial['wave'],\n",
    "    dict_initial['model'],lw=0.5\n",
    ")\n",
    "plt.plot(\n",
    "    dict_minise['wave'],\n",
    "    dict_minise['data'],lw=0.5\n",
    ")\n",
    "plt.plot(\n",
    "    dict_minise['wave'],\n",
    "    dict_minise['model'],lw=0.5\n",
    ")\n",
    "plt.xlim(6500,6600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logg_sampling = np.linspace(\n",
    "    #1.5,5,17\n",
    "    3.0,4.5,21\n",
    ")\n",
    "\n",
    "log_p_spa = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = True,\n",
    "    use_photoastrometry = True,\n",
    "    use_asteroseismology = True,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_sp = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = True,\n",
    "    use_photoastrometry = True,\n",
    "    use_asteroseismology = False,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_sa = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = True,\n",
    "    use_photoastrometry = False,\n",
    "    use_asteroseismology = True,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_pa = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = False,\n",
    "    use_photoastrometry = True,\n",
    "    use_asteroseismology = True,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_s = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = True,\n",
    "    use_photoastrometry = False,\n",
    "    use_asteroseismology = False,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_p = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = False,\n",
    "    use_photoastrometry = True,\n",
    "    use_asteroseismology = False,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])\n",
    "\n",
    "log_p_a = np.array([\n",
    "    log_probability(\n",
    "    model_parameters = [\n",
    "        spectrum['init_vrad'],\n",
    "        spectrum['init_teff'],\n",
    "        logg,\n",
    "        spectrum['init_fe_h'],\n",
    "        spectrum['init_vmic'],\n",
    "        spectrum['init_vsini'],\n",
    "        ((1./non_spec_info['parallax'] * u.arcsec * u.pc).to(u.pc)).value,\n",
    "        0.0\n",
    "    ],\n",
    "    model_labels = np.array(['vrad','teff','logg','fe_h','vmic','vsini','distance','a_v']),\n",
    "    spectrum=spectrum,\n",
    "    masks=masks,\n",
    "    non_spec_info = non_spec_info,\n",
    "    use_spectroscopy = False,\n",
    "    use_photoastrometry = False,\n",
    "    use_asteroseismology = True,\n",
    "    reuse_initial_res_wave_grid = True\n",
    ") for logg in logg_sampling\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "logg_below_value = (logg_sampling <= 5)\n",
    "\n",
    "f, gs = plt.subplots(2,4,figsize=(12,8),sharex=True)\n",
    "\n",
    "ax = gs[0,0]\n",
    "ax.plot(\n",
    "    logg_sampling,\n",
    "    log_p_spa,\n",
    "    label='$\\ln(P)$ S+P+A'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylabel('$\\ln(P)$')\n",
    "\n",
    "ax = gs[0,1]\n",
    "ax.plot(\n",
    "    logg_sampling,\n",
    "    log_p_sp,\n",
    "    label='$\\ln(P)$ spec+photo'\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax = gs[0,2]\n",
    "ax.plot(\n",
    "    logg_sampling,\n",
    "    log_p_sa,\n",
    "    label='$\\ln(P)$ spec+astero'\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax = gs[0,3]\n",
    "ax.plot(\n",
    "    logg_sampling[logg_below_value],\n",
    "    log_p_pa[logg_below_value],\n",
    "    label='$\\ln(P)$ photo+astero'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel('$\\ln (g~/~\\mathrm{cm\\,s^{-2}})$')\n",
    "ax.set_ylabel('$\\ln(P)$')\n",
    "\n",
    "ax = gs[1,0]\n",
    "ax.plot(\n",
    "    logg_sampling,\n",
    "    log_p_s,\n",
    "    label='$\\ln(P)$ spec'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel('$\\ln (g~/~\\mathrm{cm\\,s^{-2}})$')\n",
    "ax.set_ylabel('$\\ln(P)$')\n",
    "\n",
    "ax = gs[1,1]\n",
    "ax.plot(\n",
    "    logg_sampling,\n",
    "    log_p_p,\n",
    "    label='$\\ln(P)$ photo'\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel('$\\ln (g~/~\\mathrm{cm\\,s^{-2}})$')\n",
    "\n",
    "ax = gs[1,2]\n",
    "ax.plot(\n",
    "    logg_sampling[logg_below_value],\n",
    "    log_p_a[logg_below_value],\n",
    "    label='$\\ln(P)$ astero'\n",
    ")\n",
    "ax.set_ylim(2*np.median(log_p_a[logg_below_value]),0)\n",
    "ax.legend()\n",
    "ax.set_xlabel('$\\ln (g~/~\\mathrm{cm\\,s^{-2}})$')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_steps = 50\n",
    "step_burnin = int(number_steps/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "log_probability(\n",
    "    Maximum_logL.x,\n",
    "    model_labels, spectrum, masks, non_spec_info, use_spectroscopy, use_photoastrometry, use_asteroseismology, default_model, default_model_name, reuse_initial_res_wave_grid=True, debug=False\n",
    ")\n",
    "print('full time: ',time.time()-start)\n",
    "#1.08s with reading in Cannon spectrum & debug\n",
    "#0.70s with reading in Cannon spectrum, without debug\n",
    "#0.69s without reading in Cannon spectrum, with debug\n",
    "#0.33s without reading in Cannon spectrum, without debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the walkers\n",
    "coords = Maximum_logL.x + 1e-4 * np.random.randn(5*len(model_labels), len(model_labels))\n",
    "nwalkers, ndim = coords.shape\n",
    "\n",
    "# Set up the backend\n",
    "# Don't forget to clear it in case the file already exists\n",
    "file_directory = working_directory+'/analysis_products/diagnostic_plots/'+str(spectrum['sobject_id'])[:6]+'/mcmc/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "backend = emcee.backends.HDFBackend(file_directory+str(spectrum['sobject_id'])+'_sample.h5')\n",
    "backend.reset(nwalkers, ndim)\n",
    "\n",
    "# Walk that walk\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, log_probability, args=(model_labels, spectrum, masks, non_spec_info, use_spectroscopy, use_photoastrometry, use_asteroseismology, default_model, default_model_name, True, False),\n",
    "    backend=backend\n",
    ")\n",
    "\n",
    "sampler.run_mcmc(coords, number_steps, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('checkout more: https://emcee.readthedocs.io/en/stable/tutorials/monitor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_n = 100000\n",
    "\n",
    "# # We'll track how the average autocorrelation time estimate changes\n",
    "# index = 0\n",
    "# autocorr = np.empty(max_n)\n",
    "\n",
    "# # This will be useful to testing convergence\n",
    "# old_tau = np.inf\n",
    "\n",
    "# # Now we'll sample for up to max_n steps\n",
    "# for sample in sampler.sample(coords, iterations=max_n, progress=True):\n",
    "#     # Only check convergence every 100 steps\n",
    "#     if sampler.iteration % 100:\n",
    "#         continue\n",
    "\n",
    "#     # Compute the autocorrelation time so far\n",
    "#     # Using tol=0 means that we'll always get an estimate even\n",
    "#     # if it isn't trustworthy\n",
    "#     tau = sampler.get_autocorr_time(tol=0)\n",
    "#     autocorr[index] = np.mean(tau)\n",
    "#     index += 1\n",
    "\n",
    "#     # Check convergence\n",
    "#     converged = np.all(tau * 100 < sampler.iteration)\n",
    "#     converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "#     if converged:\n",
    "#         break\n",
    "#     old_tau = tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_directory = working_directory+'/analysis_products/diagnostic_plots/'+str(spectrum['sobject_id'])[:6]+'/mcmc/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(len(model_labels), figsize=(10, 2*len(model_labels)), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.axvline(step_burnin)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(model_labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");\n",
    "plt.savefig(file_directory+'/mcmc_chain_'+str(spectrum['sobject_id'])+'.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tau = sampler.get_autocorr_time()\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tau = sampler.get_autocorr_time()\n",
    "    burnin = int(2 * np.max(tau))\n",
    "    thin = int(0.5 * np.min(tau))\n",
    "    samples = sampler.get_chain(discard=burnin, flat=True, thin=thin)\n",
    "    log_prob_samples = sampler.get_log_prob(discard=burnin, flat=True, thin=thin)\n",
    "    log_prior_samples = sampler.get_blobs(discard=burnin, flat=True, thin=thin)\n",
    "    \n",
    "    print(\"burn-in: {0}\".format(burnin))\n",
    "    print(\"thin: {0}\".format(thin))\n",
    "    print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "    print(\"flat log prob shape: {0}\".format(log_prob_samples.shape))\n",
    "    print(\"flat log prior shape: {0}\".format(log_prior_samples.shape))\n",
    "\n",
    "    all_samples = np.concatenate(\n",
    "        (samples, log_prob_samples[:, None], log_prior_samples[:, None]), axis=1\n",
    "    )\n",
    "\n",
    "    labels = list(map(r\"$\\theta_{{{0}}}$\".format, range(1, ndim + 1)))\n",
    "    labels += [\"log prob\", \"log prior\"]\n",
    "\n",
    "    corner.corner(all_samples, labels=labels);\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=step_burnin, thin=1, flat=True)\n",
    "print(flat_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file_directory+'samples_'+str(spectrum['sobject_id'])+'.npy',flat_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_directory = working_directory+'/analysis_products/diagnostic_plots/'+str(spectrum['sobject_id'])[:6]+'/mcmc/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=model_labels, truths=Maximum_logL.x\n",
    ");\n",
    "plt.savefig(file_directory+'/mcmc_corner_'+str(spectrum['sobject_id'])+'.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_mcmc = plot_observation_and_model(\n",
    "    np.median(flat_samples,axis=0), \n",
    "    model_labels, spectrum, masks, default_model, default_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Table()\n",
    "\n",
    "file_directory = working_directory+'/analysis_products/fitting_output/'+str(spectrum['sobject_id'])[:6]+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for label in ['sobject_id','tmass_id','gaia_edr3_source_id']:\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = [spectrum[label]],\n",
    "        description=description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "\n",
    "for label_index, label in enumerate(model_labels):\n",
    "    percentiles = np.percentile(flat_samples[:, label_index], [16, 50, 84])\n",
    "    if label == 'teff':\n",
    "        percentiles *= 1000.\n",
    "    if label[:5] == 'cdelt':\n",
    "        percentiles /= 1000.\n",
    "\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = [np.float32(percentiles[1])],\n",
    "        description=description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "    col = Table.Column(\n",
    "        name=label+'_16',\n",
    "        data = [np.float32(percentiles[1]-percentiles[0])],\n",
    "        description='Difference to 16th percentile of '+description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "    col = Table.Column(\n",
    "        name=label+'_84',\n",
    "        data = [np.float32(percentiles[2]-percentiles[1])],\n",
    "        description='Difference to 84th percentile of '+description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "\n",
    "output.write(file_directory+str(spectrum['sobject_id'])+'_logL_MCMC_results.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probability(\n",
    "    np.median(flat_samples,axis=0),\n",
    "    model_labels, spectrum, masks, non_spec_info, use_spectroscopy, use_photoastrometry, use_asteroseismology, default_model, default_model_name, False, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(flat_samples,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def fit_leastsq(initial_model_parameters, model_labels, spectrum, masks, debug=False):\n",
    "\n",
    "#     def residuals_for_leastsq(parameters):\n",
    "\n",
    "#         print(parameters)\n",
    "#         (wave,data,sigma2,model,s2) = match_observation_and_model(model_parameters=parameters, model_labels=model_labels, spectrum=spectrum, masks=masks, debug=False)\n",
    "#         masked = np.any(np.array([((wave >= mask_beginning) & (wave <= mask_end)) for (mask_beginning, mask_end) in masks]),axis=0)\n",
    "        \n",
    "#         adjusted_sigma = sigma2 + s2\n",
    "#         adjusted_sigma[masked] = 100000.\n",
    "        \n",
    "#         return (model - data)/np.sqrt(adjusted_sigma)\n",
    "    \n",
    "#     kwds = {\n",
    "#             \"func\": residuals_for_leastsq,\n",
    "#             \"Dfun\": None,\n",
    "#             \"col_deriv\": True,\n",
    "\n",
    "#             # These get passed through to leastsq:\n",
    "#             \"ftol\": 7./3 - 4./3 - 1, # Machine precision.\n",
    "#             \"xtol\": 7./3 - 4./3 - 1, # Machine precision.\n",
    "#             \"gtol\": 0.0,\n",
    "#             \"maxfev\": 100000, # MAGIC\n",
    "#             \"epsfcn\": None,\n",
    "#             \"factor\": 1.0,\n",
    "#         }\n",
    "\n",
    "#     op_labels, cov, meta, mesg, ier = op.leastsq(\n",
    "#         x0 = initial_model_parameters,\n",
    "#         full_output=True,**kwds\n",
    "#     )\n",
    "    \n",
    "#     return(op_labels, cov, meta, mesg, ier)\n",
    "\n",
    "# start_time = time.time()\n",
    "# initial_model_parameters = [\n",
    "#     spectrum['init_vrad'],\n",
    "#     spectrum['init_teff'],\n",
    "#     spectrum['init_logg'],\n",
    "#     spectrum['init_fe_h'],\n",
    "#     spectrum['init_vmic'],\n",
    "#     0.0,\n",
    "#     0.0\n",
    "# ]\n",
    "# model_labels=np.array(['vrad','teff','logg','fe_h','vmic','li_fe','k_fe'])\n",
    "# op_labels, cov, meta, mesg, ier = fit_leastsq(initial_model_parameters, model_labels, spectrum, masks, debug=False)\n",
    "# print(time.time() - start_time)\n",
    "# # 31s for 47 calls for 4 parameters\n",
    "# # 259s for 6 parameters\n",
    "# # 118s for 11 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(workflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
