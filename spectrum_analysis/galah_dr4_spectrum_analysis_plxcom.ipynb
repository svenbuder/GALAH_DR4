{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GALAH DR4 Spectrum Analysis assuming Single Source - coadded version that includes parallax information\n",
    "\n",
    "This script is used to find the optimal set of stellar labels for GALAH spectra in combination with non-spectroscopic information\n",
    "\n",
    "The code is maintained at\n",
    "https://github.com/svenbuder/GALAH_DR4\n",
    "and described at\n",
    "https://github.com/svenbuder/galah_dr4_paper\n",
    "\n",
    "Author(s): Sven Buder (ANU, ASTRO 3D)\n",
    "\n",
    "History:  \n",
    "220902 Created from code 'galah_dr4_spectrum_analysis_single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preamble\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "    %config Completer.use_jedi = False\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import pickle\n",
    "import scipy.interpolate\n",
    "from scipy import signal\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import Akima1DInterpolator,interp1d\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary for descriptions\n",
    "description = dict()\n",
    "description['sobject_id'] = 'GALAH identifier'\n",
    "description['tmass_id'] = '2MASS identifier'\n",
    "description['gaia_edr3_source_id'] = 'Gaia DR2 source_id'\n",
    "description['flag_sp'] = 'Spectroscopic quality flag'\n",
    "description['teff'] = 'Spectroscopic effective temperature'\n",
    "description['logg'] = 'Surface gravity'\n",
    "description['fe_h'] = 'Fe abundance [Fe/H] (1D-NLTE)'\n",
    "description['vmic'] = 'Microturbulence velocity'\n",
    "description['vsini'] = 'Rotational broadening velocity'\n",
    "for element in ['Li','C','N','O','Na','Mg','Al','Si','K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','La','Ce','Nd','Sm','Eu']:\n",
    "    if element in ['Li','C','N','O','Na','Mg','Al','Si','K','Ca','Mn','Ba']:\n",
    "        lte_nlte = 'NLTE'\n",
    "    else:\n",
    "        lte_nlte = 'LTE'        \n",
    "    description[element.lower()+'_fe'] = element+' abundance ['+element+'/Fe] (1D-'+lte_nlte+')'\n",
    "description['vrad'] = 'Radial velocity fitted from spectra'\n",
    "description['rv_gauss'] = 'Radial velocity fitted from spectra with Gaussian'\n",
    "for ccd in [1,2,3,4]:\n",
    "    description['cdelt'+str(ccd)] = 'Linear wavelength increase per pixel for CCD'+str(ccd)\n",
    "    description['crval'+str(ccd)] = 'Wavelength of first pixel for CCD'+str(ccd)\n",
    "description['distance'] = 'Distance from Sun to star'\n",
    "description['a_v'] = 'Extinction in V filter'\n",
    "description['model_name'] = 'Model name used for label optimisation'\n",
    "description['closest_model'] = 'Closest model needed for label optimisation'\n",
    "description['spectrum_covariances'] = 'Covariances from CurveFit to spectra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary for units\n",
    "units = dict()\n",
    "for each in description.keys():\n",
    "    units[each] = ''\n",
    "    if each in ['sobject_id','tmass_id','gaia_edr3_source_id','model_name','closest_model','spectrum_covariances','flag_sp']:\n",
    "        pass\n",
    "    elif each in ['teff']:\n",
    "        units[each] = 'K'\n",
    "    elif each in ['logg']:\n",
    "        units[each] = 'log(cm.s**-2)'\n",
    "    elif each in ['fe_h']:\n",
    "        units[each] = 'dex'\n",
    "    elif each in ['vmic','vsini','vrad','rv_gauss','e_rv_gauss']:\n",
    "        units[each] = 'km s-1'\n",
    "    elif each[-3:] == '_fe':\n",
    "        units[each] = 'dex'\n",
    "    elif each in ['distance']:\n",
    "        units[each] = 'pc'\n",
    "    elif each in ['a_v']:\n",
    "        units[each] = 'mag'\n",
    "    elif each[:5] in ['cdelt']:\n",
    "        units[each] = 'Angstroem/pix'\n",
    "    elif each[:5] in ['crval']:\n",
    "        units[each] = 'Angstroem'\n",
    "    else:\n",
    "        print(\"'\"+each+\"',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "galah_dr4_directory = os.path.abspath(os.getcwd()+'/../')+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeat_table = Table.read(galah_dr4_directory+'observations/dr6.0_230101.fits')\n",
    "init_values_table = Table.read('galah_dr4_initial_parameters_230101_lite.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if sys.argv[1] != '-f':\n",
    "    tmass_id = str(sys.argv[1])\n",
    "else:\n",
    "    tmass_id = 'VESTA' # VESTA\n",
    "#     tmass_id = '23595997-5257480'\n",
    "#     tmass_id = '00000011+0522500'\n",
    "#     tmass_id = '00000024-5742487'\n",
    "#     tmass_id = '00000023-5709445'\n",
    "#     tmass_id = '00000025-7541166'\n",
    "    \n",
    "#     tmass_id = '00254416-7715157' # bet Hyi\n",
    "#     tmass_id = '01440402-1556141' # tau Cet\n",
    "#     tmass_id = '03013762-2805289' # eps For\n",
    "#     tmass_id = '03021680+0405226' # alf Cet\n",
    "#     tmass_id = '03325591-0927298' # eps Eri\n",
    "#     tmass_id = '03402202-0313005' # HD 22879\n",
    "#     tmass_id = '03431490-0945490' # del Eri\n",
    "#     tmass_id = '04355524+1630331' # alf Tau\n",
    "#     tmass_id = '06504983-0032270' # HD 49933\n",
    "#     tmass_id = '09485609+1344395' # HD 84937\n",
    "#     tmass_id = '09524585+2600248' # mu Leo\n",
    "#     tmass_id = '11330013-3151273' # ksi Hya\n",
    "#     tmass_id = '12202099+0318453' # HD 107328\n",
    "#     tmass_id = '13544106+1823514' # eta Boo\n",
    "    tmass_id = '14153968+1910558' # Arcturus\n",
    "#     tmass_id = '15430307-1056009' # HD 140283\n",
    "#     tmass_id = '16153726-0822096' # 18 Sco\n",
    "#     tmass_id = '17440870-5150027' # mu Ara\n",
    "#     tmass_id = 'gam Sge' # gam Sge\n",
    "#     tmass_id = '04060261-6430120' # Random star with Teff/logg/fe_h 6000 3.5 -0.25\n",
    "\n",
    "    tmass_id = '07372740-5815503'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrum = dict()\n",
    "spectrum['tmass_id'] = tmass_id\n",
    "\n",
    "spectrum['sobject_ids'] = list(repeat_table['sobject_id'][np.where(repeat_table['tmass_id'] == spectrum['tmass_id'])[0]])\n",
    "spectrum['sobject_id'] = spectrum['sobject_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neglect spectra observed with high-resolution\n",
    "initial_list = list(spectrum['sobject_ids'])\n",
    "for ind_sobject_id in initial_list:\n",
    "    index_init_table = np.where(init_values_table['sobject_id'] == ind_sobject_id)[0]\n",
    "    if len(index_init_table) > 0:\n",
    "        index_init_table = index_init_table[0]\n",
    "        if (init_values_table['reduction_flags'][index_init_table] >= 2**18) & (init_values_table['reduction_flags'][index_init_table] < 2**19):\n",
    "            spectrum['sobject_ids'].remove(ind_sobject_id)\n",
    "            if sys.argv[1] == '-f':\n",
    "                print(str(ind_sobject_id)+' observed at high resolution. Dropping this one for the plxcom setup')\n",
    "        elif ind_sobject_id in [131217003901033,141231005201174,140207003801201,140207004801201,140208005101201,140208005101210,140209004901151,140209004901160,140314005201099,140315002501099]: # 140710000801284\n",
    "            spectrum['sobject_ids'].remove(ind_sobject_id)\n",
    "            if sys.argv[1] == '-f':\n",
    "                print('Manually removed spectrum '+str(ind_sobject_id))\n",
    "        else:\n",
    "            print(str(ind_sobject_id))\n",
    "    else:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print(str(ind_sobject_id)+' no entry found')\n",
    "        spectrum['sobject_ids'].remove(ind_sobject_id)\n",
    "        \n",
    "if len(spectrum['sobject_ids'])==0:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('\\nEither no useful spectrum available, or all at high-res. Testing 2nd case')\n",
    "    spectrum['sobject_ids'] = initial_list\n",
    "    for ind_sobject_id in initial_list:\n",
    "        index_init_table = np.where(init_values_table['sobject_id'] == ind_sobject_id)[0]\n",
    "        if len(index_init_table) > 0:\n",
    "            index_init_table = index_init_table[0]\n",
    "            if init_values_table['reduction_flags'][index_init_table] == 2**18:\n",
    "                if sys.argv[1] == '-f':\n",
    "                    print(str(ind_sobject_id)+' observed at high resolution.')\n",
    "            else:\n",
    "                if sys.argv[1] == '-f':\n",
    "                    print(str(ind_sobject_id))\n",
    "        else:\n",
    "            print(str(ind_sobject_id)+' no entry found')\n",
    "            spectrum['sobject_ids'].remove(ind_sobject_id)\n",
    "    print(len(spectrum['sobject_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrum['sobject_ids'].sort()\n",
    "spectrum['mjds'] = np.array(repeat_table['mjd'][np.where(repeat_table['tmass_id'] == spectrum['tmass_id'])[0]],dtype=float)\n",
    "\n",
    "sobject_id_initial_index = np.where(init_values_table['sobject_id'] == spectrum['sobject_ids'][0])[0]\n",
    "if len(sobject_id_initial_index) > 0:\n",
    "    if len(sobject_id_initial_index) > 1:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Warn: More than one entry in initial parameter table for '+str(spectrum['sobject_id']))\n",
    "    sobject_id_initial_index = sobject_id_initial_index[0]\n",
    "else:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('No initial values found in 230101_lite catalogue.')\n",
    "\n",
    "try:\n",
    "    spectrum['gaia_edr3_source_id'] = np.int64(init_values_table['source_id'][sobject_id_initial_index])\n",
    "\n",
    "except:\n",
    "    spectrum['gaia_edr3_source_id'] = np.int64(-1)\n",
    "\n",
    "spectrum['ebv'] = float(init_values_table['ebv'][sobject_id_initial_index].clip(min=0.0,max=0.72))\n",
    "\n",
    "success = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_info_all = Table.read('../auxiliary_information/dr60_230101_ebv_wise_tmass_gaiadr3corr_xmatch.fits', format='fits')\n",
    "extra_info_match = np.where(extra_info_all['sobject_id'] == spectrum['sobject_id'])[0]\n",
    "if spectrum['sobject_id'] == 140808000901102:\n",
    "    extra_info_match = np.where(extra_info_all['sobject_id'] == 140208005101201)[0]\n",
    "if spectrum['sobject_id'] == 210115002201239: # No matches for VESTA etc.\n",
    "    extra_info_match = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_info = dict()\n",
    "\n",
    "if len(extra_info_match) > 0:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Found extra information for star')\n",
    "    for key in [\n",
    "        'sobject_id','phot_g_mean_mag','phot_bp_mean_mag','bp_rp',\n",
    "        'h_m','h_msigcom', 'ks_m', 'ks_msigcom', 'W2mag', 'e_W2mag',\n",
    "        'parallax', 'radial_velocity','radial_velocity_error','ebv'\n",
    "    ]:\n",
    "        extra_info[key] = extra_info_all[key][extra_info_match[0]]\n",
    "        if key == 'parallax':\n",
    "            extra_info['e_'+key] = extra_info_all[key+'_error'][extra_info_match[0]]\n",
    "            extra_info['parallax_gaia_edr3'] = extra_info_all[key][extra_info_match[0]]\n",
    "            extra_info['e_parallax_gaia_edr3'] = extra_info_all[key+'_error'][extra_info_match[0]]\n",
    "\n",
    "    extra_info['rv_gaia_dr3'] = extra_info_all['radial_velocity'][extra_info_match[0]]\n",
    "    extra_info['e_rv_gaia_dr3'] = extra_info_all['radial_velocity_error'][extra_info_match[0]]\n",
    "    extra_info['ruwe_gaia_dr3'] = extra_info_all['ruwe'][extra_info_match[0]]\n",
    "\n",
    "    if np.isfinite(extra_info_all['r_med_photogeo'][extra_info_match[0]]):\n",
    "        extra_info['r_med'] = extra_info_all['r_med_photogeo'][extra_info_match[0]]\n",
    "        extra_info['r_lo'] = extra_info_all['r_lo_photogeo'][extra_info_match[0]]\n",
    "        extra_info['r_hi'] = extra_info_all['r_hi_photogeo'][extra_info_match[0]]\n",
    "    elif np.isfinite(extra_info_all['r_med_geo'][extra_info_match[0]]):\n",
    "        extra_info['r_med'] = extra_info_all['r_med_geo'][extra_info_match[0]]\n",
    "        extra_info['r_lo'] = extra_info_all['r_lo_geo'][extra_info_match[0]]\n",
    "        extra_info['r_hi'] = extra_info_all['r_hi_geo'][extra_info_match[0]]\n",
    "    elif np.isfinite(extra_info_all['parallax'][extra_info_match[0]]):\n",
    "        extra_info['r_med'] = 1000. /extra_info['parallax']\n",
    "        extra_info['r_lo'] = 1000. /(extra_info['parallax']+extra_info['e_parallax'])\n",
    "        extra_info['r_hi'] = 1000. /(extra_info['parallax']-extra_info['e_parallax'])\n",
    "\n",
    "    if (np.isfinite(extra_info['h_m']) & np.isfinite(extra_info['W2mag'])):\n",
    "        extra_info['a_ks'] = (0.918 * (extra_info['h_m'] - extra_info['W2mag'] - 0.08)).clip(min=0.00,max=0.50)\n",
    "        print('A(Ks)  via E(B-V) = '+\"{:.2f}\".format(spectrum['ebv'])+': '+\"{:.2f}\".format(0.36 * spectrum['ebv']))\n",
    "        print('A(Ks) from RJCE (H & W2): '+\"{:.2f}\".format(extra_info['a_ks'])+' +- '+\"{:.2f}\".format(np.sqrt(extra_info_all['h_msigcom'][extra_info_match[0]]**2+extra_info_all['e_W2mag'][extra_info_match[0]]**2)))\n",
    "        if (extra_info_all['tmass_ph_qual'][extra_info_match[0]][1] != 'A') | (extra_info_all['qph'][extra_info_match[0]][1] != 'A'):\n",
    "            print('RJCE Quality not good ('+str(extra_info_all['tmass_ph_qual'][extra_info_match[0]][1])+' & '+str(extra_info_all['qph'][extra_info_match[0]][1])+'), using 0.36 * E(B-V) instead')\n",
    "            if np.isfinite(spectrum['ebv']):\n",
    "                extra_info['a_ks'] = 0.36*spectrum['ebv']\n",
    "                if extra_info['a_ks'] < 0:\n",
    "                    extra_info['a_ks'] = 0.0\n",
    "                elif extra_info['a_ks'] > 0.5:\n",
    "                    extra_info['a_ks'] = 0.5\n",
    "            else:\n",
    "                extra_info['a_ks'] = 0.00 # No measurement, so let's assume 0\n",
    "    else:\n",
    "        print('No finite H or W2; using E(B-V)')\n",
    "        if np.isfinite(spectrum['ebv']):\n",
    "            extra_info['a_ks'] = 0.36*spectrum['ebv']\n",
    "            if extra_info['a_ks'] < 0:\n",
    "                extra_info['a_ks'] = 0.0\n",
    "            elif extra_info['a_ks'] > 0.5:\n",
    "                extra_info['a_ks'] = 0.5\n",
    "        else:\n",
    "            extra_info['a_ks'] = 0.00 # No measurement, so let's assume 0\n",
    "    run_star = True\n",
    "    extra_info['age'] = 4.5\n",
    "else:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Found no extra information for star, should not run this star')\n",
    "    run_star = False\n",
    "    \n",
    "if spectrum['sobject_id'] in [140710008301032,131220004401099,140207004801201]:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Adjusting Ks magnitude')\n",
    "    if spectrum['sobject_id'] == 140710008301032:\n",
    "        extra_info['ks_m'] = 1.43 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.02 # * u.mag\n",
    "    if spectrum['sobject_id'] == 131220004401099:\n",
    "        extra_info['ks_m'] = 1.46 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.03 # * u.mag\n",
    "    if spectrum['sobject_id'] == 140207004801201:\n",
    "        extra_info['ks_m'] = 2.20 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.01 # * u.mag\n",
    "\n",
    "if spectrum['sobject_id'] in [210115002201239,150210005801171,140710006601104,140709004401117,140708005801203,141102003801353,140710000801284,140709001901194]:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Special star (will add extra information manually)')\n",
    "    \n",
    "    extra_info['sobject_id'] = spectrum['sobject_id']\n",
    "    if spectrum['sobject_id'] == 210115002201239:\n",
    "        extra_info['ks_m'] = 3.28 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.02 # * u.mag\n",
    "        extra_info['parallax'] = 100.0 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.1 # * u.mas\n",
    "    if spectrum['sobject_id'] == 150210005801171:\n",
    "        extra_info['ks_m'] = -3.00 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.03 # * u.mag\n",
    "        extra_info['parallax'] = 88.83 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.54 # * u.mas\n",
    "    if spectrum['sobject_id'] == 140710006601104:\n",
    "        extra_info['ks_m'] = -1.68 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.05 # * u.mag\n",
    "        extra_info['parallax'] = 13.09 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.44 # * u.mas\n",
    "    if spectrum['sobject_id'] == 140709004401117:\n",
    "        extra_info['ks_m'] = -0.16 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.04 # * u.mag\n",
    "        extra_info['parallax'] = 12.62 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.18 # * u.mas\n",
    "    if spectrum['sobject_id'] == 140708005801203:\n",
    "        extra_info['parallax'] = 134.07 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.11 # * u.mas\n",
    "    if spectrum['sobject_id'] == 141102003801353:\n",
    "        extra_info['ks_m'] = -2.84 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.06 # * u.mag\n",
    "        extra_info['parallax'] = 48.94 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.77 # * u.mas\n",
    "    if spectrum['sobject_id'] == 140710000801284:\n",
    "        extra_info['ks_m'] = 2.20 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.01 # * u.mag\n",
    "        extra_info['parallax'] = 9.705958463334975 # * u.mas\n",
    "        extra_info['e_parallax'] = 0.15301941 # * u.mas\n",
    "    if spectrum['sobject_id'] == 140709001901194:\n",
    "        extra_info['ks_m'] = 1.36 # * u.mag\n",
    "        extra_info['ks_msigcom'] = 0.02 # * u.mag\n",
    "        extra_info['parallax'] = 87.75 # * u.mas\n",
    "        extra_info['e_parallax'] = 1.24 # * u.mas  \n",
    "    \n",
    "    extra_info['r_med'] = 1000. /extra_info['parallax']\n",
    "    extra_info['r_lo'] = 1000. /(extra_info['parallax']+extra_info['e_parallax'])\n",
    "    extra_info['r_hi'] = 1000. /(extra_info['parallax']-extra_info['e_parallax'])\n",
    "    \n",
    "    spectrum['ebv'] = 0.0\n",
    "    extra_info['a_ks'] = 0.0\n",
    "    \n",
    "    for key in ['rv_gaia_dr3','e_rv_gaia_dr3','ebv','ruwe_gaia_dr3']:\n",
    "        extra_info[key] = np.NaN\n",
    "        \n",
    "    run_star = True\n",
    "\n",
    "if extra_info['parallax'] > 10.:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Star within 100 pc, nulling E(B-V) and A(Ks)')\n",
    "    spectrum['ebv'] = 0.0\n",
    "    extra_info['a_ks'] = 0.0\n",
    "\n",
    "if extra_info['a_ks'] > 2 * 0.36 * spectrum['ebv']:\n",
    "    extra_info['a_ks'] = 0.36 * spectrum['ebv']\n",
    "    print('A(Ks) estimate 2 times higher than 0.36 * E(B-V). Lowering to that 0.36 * E(B-V): '+\"{:.2f}\".format(extra_info['a_ks']))    \n",
    "\n",
    "if  spectrum['ebv'] > 2 * extra_info['a_ks'] / 0.36:\n",
    "    spectrum['ebv'] = 2.78 * extra_info['a_ks']\n",
    "    print('E(B-V) estimate 2 times higher than A(Ks)/0.36. Lowering to A(Ks)/0.36: '+\"{:.2f}\".format(spectrum['ebv']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check entries in open cluster catalog by Cantat-Gaudin et al., 2020, A&A 640, 1\n",
    "cantatgaudin2020_parallaxes = Table.read('../auxiliary_information/CantatGaudin_2020_AandA_640_1.fits')\n",
    "cantatgaudin2020_match = np.where(spectrum['gaia_edr3_source_id'] == cantatgaudin2020_parallaxes['GaiaDR2'])[0]\n",
    "# If there is an entry in this catalog\n",
    "if len(cantatgaudin2020_match) > 0:\n",
    "    extra_info['parallax_cg2020'] = cantatgaudin2020_parallaxes['plx'][cantatgaudin2020_match[0]] # * u.mas\n",
    "    extra_info['e_parallax_cg2020'] = cantatgaudin2020_parallaxes['e_plx'][cantatgaudin2020_match[0]] # * u.mas\n",
    "    # replace parallax to be used, if Cantat-Gaudin et al. parallax has smaller uncertainty\n",
    "    if extra_info['e_parallax_cg2020'] < extra_info['e_parallax']:\n",
    "        print('Open cluster entry by Cantat-Gaudin et al. (2020) more precise')\n",
    "        extra_info['parallax'] = extra_info['parallax_cg2020']\n",
    "        extra_info['e_parallax'] = extra_info['e_parallax_cg2020']\n",
    "        extra_info['r_med'] = 1000. /extra_info['parallax']\n",
    "        extra_info['r_lo'] = 1000. /(extra_info['parallax']+extra_info['e_parallax'])\n",
    "        extra_info['r_hi'] = 1000. /(extra_info['parallax']-extra_info['e_parallax'])\n",
    "    else:\n",
    "        print('Open cluster entry by Cantat-Gaudin et al. (2020) less precise')\n",
    "        print(r'Gaia EDR3:                   $'+\"{:.3f}\".format(extra_info['parallax'])+' \\pm '+\"{:.3f}\".format(extra_info['e_parallax'])+'$')\n",
    "        print(r'Cantat-Gaudin et al. (2020): $'+\"{:.3f}\".format(extra_info['parallax_cg2020'])+' \\pm '+\"{:.3f}\".format(extra_info['e_parallax_cg2020'])+'$')\n",
    "    extra_info['age'] = np.round(10**(cantatgaudin2020_parallaxes['logAge'][cantatgaudin2020_match[0]]-9),2)\n",
    "    print('Open cluster age / Gyr: ',extra_info['age'])\n",
    "else:\n",
    "    print('No entry in Cantat-Gaudin et al. (2020) found')\n",
    "\n",
    "if spectrum['sobject_id'] == 140307003101118:\n",
    "    spectrum['gaia_edr3_source_id'] = 6083713495698719744\n",
    "    for key in ['parallax','e_parallax','r_med','r_lo','r_hi']:\n",
    "        extra_info[key] = np.NaN\n",
    "    extra_info['j_m'] = 11.231\n",
    "    extra_info['j_msigcom'] = 0.033\n",
    "    extra_info['h_m'] = 10.774\n",
    "    extra_info['h_msigcom'] = 0.046\n",
    "    extra_info['ks_m'] = 10.599\n",
    "    extra_info['ks_msigcom'] = 0.030\n",
    "    \n",
    "if spectrum['sobject_id'] == 140808004201212:\n",
    "    spectrum['gaia_edr3_source_id'] = 4689830994171444096\n",
    "    for key in ['parallax','e_parallax','r_med','r_lo','r_hi']:\n",
    "        extra_info[key] = np.NaN\n",
    "    extra_info['j_m'] = 11.985\n",
    "    extra_info['j_msigcom'] = 0.029\n",
    "    extra_info['h_m'] = 11.512\n",
    "    extra_info['h_msigcom'] = 0.032\n",
    "    extra_info['ks_m'] = 11.434\n",
    "    extra_info['ks_msigcom'] = 0.030\n",
    "    \n",
    "if spectrum['sobject_id'] == 140303002001034:\n",
    "    spectrum['gaia_edr3_source_id'] = 6083511804012335872\n",
    "    for key in ['parallax','e_parallax','r_med','r_lo','r_hi']:\n",
    "        extra_info[key] = np.NaN\n",
    "    extra_info['j_m'] = 12.528\n",
    "    extra_info['j_msigcom'] = 0.041\n",
    "    extra_info['h_m'] = 12.111\n",
    "    extra_info['h_msigcom'] = 0.058\n",
    "    extra_info['ks_m'] = 11.927\n",
    "    extra_info['ks_msigcom'] = 0.042   \n",
    "    \n",
    "if spectrum['sobject_id'] == 140305003201024:\n",
    "    spectrum['gaia_edr3_source_id'] = 6083515033827932800\n",
    "    for key in ['parallax','e_parallax','r_med','r_lo','r_hi']:\n",
    "        extra_info[key] = np.NaN\n",
    "    extra_info['j_m'] = 11.698\n",
    "    extra_info['j_msigcom'] = 0.033\n",
    "    extra_info['h_m'] = 11.225\n",
    "    extra_info['h_msigcom'] = 0.035\n",
    "    extra_info['ks_m'] = 11.078\n",
    "    extra_info['ks_msigcom'] = 0.033\n",
    "    \n",
    "if spectrum['sobject_id'] == 210803002201013:\n",
    "    spectrum['gaia_edr3_source_id'] = 6045465918541687296\n",
    "    for key in ['parallax','e_parallax','r_med','r_lo','r_hi']:\n",
    "        extra_info[key] = np.NaN\n",
    "\n",
    "# Check entries in open cluster catalog by Cantat-Gaudin et al., 2020, A&A 640, 1\n",
    "vasiliev2021_parallaxes = Table.read('../auxiliary_information/VasilievBaumgardt_2021_MNRAS_505_5978_cluster_source_id_memberprob0p7.fits')\n",
    "vasiliev2021_match = np.where(spectrum['gaia_edr3_source_id'] == vasiliev2021_parallaxes['source_id'])[0]    \n",
    "\n",
    "# If there is an entry in this catalog\n",
    "if len(vasiliev2021_match) > 0:\n",
    "    print('Match found in '+vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]])\n",
    "    # replace parallax to be used, if Cantat-Gaudin et al. parallax has smaller uncertainty\n",
    "    \n",
    "    globular_clusters = Table.read('../auxiliary_information/GlobularClustersGALAHDR4.fits')\n",
    "    correct_cluster = np.where(globular_clusters['Cluster'] == vasiliev2021_parallaxes['cluster'][vasiliev2021_match[0]])[0]\n",
    "    if len(correct_cluster) > 0:\n",
    "        correct_cluster = globular_clusters[correct_cluster[0]]\n",
    "        extra_info['parallax_vb21'] = correct_cluster['parallax']\n",
    "        extra_info['e_parallax_vb21'] = correct_cluster['e_parallax']\n",
    "        extra_info['r_med_vb21'] = correct_cluster['r_med']\n",
    "        extra_info['r_lo_vb21'] = correct_cluster['r_lo']\n",
    "        extra_info['r_hi_vb21'] = correct_cluster['r_hi']\n",
    "    else:\n",
    "        raise ValueError('No extra information for Globular Cluster in auxiliary_information/GlobularClustersGALAHDR4.fits')\n",
    "    \n",
    "    if extra_info['e_parallax_vb21'] < extra_info['e_parallax']:\n",
    "        print('Globular cluster entry by Vasiliev & Baumgardt (2021) more precise for plx (and better for distance)')\n",
    "    else:\n",
    "        print('Globular cluster entry by Vasiliev & Baumgardt (2021) less precise for plx (but better for distance):')\n",
    "\n",
    "    print(r'Gaia EDR3:                   $'+\"{:.3f}\".format(extra_info['parallax'])+' \\pm '+\"{:.3f}\".format(extra_info['e_parallax'])+'$, R / pc = ',\"{:.0f}\".format(extra_info['r_med']),\"{:.0f}\".format(extra_info['r_lo']-extra_info['r_med']),\"{:.0f}\".format(extra_info['r_hi']-extra_info['r_med']))\n",
    "    print(r'Vasiliev & Baumgardt (2021): $'+\"{:.3f}\".format(extra_info['parallax_vb21'])+' \\pm '+\"{:.3f}\".format(extra_info['e_parallax_vb21'])+'$, R / pc = ',\"{:.0f}\".format(extra_info['r_med_vb21']),\"{:.0f}\".format(extra_info['r_lo_vb21']-extra_info['r_med_vb21']),\"{:.0f}\".format(extra_info['r_hi_vb21']-extra_info['r_med_vb21']))\n",
    "    extra_info['parallax'] = extra_info['parallax_vb21']\n",
    "    extra_info['e_parallax'] = extra_info['e_parallax_vb21']\n",
    "    extra_info['r_med'] = extra_info['r_med_vb21']\n",
    "    extra_info['r_lo'] = extra_info['r_lo_vb21']\n",
    "    extra_info['r_hi'] = extra_info['r_hi_vb21']\n",
    "\n",
    "else:\n",
    "    print('No entry in Vasiliev & Baumgardt (2021) found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in single star analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_results = dict()\n",
    "single_results['sobject_id'] = []\n",
    "single_results['rv_gauss'] = []\n",
    "single_results['e_rv_gauss'] = []\n",
    "single_results['mjd'] = []\n",
    "single_results['flag_sp_fit'] = []\n",
    "single_results['teff'] = []\n",
    "single_results['logg'] = []\n",
    "single_results['fe_h'] = []\n",
    "single_results['vmic'] = []\n",
    "single_results['vsini'] = []\n",
    "single_results['e_vsini'] = []\n",
    "\n",
    "for sobject_id,mjd in zip(spectrum['sobject_ids'],spectrum['mjds']):\n",
    "    try:\n",
    "        results = Table.read(galah_dr4_directory+'analysis_products/'+str(sobject_id)[:6]+'/'+str(sobject_id)+'/'+str(sobject_id)+'_single_fit_results.fits')\n",
    "        single_results['sobject_id'].append(sobject_id)\n",
    "        single_results['rv_gauss'].append(results['rv_gauss'][0])\n",
    "        single_results['e_rv_gauss'].append(results['e_rv_gauss'][0])\n",
    "        single_results['mjd'].append(mjd)\n",
    "        single_results['flag_sp_fit'].append(results['flag_sp_fit'][0])\n",
    "        single_results['teff'].append(results['teff'][0])\n",
    "        single_results['logg'].append(results['logg'][0])\n",
    "        single_results['fe_h'].append(results['fe_h'][0])\n",
    "        single_results['vmic'].append(results['vmic'][0])\n",
    "        single_results['vsini'].append(results['vsini'][0])\n",
    "        single_results['e_vsini'].append(results['cov_e_vsini'][0])\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Read in '+str(sobject_id))\n",
    "    except:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Could not read in '+str(sobject_id))\n",
    "        pass\n",
    "    \n",
    "for label in ['rv_gauss','e_rv_gauss','mjd','flag_sp_fit','teff','logg','fe_h','vmic','vsini','e_vsini']:\n",
    "    single_results[label] = np.array(single_results[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our initial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrum['flag_sp'] = np.int32(0)\n",
    "flag_sp_closest_3x3x3_model_not_available = np.int32(1)\n",
    "flag_sp_closest_extra6_model_not_available = np.int32(2)\n",
    "flag_sp_no_successful_convergence_within_maximum_loops = np.int32(4)\n",
    "flag_sp_not_all_ccds_available = np.int32(8)\n",
    "flag_sp_negative_fluxes_in_ccds = np.int32(16)\n",
    "flag_sp_negative_resolution_profile = np.int32(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unflagged_single_measurements = (single_results['flag_sp_fit'] == 0)\n",
    "len_unflagged = len(single_results['flag_sp_fit'][unflagged_single_measurements])\n",
    "\n",
    "available_single_measurements = (single_results['flag_sp_fit'] < 128)\n",
    "len_available = len(single_results['flag_sp_fit'][available_single_measurements])\n",
    "\n",
    "if len_unflagged > 0:\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(str(len_unflagged)+' unflagged single measurement(s) available.')\n",
    "    use_for_starting_values = unflagged_single_measurements\n",
    "else:\n",
    "    if len_available > 0:\n",
    "        print('No unflagged single measurement(s) available, but '+str(len_available)+' flagged ones.')\n",
    "        use_for_starting_values = available_single_measurements\n",
    "    else:\n",
    "        raise ValueError('No measurements available at all!')\n",
    "\n",
    "rv_mean,rv_sigma = (np.mean(single_results['rv_gauss'][use_for_starting_values]),np.std(single_results['rv_gauss'][use_for_starting_values]))\n",
    "if sys.argv[1] == '-f':\n",
    "    print(r'RV (mean ± sigma) = '+\"{:.2f}\".format(rv_mean)+r' ± '+\"{:.2f}\".format(rv_sigma)+' km/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, gs = plt.subplots(1,3,figsize=(9,2.5))\n",
    "\n",
    "ax = gs[0]\n",
    "ax.set_title('GALAH DR4: '+str(spectrum['sobject_id']))\n",
    "if np.isfinite(rv_mean):\n",
    "    ax.axhspan(\n",
    "        rv_mean - rv_sigma,\n",
    "        rv_mean + rv_sigma,\n",
    "        alpha=0.15,zorder=1,color='grey'\n",
    "    )\n",
    "    ax.axhline(\n",
    "        rv_mean,zorder=3,color='grey',label=r'$\\mu \\pm \\sigma(v_\\mathrm{rad})$'\n",
    "    )\n",
    "    \n",
    "if np.isfinite(extra_info['rv_gaia_dr3']):\n",
    "    ax.axhspan(\n",
    "        extra_info['rv_gaia_dr3']-extra_info['e_rv_gaia_dr3'],\n",
    "        extra_info['rv_gaia_dr3']+extra_info['e_rv_gaia_dr3'],\n",
    "        alpha=0.25,zorder=2\n",
    "    )\n",
    "    ax.axhline(\n",
    "        extra_info['rv_gaia_dr3'],zorder=4,label='$Gaia$ DR3'\n",
    "    )\n",
    "    ax.text(0.5,0.9,'$Gaia$ DR3 RUWE: '+\"{:.2f}\".format(extra_info['ruwe_gaia_dr3']),transform=ax.transAxes,ha='center')\n",
    "\n",
    "if (len_available > 0) & (len_available != len_unflagged):\n",
    "    ax.errorbar(\n",
    "        single_results['mjd'][available_single_measurements],\n",
    "        single_results['rv_gauss'][available_single_measurements],\n",
    "        yerr = 10*np.array(single_results['e_rv_gauss'][available_single_measurements]),\n",
    "        ms=3,c='grey',capsize=3,lw=1,\n",
    "        fmt='o',zorder=5,label='flagged'\n",
    "    )\n",
    "if len_unflagged > 0:\n",
    "    ax.errorbar(\n",
    "        single_results['mjd'][unflagged_single_measurements],\n",
    "        single_results['rv_gauss'][unflagged_single_measurements],\n",
    "        yerr = 10*np.array(single_results['e_rv_gauss'][unflagged_single_measurements]),\n",
    "        ms=3,c='k',capsize=3,lw=1,\n",
    "        fmt='o',zorder=6,label='unflagged'\n",
    "    )\n",
    "ax.legend(handlelength=0.5,fontsize=8)\n",
    "ax.yaxis.set_major_locator(MaxNLocator(4)) \n",
    "ax.xaxis.set_major_locator(MaxNLocator(4)) \n",
    "ax.set_xlabel(r'$\\mathrm{Modified~Julian~Date}~/~\\mathrm{d}$')\n",
    "ax.set_ylabel(r'$v_\\mathrm{rad}~/~\\mathrm{km\\,s^{-1}}$')\n",
    "\n",
    "ax = gs[1]\n",
    "ax.set_title('GALAH DR4: '+str(spectrum['sobject_id']))\n",
    "\n",
    "ax.axhline(c='grey',lw=1,ls='dashed')\n",
    "if (len_available > 0) & (len_available != len_unflagged):\n",
    "    ax.scatter(\n",
    "        single_results['mjd'][available_single_measurements],\n",
    "        # (single_results['rv_gauss'][available_single_measurements] - rv_mean) / np.sqrt(single_results['e_rv_gauss'][available_single_measurements]**2 + rv_sigma**2)\n",
    "        (single_results['rv_gauss'][available_single_measurements] - rv_mean) / np.sqrt((10*single_results['e_rv_gauss'][available_single_measurements])**2 + 0.1**2),\n",
    "        c='grey',s=9\n",
    "    )\n",
    "if len_unflagged > 0:\n",
    "    ax.scatter(\n",
    "        single_results['mjd'][unflagged_single_measurements],\n",
    "        # (single_results['rv_gauss'][unflagged_single_measurements] - rv_mean) / np.sqrt(single_results['e_rv_gauss'][unflagged_single_measurements]**2 + rv_sigma**2)\n",
    "        (single_results['rv_gauss'][unflagged_single_measurements] - rv_mean) / np.sqrt((10*single_results['e_rv_gauss'][unflagged_single_measurements])**2 + 0.1**2),\n",
    "        c='k',s=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(r'$\\mathrm{Modified~Julian~Date}~/~\\mathrm{d}$')\n",
    "ax.set_ylabel(r'$\\Delta v_\\mathrm{rad}~/~\\sigma_{v_\\mathrm{rad}}$')\n",
    "\n",
    "ax = gs[2]\n",
    "ax.set_title('GALAH DR4: '+str(spectrum['sobject_id']))\n",
    "\n",
    "if (len_available > 0) & (len_available != len_unflagged):\n",
    "    ax.errorbar(\n",
    "        single_results['mjd'][available_single_measurements],\n",
    "        single_results['vsini'][available_single_measurements],\n",
    "        yerr=single_results['e_vsini'][available_single_measurements],\n",
    "        ms=3,c='grey',capsize=3,lw=1,\n",
    "        fmt='o',zorder=5,label='flagged'\n",
    "    )\n",
    "if len_unflagged > 0:\n",
    "    ax.errorbar(\n",
    "        single_results['mjd'][unflagged_single_measurements],\n",
    "        single_results['vsini'][unflagged_single_measurements],\n",
    "        yerr=single_results['e_vsini'][unflagged_single_measurements],\n",
    "        ms=3,c='k',capsize=3,lw=1,\n",
    "        fmt='o',zorder=6,label='unflagged'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(r'$\\mathrm{Modified~Julian~Date}~/~\\mathrm{d}$')\n",
    "ax.set_ylabel(r'$v \\sin i~/~\\mathrm{km\\,s^{-1}}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "file_directory = galah_dr4_directory+'analysis_products/'+str(spectrum['sobject_id'])[:6]+'/'+str(spectrum['sobject_id'])+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.savefig(file_directory+str(spectrum['sobject_id'])+'_plxcom_rv_comparison.png',bbox_inches='tight')\n",
    "\n",
    "if sys.argv[1] == '-f':\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rv_difference_sigma = np.abs(single_results['rv_gauss'][use_for_starting_values] - rv_mean) / np.sqrt((10*single_results['e_rv_gauss'][use_for_starting_values])**2 + 0.1**2),\n",
    "if np.max(rv_difference_sigma) > 2:\n",
    "    spectrum['fit_global_rv'] = False\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('RV difference above 2 sigma. Applying initial RV correction and not fitting global RV.')\n",
    "        print('Fixing global RV init_vrad to 0.0 and applying single_fit RV estimates.')\n",
    "    spectrum['init_vrad'] = 0.0\n",
    "else:\n",
    "    spectrum['fit_global_rv'] = True\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('RV difference below 2 sigma. Applying no initial RV correction and fitting global RV.')\n",
    "    spectrum['init_vrad'] = np.median(single_results['rv_gauss'][use_for_starting_values])\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Starting fit of global RV from init_vrad '+\"{:.2f}\".format(spectrum['init_vrad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in ['teff','logg','fe_h','vmic','vsini']:\n",
    "    spectrum['init_'+label] = np.median(single_results[label][use_for_starting_values])\n",
    "    if label == 'teff':\n",
    "        spectrum['init_'+label] /= 1000.\n",
    "    if label == 'fe_h':\n",
    "        if spectrum['init_'+label] > 0.8:\n",
    "            spectrum['init_'+label] = np.float32(-0.07)\n",
    "            print('[Fe/H] > 0.8, nulling.')\n",
    "\n",
    "if spectrum['sobject_id'] in [140414004601184,140309003101001,140113002401230]:\n",
    "    spectrum['init_teff'] = np.float32(3.2)\n",
    "if spectrum['tmass_id'] == '13255577-4717440':\n",
    "    spectrum['init_teff'] = np.float32(4.5)\n",
    "    spectrum['init_logg'] = np.float32(1.5)\n",
    "    spectrum['init_fe_h'] = np.float32(-1.8)\n",
    "    spectrum['init_vsini'] = np.float32(5.)\n",
    "            \n",
    "spectrum['init_teff'] = spectrum['init_teff'].clip(min=3.01,max=7.99)\n",
    "\n",
    "# We know if a bias for low [Fe/H] for giants\n",
    "if (spectrum['init_teff'] >= 4.0) & (spectrum['init_teff'] <= 5.0) & (spectrum['init_teff'] <= 5.0) & (spectrum['init_logg'] <= 2.5):\n",
    "    spectrum['init_logg'] += 0.1 - 0.3 * (spectrum['init_teff'] - 5.)\n",
    "    spectrum['init_fe_h'] += 0.07 - 0.15 * (spectrum['init_teff'] - 5.)\n",
    "    print('Identified giant with 4000-5000K: logg increased by '+\"{:.2f}\".format(0.1 - 0.3 * (spectrum['init_teff'] - 5.))+', [Fe/H] increased by '+\"{:.2f}\".format(0.07 - 0.15 * (spectrum['init_teff'] - 5.))+'\\n')\n",
    "else:\n",
    "    print('Applying usual offset for logg and [Fe/H] of +0.1 and +0.07\\n')\n",
    "    spectrum['init_logg'] = spectrum['init_logg'] + 0.1\n",
    "    spectrum['init_fe_h'] = spectrum['init_fe_h'] + 0.07\n",
    "\n",
    "spectrum['init_logg'] = spectrum['init_logg'].clip(min=-0.4,max=5.3)\n",
    "spectrum['init_fe_h'] = spectrum['init_fe_h'].clip(min=-3.9,max=0.7)\n",
    "\n",
    "spectrum['init_vmic'] = spectrum['init_vmic'].clip(min=0.35,max=3.9)\n",
    "if (spectrum['init_teff'] < 4.5) & (spectrum['init_logg'] > 4.0):\n",
    "    spectrum['init_vmic'] = spectrum['init_vmic'].clip(min=0.35,max=1.5)\n",
    "spectrum['init_vsini'] = spectrum['init_vsini'].clip(min=1.2,max=39)\n",
    "\n",
    "print('Initial values:')\n",
    "print('RV = '+\"{:.2f}\".format(spectrum['init_vrad'])+' (Gaia DR3: '+\"{:.2f}\".format(extra_info['rv_gaia_dr3'])+')')\n",
    "print('Teff, logg, fe_h, vmic, vsini')\n",
    "print(str(np.int32(1000*spectrum['init_teff']))+', '+\"{:.2f}\".format(spectrum['init_logg'])+', '+\"{:.2f}\".format(spectrum['init_fe_h'])+', '+\"{:.2f}\".format(spectrum['init_vmic'])+', '+\"{:.2f}\".format(spectrum['init_vsini']))\n",
    "print()\n",
    "\n",
    "for element in ['Li','C','N','O','Na','Mg','Al','Si','K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn','Rb','Sr','Y','Zr','Mo','Ru','Ba','La','Ce','Nd','Sm','Eu']:\n",
    "    spectrum['init_'+element.lower()+'_fe'] = 0.0\n",
    "    \n",
    "alpha_fe = np.max([0.0,np.min([0.4,-0.4*spectrum['init_fe_h']])])\n",
    "for each_alpha in ['o','mg','si','ca','ti']:\n",
    "    spectrum['init_'+each_alpha+'_fe'] = alpha_fe\n",
    "if sys.argv[1] == '-f': print('Enhancing [X/Fe] to '+\"{:.2f}\".format(alpha_fe)+' based on [Fe/H] for O, Mg, Si, Ca, and Ti ▔\\▁') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we should also include younger isocheones < 100 Myr\n",
    "\n",
    "parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_8p00_0p01_10p17_mh_m2p75_0p25_m0p75_mh_m0p60_0p10_0p70_GaiaEDR3_2MASS.fits')\n",
    "cooler = False\n",
    "more_luminous = False\n",
    "if extra_info['age'] > 1:\n",
    "    unique_m_h = np.unique(parsec['m_h'])\n",
    "    old_isochrone_same_m_h = (parsec['logg'] > 2.3) & (parsec['label'] < 4) & (parsec['logAge'] == np.max(parsec['logAge'])) & (parsec['m_h'] == unique_m_h[np.argmin(np.abs(spectrum['init_fe_h'] + 0.25 - unique_m_h))])\n",
    "\n",
    "    # Where is the turnoff point for this isochrone?\n",
    "    turn_off_index = np.argmax(parsec['logT'][old_isochrone_same_m_h])\n",
    "    turn_off_logg = parsec['logg'][old_isochrone_same_m_h][turn_off_index]\n",
    "    turn_off_teff = 10**parsec['logT'][old_isochrone_same_m_h][turn_off_index]\n",
    "    \n",
    "    # Test if star is cool enough and has high enough surface gravity:\n",
    "    if (spectrum['init_logg'] > 2.5) & (spectrum['init_teff']*1000. < turn_off_teff):\n",
    "        \n",
    "        # Is star cooler than evolved giant stars?\n",
    "        if spectrum['init_logg'] < turn_off_logg + 0.2:\n",
    "            \n",
    "            # let's interpolate the LOGG for more evolved stars and shift by -100K and +0.2 dex (reasonable uncertainties)\n",
    "            iso_teff = np.interp(spectrum['init_logg'], parsec['logg'][old_isochrone_same_m_h][turn_off_index:][::-1]+0.2, 10**parsec['logT'][old_isochrone_same_m_h][turn_off_index:][::-1]-100)\n",
    "\n",
    "#             plt.scatter(\n",
    "#                 iso_teff,\n",
    "#                 spectrum['init_logg']\n",
    "#             )\n",
    "            \n",
    "            if spectrum['init_teff']*1000. < iso_teff:\n",
    "                print('Star luminous, but cooler than evolved giant stars')\n",
    "                cooler = True\n",
    "            else:\n",
    "                print('Star luminous, but consistent with evolved giant stars ',spectrum['init_teff']*1000.,iso_teff)\n",
    "        else:\n",
    "            cooler = True\n",
    "\n",
    "        # Is star more luminous than old stars on MS?\n",
    "\n",
    "        # let's interpolate the LOGG for the part up until the turnoff point\n",
    "        iso_logg = np.interp(spectrum['init_teff']*1000., 10**parsec['logT'][old_isochrone_same_m_h][:turn_off_index], parsec['logg'][old_isochrone_same_m_h][:turn_off_index])\n",
    "        if spectrum['init_logg'] < iso_logg:\n",
    "            print('Star more luminous than old MS star could be')\n",
    "            more_luminous = True\n",
    "        else:\n",
    "            print('Star consistent with old MS star')\n",
    "            more_luminous = False\n",
    "            \n",
    "#         plt.plot(\n",
    "#             10**parsec['logT'][old_isochrone_same_m_h][turn_off_index:]-100,\n",
    "#             parsec['logg'][old_isochrone_same_m_h][turn_off_index:]+0.2\n",
    "#         )\n",
    "#         plt.plot(\n",
    "#             10**parsec['logT'][old_isochrone_same_m_h][:turn_off_index],\n",
    "#             parsec['logg'][old_isochrone_same_m_h][:turn_off_index]\n",
    "#         )\n",
    "#         plt.scatter(\n",
    "#             spectrum['init_teff']*1000.,\n",
    "#             spectrum['init_logg']\n",
    "#         )\n",
    "    else:\n",
    "        print('Star not in young MS region')\n",
    "        \n",
    "    if cooler & more_luminous:\n",
    "        print('Assuming Young star.')\n",
    "        print('Using Parsec grid starting from 1.5 Myr instead of only 100 Myr.')\n",
    "        parsec = Table.read('../auxiliary_information/parsec_isochrones/parsec_isochrones_logt_6p19_0p01_10p17_mh_m2p75_0p25_m0p75_mh_m0p60_0p10_0p70_GaiaEDR3_2MASS.fits')   \n",
    "    else:\n",
    "        print('Assuming Old star with Parsec grid starting from 100 Myr')\n",
    "else:\n",
    "    print('Assuming Young star because this star is within a young open cluster with age < 1 Gyr.')\n",
    "    print('Using Parsec grid starting from 1.5 Myr instead of only 100 Myr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_spectrum(sobject_id, spectrum, init_values_table, neglect_ir_beginning=True):\n",
    "\n",
    "    try:\n",
    "        fits_file = fits.open(galah_dr4_directory+'observations_6p1/'+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+'1.fits')\n",
    "        print('Using dr6.1 spectrum')\n",
    "    except:\n",
    "        fits_file = fits.open(galah_dr4_directory+'observations/'+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+'1.fits')\n",
    "        print('Using dr6.0 spectrum')\n",
    "    if fits_file[0].header['SLITMASK'] in ['IN','IN      ']:\n",
    "        spectrum['resolution'] = 'high-res'\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Warning: Spectrum '+str(sobject_id)+' is high-resolution!')\n",
    "    else:\n",
    "        spectrum['resolution'] = 'low-res'\n",
    "\n",
    "    if fits_file[0].header['WAV_OK']==0:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Warning: Wavelength solution not ok!')\n",
    "\n",
    "    if fits_file[0].header['CROSS_OK']==0:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Warning: Cross-talk not calculated reliably!')\n",
    "\n",
    "    spectrum['plate'] = np.int32(fits_file[0].header['PLATE'])\n",
    "    \n",
    "    # This is a test if the CCD is actually available. For 181221001601377, CCD4 is missing for example.\n",
    "    # We therefore implement a keyword 'available_ccds' to trigger only to loop over the available CCDs\n",
    "    spectrum['available_ccds'] = []\n",
    "    \n",
    "    match = np.where(init_values_table['sobject_id'] == sobject_id)[0]\n",
    "    if len(match) == 0:\n",
    "        raise ValueError('No entry in initial values table for sobject_id '+str(sobject_id))\n",
    "    else:\n",
    "        match = match[0]\n",
    "    \n",
    "    for ccd in [1,2,3,4]:\n",
    "        \n",
    "        wavelength_solution_flagged = (\n",
    "            ((2**(ccd-1) & init_values_table['cdelt_flag'][match]) > 0) | \n",
    "            ((2**(ccd-1) & init_values_table['crval_flag'][match]) > 0)\n",
    "        )\n",
    "        if wavelength_solution_flagged:\n",
    "            if sys.argv[1] == '-f':\n",
    "                print('CDELT or CRVAL off in CCD'+str(ccd)+'?',wavelength_solution_flagged)\n",
    "                print('Manually overwriting to be False')\n",
    "            wavelength_solution_flagged=False\n",
    "        \n",
    "        if not wavelength_solution_flagged:\n",
    "\n",
    "            try:\n",
    "\n",
    "                if ccd != 1:\n",
    "                    try:\n",
    "                        fits_file = fits.open(galah_dr4_directory+'observations_6p1/'+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+str(ccd)+'.fits')\n",
    "                    except:\n",
    "                        fits_file = fits.open(galah_dr4_directory+'observations/'+str(sobject_id)[:6]+'/spectra/com/'+str(sobject_id)+str(ccd)+'.fits')\n",
    "\n",
    "                spectrum['crval_ccd'+str(ccd)] = fits_file[0].header['CRVAL1']\n",
    "                spectrum['cdelt_ccd'+str(ccd)] = fits_file[0].header['CDELT1']\n",
    "\n",
    "                spectrum['counts_ccd'+str(ccd)]   = fits_file[0].data\n",
    "                counts_relative_uncertainty = fits_file[2].data\n",
    "\n",
    "                bad_counts_unc = np.where(~(counts_relative_uncertainty > 0) == True)[0]\n",
    "                if len(bad_counts_unc) > 0:\n",
    "                    if sys.argv[1] == '-f': print('Relative counts uncertainties <= 0 detected for '+str(len(bad_counts_unc))+' pixels in CCD'+str(ccd)+', setting to median flux with SNR 1')\n",
    "                    counts_relative_uncertainty[bad_counts_unc] = 1.0\n",
    "                    spectrum['counts_ccd'+str(ccd)][bad_counts_unc] = np.nanmedian(spectrum['counts_ccd'+str(ccd)])\n",
    "\n",
    "                spectrum['counts_unc_ccd'+str(ccd)] = counts_relative_uncertainty * spectrum['counts_ccd'+str(ccd)]\n",
    "\n",
    "                spectrum['sky_ccd'+str(ccd)]   = fits_file[3].data\n",
    "                spectrum['telluric_ccd'+str(ccd)]   = fits_file[4].data\n",
    "\n",
    "                spectrum['lsf_b_ccd'+str(ccd)] = fits_file[0].header['B']\n",
    "                spectrum['lsf_ccd'+str(ccd)]   = fits_file[7].data\n",
    "\n",
    "                spectrum['available_ccds'].append(ccd)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if ccd in spectrum['available_ccds']:\n",
    "            if np.shape(spectrum['lsf_ccd'+str(ccd)])[0] == 1:\n",
    "\n",
    "                # find all spectra are\n",
    "                # a) observed with same FIBRE (*pivot*) and\n",
    "                # b) observed with the same PLATE (*plate*) \n",
    "                # c) have a measured LSF in the particular CCD\n",
    "                # d) have the same resolution setup (low- or high-res)\n",
    "                if spectrum['resolution'] != 'high-res':\n",
    "                    same_fibre_plate_ccd_and_has_res_profile = np.where(\n",
    "                        (\n",
    "                            (int(str(spectrum['sobject_id'])[-3:]) == init_values_table['pivot']) & \n",
    "                            (spectrum['plate'] == init_values_table['plate']) &\n",
    "                            (init_values_table['res'][:,ccd-1] > 0) & \n",
    "                            (init_values_table['reduction_flags'] < 262144)\n",
    "                        )==True)[0]\n",
    "                else:\n",
    "                    same_fibre_plate_ccd_and_has_res_profile = np.where(\n",
    "                        (\n",
    "                            (int(str(spectrum['sobject_id'])[-3:]) == init_values_table['pivot']) & \n",
    "                            (spectrum['plate'] == init_values_table['plate']) &\n",
    "                            (init_values_table['res'][:,ccd-1] > 0) & \n",
    "                            (init_values_table['reduction_flags'] >= 262144)\n",
    "                        )==True)[0]\n",
    "\n",
    "                # Difference between observing runs == abs(sobject_id - all possible sobject_ids)\n",
    "                sobject_id_differences = np.abs(spectrum['sobject_id'] - init_values_table['sobject_id'][same_fibre_plate_ccd_and_has_res_profile])\n",
    "                # Now find the closest observing run\n",
    "                closest_valid_sobject_id_index = np.argmin(sobject_id_differences)\n",
    "                closest_valid_sobject_id = init_values_table['sobject_id'][same_fibre_plate_ccd_and_has_res_profile][closest_valid_sobject_id_index]\n",
    "\n",
    "                lsf_replacement_fits_file = fits.open(galah_dr4_directory+'observations/'+str(closest_valid_sobject_id)[:6]+'/spectra/com/'+str(closest_valid_sobject_id)+str(ccd)+'.fits')\n",
    "                spectrum['lsf_b_ccd'+str(ccd)] = lsf_replacement_fits_file[0].header['B']\n",
    "                spectrum['lsf_ccd'+str(ccd)]   = lsf_replacement_fits_file[7].data\n",
    "                lsf_replacement_fits_file.close()\n",
    "\n",
    "                if sys.argv[1] == '-f': print('No LSF reported for CCD'+str(ccd)+'. Replaced LSF and LSF-B for CCD '+str(ccd)+' with profile from '+str(closest_valid_sobject_id))\n",
    "\n",
    "            zero_or_negative_flux = np.where(~(spectrum['counts_ccd'+str(ccd)] > 0))\n",
    "            if len(zero_or_negative_flux) > 10:\n",
    "                if sys.argv[1] == '-f':\n",
    "                    print('Missing/negative flux in more than 10 pixels')\n",
    "                    \n",
    "        fits_file.close()\n",
    "\n",
    "        if (ccd == 4) & (ccd in spectrum['available_ccds']) & neglect_ir_beginning:\n",
    "            wave_ccd4 = spectrum['crval_ccd4'] + spectrum['cdelt_ccd4'] * np.arange(len(spectrum['counts_ccd4']))\n",
    "            bad_ir = wave_ccd4 > 7680\n",
    "\n",
    "            spectrum['crval_ccd4'] = wave_ccd4[bad_ir][0]\n",
    "            spectrum['counts_ccd4'] = spectrum['counts_ccd4'][bad_ir]\n",
    "            spectrum['counts_unc_ccd4'] = spectrum['counts_unc_ccd4'][bad_ir]\n",
    "            spectrum['lsf_ccd4'] = spectrum['lsf_ccd4'][bad_ir]\n",
    "\n",
    "    return(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rv_shift(rv_value, wavelength):\n",
    "    '''\n",
    "    Shifts observed wavelengths to account for radial velocity measurements\n",
    "    \n",
    "    speed of light: 299792.458 km/s via astropy.constants\n",
    "    \n",
    "    INPUT:\n",
    "    rv_value = radial velocity in km/s (negative if moving towards earth)\n",
    "    wavelengths = array of observed wavelengths\n",
    "    \n",
    "    OUTPUT:\n",
    "    array of shifted wavelengths\n",
    "    '''\n",
    "    return wavelength / (1.+rv_value/299792.458)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def galah_kern(fwhm, b):\n",
    "    \"\"\" Returns a normalized 1D kernel as is used for GALAH resolution profile \"\"\"\n",
    "    size=2*(fwhm/2.355)**2\n",
    "    size_grid = int(size) # we limit the size of kernel, so it is as small as possible (or minimal size) for faster calculations\n",
    "    if size_grid<7: size_grid=7\n",
    "    x= scipy.mgrid[-size_grid:size_grid+1]\n",
    "    g = scipy.exp(-0.693147*np.power(abs(2*x/fwhm), b))\n",
    "    return g / np.sum(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exclude_pixels = dict()\n",
    "exclude_pixels['ccd1'] = []\n",
    "exclude_pixels['ccd2'] = []\n",
    "exclude_pixels['ccd3'] = []\n",
    "exclude_pixels['ccd4'] = []\n",
    "\n",
    "first_spectrum = dict()\n",
    "first_spectrum['sobject_id'] = spectrum['sobject_ids'][0]\n",
    "first_spectrum = read_spectrum(first_spectrum['sobject_id'],first_spectrum,init_values_table,neglect_ir_beginning=False)\n",
    "spectrum['resolution'] = first_spectrum['resolution']\n",
    "spectrum['available_ccds'] = first_spectrum['available_ccds']\n",
    "\n",
    "for ccd in [1,2,3,4]:\n",
    "    \n",
    "    if ccd in spectrum['available_ccds']:\n",
    "        for label in [\n",
    "            'crval','cdelt','counts','lsf_b','lsf'\n",
    "        ]:\n",
    "            spectrum[label+'_ccd'+str(ccd)] = first_spectrum[label+'_ccd'+str(ccd)]\n",
    "\n",
    "        # Note: We add the variance here!\n",
    "        spectrum['counts_unc_ccd'+str(ccd)] = (first_spectrum['counts_unc_ccd'+str(ccd)])**2\n",
    "\n",
    "        wave_observed_1 = spectrum['crval_ccd'+str(ccd)] + spectrum['cdelt_ccd'+str(ccd)] * np.arange(len(spectrum['counts_ccd'+str(ccd)]))\n",
    "        if spectrum['fit_global_rv'] == True:\n",
    "            spectrum['wave_ccd'+str(ccd)] = wave_observed_1\n",
    "        else:\n",
    "            spectrum['wave_ccd'+str(ccd)] = rv_shift(single_results['rv_gauss'][0],wave_observed_1)\n",
    "            spectrum['crval_ccd'+str(ccd)] = spectrum['wave_ccd'+str(ccd)][0]\n",
    "            spectrum['cdelt_ccd'+str(ccd)] = spectrum['wave_ccd'+str(ccd)][1] - spectrum['wave_ccd'+str(ccd)][0]\n",
    "\n",
    "for repeat_index, repeat_sobject_id in enumerate(spectrum['sobject_ids'][1:]):\n",
    "    \n",
    "    single_spectrum = dict()\n",
    "    single_spectrum['sobject_id'] = repeat_sobject_id\n",
    "    single_spectrum = read_spectrum(repeat_sobject_id,single_spectrum,init_values_table,neglect_ir_beginning=False)\n",
    "    \n",
    "    if single_spectrum['resolution'] != spectrum['resolution']:\n",
    "\n",
    "        print(spectrum['resolution'],spectrum['sobject_id'])\n",
    "        print(single_spectrum['sobject_id'],single_spectrum['resolution'])\n",
    "        #raise ValueError('ToDo: Implement how to handle different resolutions!')\n",
    "        \n",
    "    for ccd in [1,2,3,4]:\n",
    "\n",
    "        # If the first spectrum already had a useful CCD \n",
    "        if ccd in single_spectrum['available_ccds']:\n",
    "            wave_observed_next = single_spectrum['crval_ccd'+str(ccd)] + single_spectrum['cdelt_ccd'+str(ccd)] * np.arange(len(single_spectrum['counts_ccd'+str(ccd)]))\n",
    "            if spectrum['fit_global_rv'] == True:\n",
    "                wave_shifted_next = wave_observed_next\n",
    "            else:\n",
    "                wave_shifted_next = rv_shift(single_results['rv_gauss'][repeat_index],wave_observed_next)\n",
    "\n",
    "            for label in [\n",
    "                'counts'\n",
    "            ]:\n",
    "                spectrum[label+'_ccd'+str(ccd)] += np.interp(spectrum['wave_ccd'+str(ccd)], wave_shifted_next, single_spectrum[label+'_ccd'+str(ccd)], left = 0, right = 0)\n",
    "\n",
    "                if label == 'counts':\n",
    "                    interpolated_counts = np.interp(spectrum['wave_ccd'+str(ccd)], wave_shifted_next, single_spectrum[label+'_ccd'+str(ccd)], left = np.NaN, right = np.NaN)\n",
    "                    extrapolated_pixels = np.where(np.isnan(interpolated_counts))[0]\n",
    "                    if len(extrapolated_pixels) > 0:\n",
    "                        exclude_pixels['ccd'+str(ccd)].append(extrapolated_pixels)\n",
    "\n",
    "            # Add variance to uncertainty label (will be converted to std later)\n",
    "            spectrum['counts_unc_ccd'+str(ccd)] += (np.interp(spectrum['wave_ccd'+str(ccd)], wave_shifted_next, single_spectrum['counts_unc_ccd'+str(ccd)], left = 0, right = 0))**2\n",
    "    \n",
    "        # If this is the first time we can use this CCD\n",
    "        elif ccd in single_spectrum['available_ccds']:\n",
    "            \n",
    "            spectrum['available_ccds'].append(ccd)\n",
    "            spectrum['available_ccds'].sort()\n",
    "            \n",
    "            wave_observed_next = single_spectrum['crval_ccd'+str(ccd)] + single_spectrum['cdelt_ccd'+str(ccd)] * np.arange(len(single_spectrum['counts_ccd'+str(ccd)]))\n",
    "            if spectrum['fit_global_rv'] == True:\n",
    "                wave_shifted_next = wave_observed_next\n",
    "            else:\n",
    "                wave_shifted_next = rv_shift(single_results['rv_gauss'][repeat_index],wave_observed_next)\n",
    "\n",
    "            for label in [\n",
    "                'crval','cdelt','counts','lsf_b','lsf'\n",
    "            ]:\n",
    "                spectrum[label+'_ccd'+str(ccd)] = single_spectrum[label+'_ccd'+str(ccd)]\n",
    "\n",
    "            # Add variance to uncertainty label (will be converted to std later)\n",
    "            spectrum['counts_unc_ccd'+str(ccd)] = single_spectrum['counts_unc_ccd'+str(ccd)]\n",
    "\n",
    "# Because the spectra could all be slightly shifted, we may miss out on some pixels left/right without counts\n",
    "# To avoid them causing issues, we focus only on the pixels that have counts from all coadded spectra\n",
    "for ccd in [1,2,3,4]:\n",
    "    if len(exclude_pixels['ccd'+str(ccd)]) > 0:\n",
    "        unique_exclude_pixels = np.unique(np.concatenate((exclude_pixels['ccd'+str(ccd)])))\n",
    "        left_end = unique_exclude_pixels < 2000\n",
    "        try:\n",
    "            left_end = unique_exclude_pixels[left_end][-1]\n",
    "        except:\n",
    "            left_end = 0\n",
    "        right_end = unique_exclude_pixels > 2000\n",
    "        try:\n",
    "            right_end = unique_exclude_pixels[right_end][0]\n",
    "        except:\n",
    "            right_end = len(spectrum['wave_ccd'+str(ccd)])\n",
    "            \n",
    "        spectrum['crval_ccd'+str(ccd)] = spectrum['wave_ccd'+str(ccd)][left_end]\n",
    "        for label in [\n",
    "            'counts','counts_unc','lsf'\n",
    "        ]:\n",
    "            spectrum[label+'_ccd'+str(ccd)] = spectrum[label+'_ccd'+str(ccd)][left_end:right_end]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    neglect_ir_beginning = True\n",
    "    if (ccd == 4) & (ccd in spectrum['available_ccds']) & neglect_ir_beginning:\n",
    "        wave_ccd4 = spectrum['crval_ccd4'] + spectrum['cdelt_ccd4'] * np.arange(len(spectrum['counts_ccd4']))\n",
    "        bad_ir = wave_ccd4 > 7680\n",
    "\n",
    "        spectrum['crval_ccd4'] = wave_ccd4[bad_ir][0]\n",
    "        spectrum['counts_ccd4'] = spectrum['counts_ccd4'][bad_ir]\n",
    "        spectrum['counts_unc_ccd4'] = spectrum['counts_unc_ccd4'][bad_ir]\n",
    "        spectrum['lsf_ccd4'] = spectrum['lsf_ccd4'][bad_ir]\n",
    "\n",
    "# Finally, take sqrt of variance\n",
    "for ccd in spectrum['available_ccds']:\n",
    "    spectrum['counts_unc_ccd'+str(ccd)] = np.sqrt(spectrum['counts_unc_ccd'+str(ccd)])\n",
    "\n",
    "# if sys.argv[1] == '-f':\n",
    "#     print('ToDo: Implement LSF handling. Currently ignoring all but first LSF!')\n",
    "#     print('ToDo: Implement sigma clipping for co-adding!')\n",
    "#     print('ToDo: Implement how to handle different resolutions!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccds_with_positive_flux = []\n",
    "for ccd in spectrum['available_ccds']:\n",
    "    below_0 = (spectrum['counts_ccd'+str(ccd)] < 0) | (spectrum['counts_ccd'+str(ccd)]/spectrum['counts_unc_ccd'+str(ccd)] < 1)\n",
    "    if len(spectrum['counts_ccd'+str(ccd)][below_0])/len(spectrum['counts_ccd'+str(ccd)]) > 0.05:\n",
    "        print('More than 5% of counts below 0 for CCD'+str(ccd)+' or below SNR=1. Neglecting this CCD!')\n",
    "        if (spectrum['flag_sp'] & flag_sp_negative_fluxes_in_ccds) == 0:\n",
    "            spectrum['flag_sp'] += flag_sp_negative_fluxes_in_ccds\n",
    "    else:\n",
    "        ccds_with_positive_flux.append(ccd)\n",
    "spectrum['available_ccds'] = ccds_with_positive_flux\n",
    "\n",
    "ccds_with_positive_resolution_profile = []\n",
    "for ccd in spectrum['available_ccds']:\n",
    "    below_0 = np.where(spectrum['lsf_ccd'+str(ccd)] < 0)[0]\n",
    "    if len(below_0) > 0:\n",
    "        print('Negative resolution profile detected. Neglecting this CCD!')\n",
    "        if (spectrum['flag_sp'] & flag_sp_negative_resolution_profile) == 0:\n",
    "            spectrum['flag_sp'] += flag_sp_negative_resolution_profile\n",
    "    else:\n",
    "        ccds_with_positive_resolution_profile.append(ccd)\n",
    "spectrum['available_ccds'] = ccds_with_positive_resolution_profile\n",
    "\n",
    "print('Working with the following CCDs: ',spectrum['available_ccds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on feedback from Adam Rains:\n",
    "# Line information of blue wavelengths in the coolest stars maybe not reliable.\n",
    "if (spectrum['init_teff'] < 4.1) & (1 in spectrum['available_ccds']):\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Models are not reliable for bluest part of spectra (CCD1) for cool stars (< 4100K).')\n",
    "        print('Doubling observational uncertainties of that region to give less weight here during fitting')\n",
    "    spectrum['counts_unc_ccd1'] *= 2\n",
    "    \n",
    "if len(spectrum['available_ccds']) != 4:\n",
    "    if (spectrum['flag_sp'] & flag_sp_not_all_ccds_available) == 0:\n",
    "        spectrum['flag_sp'] += flag_sp_not_all_ccds_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare spectroscopic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load spectrum masks\n",
    "masks = Table.read('spectrum_masks/solar_spectrum_mask.fits')\n",
    "vital_lines = Table.read('spectrum_masks/vital_lines.fits')\n",
    "\n",
    "# Load wavelength array of synthetic spectra\n",
    "wavelength_file = '../spectrum_interpolation/training_input/galah_dr4_3dbin_wavelength_array.pickle'\n",
    "wavelength_file_opener = open(wavelength_file,'rb')\n",
    "default_model_wave = pickle.load(wavelength_file_opener)\n",
    "wavelength_file_opener.close()\n",
    "\n",
    "# Load model grid indices of all and of available grids\n",
    "grids = Table.read('../spectrum_grids/galah_dr4_model_trainingset_gridpoints.fits')\n",
    "grids_avail = Table.read('../spectrum_grids/galah_dr4_model_trainingset_gridpoints_trained.fits')\n",
    "grids_avail = grids_avail[grids_avail['has_model_3x3x3']]\n",
    "\n",
    "grid_index_tree = cKDTree(np.c_[grids['teff_subgrid']/1000.,grids['logg_subgrid'],grids['fe_h_subgrid']])\n",
    "grid_avail_index_tree = cKDTree(np.c_[grids_avail['teff_subgrid']/1000.,grids_avail['logg_subgrid'],grids_avail['fe_h_subgrid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dr3_lines(mode_dr3_path = './spectrum_masks/important_lines'):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    important_lines = [\n",
    "        [4861.3230,r'H$_\\beta$',r'H$_\\beta$'],\n",
    "        [6562.7970,r'H$_\\alpha$',r'H$_\\alpha$']\n",
    "    ]\n",
    "    \n",
    "    important_molecules = [\n",
    "        [4710,4740,'Mol. C2','Mol. C2'],\n",
    "        [7594,7695,'Mol. O2 (tell.)','Mol. O2 (tell.)']\n",
    "        ]\n",
    "\n",
    "    line, wave = np.loadtxt(mode_dr3_path,usecols=(0,1),unpack=True,dtype=str, comments=';')\n",
    "\n",
    "    for each_index in range(len(line)):\n",
    "        if line[each_index] != 'Sp':\n",
    "            if len(line[each_index]) < 5:\n",
    "                important_lines.append([float(wave[each_index]), line[each_index], line[each_index]])\n",
    "            else:\n",
    "                important_lines.append([float(wave[each_index]), line[each_index][:-4], line[each_index]])\n",
    "        \n",
    "    return(important_lines,important_molecules)\n",
    "\n",
    "important_lines, important_molecules = load_dr3_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_spectrum(wave,flux,flux_uncertainty,unmasked_region,title_text,comp1_text,comp2_text,neglect_ir_beginning=neglect_ir_beginning):\n",
    "    \"\"\"\n",
    "    Let's plot a spectrum, that is, flux over wavelenth\n",
    "    \n",
    "    We will plot 12 different subplot ranges (3 for each CCD) to allow better assessment of the results\n",
    "    \n",
    "    INPUT:\n",
    "    wave : 1D-array with N pixels\n",
    "    flux : 1D-array with N pixels or (M,N)-array with N pixels for M spectra (e.g. M = 2 for observed and synthetic spectrum)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's define the wavelength beginnings and ends for each suplot\n",
    "    if neglect_ir_beginning:\n",
    "        subplot_wavelengths = np.array([\n",
    "            [4710,4775],\n",
    "            [4770,4850],\n",
    "            [4840,4905],\n",
    "            [5645,5730],\n",
    "            [5720,5805],\n",
    "            [5795,5878],\n",
    "            [6470,6600],\n",
    "            [6590,6670],\n",
    "            [6660,6739],\n",
    "            [7677,7720],\n",
    "            [7710,7820],\n",
    "            [7810,7890]\n",
    "        ])\n",
    "    else:\n",
    "        subplot_wavelengths = np.array([\n",
    "            [4710,4775],\n",
    "            [4770,4850],\n",
    "            [4840,4905],\n",
    "            [5645,5730],\n",
    "            [5720,5805],\n",
    "            [5795,5878],\n",
    "            [6470,6600],\n",
    "            [6590,6670],\n",
    "            [6660,6739],\n",
    "            [7577,7697],\n",
    "            [7677,7720],\n",
    "            [7710,7820],\n",
    "            [7810,7890]\n",
    "        ])\n",
    "    \n",
    "    # How many subplots will we need?\n",
    "    nr_subplots = np.shape(subplot_wavelengths)[0]\n",
    "    \n",
    "    f, gs = plt.subplots(nr_subplots,1,figsize=(8.3,11.7),sharey=True)\n",
    "    \n",
    "    ylim_upper = np.min([1.2,1.2*np.percentile(flux,q=99)])\n",
    "    \n",
    "    try:\n",
    "        # test if several spectra fed into flux\n",
    "        dummy = np.shape(flux)[1] == len(wave)\n",
    "        flux_array_indices = np.shape(flux)[0]\n",
    "        flux = np.array(flux)\n",
    "    except:\n",
    "        flux_array_indices = 1\n",
    "\n",
    "    # Let's loop over the subplots\n",
    "    for subplot in range(nr_subplots):\n",
    "        \n",
    "        # Which part of the observed/model spectrum is in our subplot wavelength range?\n",
    "        in_subplot_wavelength_range = (wave > subplot_wavelengths[subplot,0]) & (wave < subplot_wavelengths[subplot,1])\n",
    "\n",
    "        ax = gs[subplot]\n",
    "        ax.set_xlim(subplot_wavelengths[subplot,0],subplot_wavelengths[subplot,1])\n",
    "        ax.set_ylim(-0.1,ylim_upper)\n",
    "        \n",
    "        if len(wave[in_subplot_wavelength_range]) > 0:\n",
    "            # if only 1 spectrum\n",
    "            if flux_array_indices == 1:\n",
    "                ax.plot(wave[in_subplot_wavelength_range],flux[in_subplot_wavelength_range],lw=0.5);\n",
    "            else:\n",
    "                for index in range(flux_array_indices):\n",
    "                    if index == 0:\n",
    "                        ax.plot(wave[in_subplot_wavelength_range],flux[0,in_subplot_wavelength_range],lw=0.5,c='k',label='data');\n",
    "                        ax.plot(wave[in_subplot_wavelength_range],1.05 + flux_uncertainty[in_subplot_wavelength_range],lw=0.5,c='C3',label='scatter');\n",
    "                    if index == 1:\n",
    "                        ax.plot(wave[in_subplot_wavelength_range],flux[index,in_subplot_wavelength_range],lw=0.5,c='r',label='model (optimised)');\n",
    "                        ax.plot(wave[in_subplot_wavelength_range],1.05 + np.abs(flux[0,in_subplot_wavelength_range]-flux[index,in_subplot_wavelength_range]),lw=0.5,c='C4',label='residuals');\n",
    "                if subplot == nr_subplots-1:\n",
    "                    ax.legend(ncol=2,loc='lower right',fontsize=6)\n",
    "\n",
    "            maski = 0\n",
    "            for maski, pixel in enumerate(wave[in_subplot_wavelength_range & unmasked_region]):\n",
    "                if maski == 0:\n",
    "                    ax.axvline(pixel,color='C0',alpha=0.1,label='Mask')\n",
    "                    maski += 1\n",
    "                else:\n",
    "                    ax.axvline(pixel,color='C0',alpha=0.1)\n",
    "            each_index = 0 \n",
    "            for each_element in important_lines:\n",
    "                if (each_element[0] > subplot_wavelengths[subplot,0]) & (each_element[0] < subplot_wavelengths[subplot,1]):\n",
    "\n",
    "                    offset = -0.05+0.1*(each_index%3)\n",
    "                    each_index+=1\n",
    "                    ax.axvline(each_element[0],lw=0.2,ls='dashed',c='r')\n",
    "                    if each_element[1] in ['Li','C','O']:\n",
    "                        ax.text(each_element[0],offset,each_element[1],fontsize=10,ha='center',color='pink')\n",
    "                    elif each_element[1] in ['Mg','Si','Ca','Ti','Ti2']:\n",
    "                        ax.text(each_element[0],offset,each_element[1],fontsize=10,ha='center',color='b')\n",
    "                    elif each_element[1] in ['Na','Al','K']:\n",
    "                        ax.text(each_element[0],offset,each_element[1],fontsize=10,ha='center',color='orange')\n",
    "                    elif each_element[1] in ['Sc','V', 'Cr','Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']:\n",
    "                        ax.text(each_element[0],offset,each_element[1],fontsize=10,ha='center',color='brown')\n",
    "                    elif each_element[1] in ['Rb', 'Sr', 'Y', 'Zr', 'Ba', 'La', 'Ce','Mo','Ru', 'Nd', 'Sm','Eu']:\n",
    "                        ax.text(each_element[0],offset,each_element[1],fontsize=10,ha='center',color='purple')\n",
    "\n",
    "        if subplot == nr_subplots-1:\n",
    "            ax.set_xlabel(r'Wavelength / $\\mathrm{\\AA}$')\n",
    "        ax.set_ylabel('Flux / norm.')\n",
    "    f.suptitle(title_text+' \\n '+comp1_text+' \\n '+comp2_text)\n",
    "    plt.tight_layout(h_pad=0)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cubic_spline_interpolate(old_wavelength, old_flux, new_wavelength):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    old_wavelength, old_flux: Input spectrum that has to be interpolated\n",
    "    new_wavelength: Wavelength array onto which we want to interpolate\n",
    "    \n",
    "    OUTPUT:\n",
    "    flux interpolated on new_wavelength array\n",
    "    \"\"\"\n",
    "    return scipy.interpolate.CubicSpline(old_wavelength, old_flux)(new_wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gaussbroad(w, s, hwhm):\n",
    "    \"\"\"\n",
    "    Smooths a spectrum by convolution with a gaussian of specified hwhm.\n",
    "    Parameters\n",
    "    -------\n",
    "    w : array[n]\n",
    "        wavelength scale of spectrum to be smoothed\n",
    "    s : array[n]\n",
    "        spectrum to be smoothed\n",
    "    hwhm : float\n",
    "        half width at half maximum of smoothing gaussian.\n",
    "    Returns\n",
    "    -------\n",
    "    sout: array[n]\n",
    "        the gaussian-smoothed spectrum.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    --------\n",
    "        Dec-90 GB,GM\n",
    "            Rewrote with fourier convolution algorithm.\n",
    "        Jul-91 AL\n",
    "            Translated from ANA to IDL.\n",
    "        22-Sep-91 JAV\n",
    "            Relaxed constant dispersion check# vectorized, 50% faster.\n",
    "        05-Jul-92 JAV\n",
    "            Converted to function, handle nonpositive hwhm.\n",
    "        Oct-18 AW\n",
    "            Python version\n",
    "    \"\"\"\n",
    "\n",
    "    # Warn user if hwhm is negative.\n",
    "    if hwhm < 0:\n",
    "        logger.warning(\"Forcing negative smoothing width to zero.\")\n",
    "\n",
    "    # Return input argument if half-width is nonpositive.\n",
    "    if hwhm <= 0:\n",
    "        return s  # true: no broadening\n",
    "\n",
    "    # Calculate (uniform) dispersion.\n",
    "    nw = len(w)  ## points in spectrum\n",
    "    wrange = w[-1] - w[0]\n",
    "    dw = wrange / (nw - 1)  # wavelength change per pixel\n",
    "\n",
    "    # Make smoothing gaussian# extend to 4 sigma.\n",
    "    # 4.0 / sqrt(2.0*alog(2.0)) = 3.3972872 and sqrt(alog(2.0))=0.83255461\n",
    "    # sqrt(alog(2.0)/pi)=0.46971864 (*1.0000632 to correct for >4 sigma wings)\n",
    "    if hwhm >= 5 * wrange:\n",
    "        return np.full(nw, np.sum(s) / nw)\n",
    "    nhalf = int(3.3972872 * hwhm / dw)  ## points in half gaussian\n",
    "    ng = 2 * nhalf + 1  ## points in gaussian (odd!)\n",
    "    wg = dw * (\n",
    "        np.arange(ng, dtype=float) - (ng - 1) / 2\n",
    "    )  # wavelength scale of gaussian\n",
    "    xg = (0.83255461 / hwhm) * wg  # convenient absisca\n",
    "    gpro = (0.46974832 * dw / hwhm) * np.exp(-xg * xg)  # unit area gaussian w/ FWHM\n",
    "    gpro = gpro / np.sum(gpro)\n",
    "\n",
    "    # Pad spectrum ends to minimize impact of Fourier ringing.\n",
    "    sout = convolve(s, gpro, mode=\"nearest\")\n",
    "\n",
    "    return sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_gauss_broad(wave, smod, ipres=30000, debug=True):\n",
    "    # Apply Gaussian Instrument Broadening\n",
    "    if ipres == 0.0:\n",
    "        hwhm = 0\n",
    "    else:\n",
    "        hwhm = 0.5 * wave[0] / ipres\n",
    "    if hwhm > 0: smod = gaussbroad(wave, smod, hwhm)\n",
    "\n",
    "    return(smod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sclip(p,fit,n,ye=[],sl=99999,su=99999,min=0,max=0,min_data=1,grow=0,global_mask=None,verbose=True):\n",
    "    \"\"\"\n",
    "    p: array of coordinate vectors. Last line in the array must be values that are fitted. The rest are coordinates.\n",
    "    fit: name of the fitting function. It must have arguments x,y,ye,and mask and return an array of values of the fitted function at coordinates x\n",
    "    n: number of iterations\n",
    "    ye: array of errors for each point\n",
    "    sl: lower limit in sigma units\n",
    "    su: upper limit in sigma units\n",
    "    min: number or fraction of rejected points below the fitted curve\n",
    "    max: number or fraction of rejected points above the fitted curve\n",
    "    min_data: minimal number of points that can still be used to make a constrained fit\n",
    "    global_mask: if initial mask is given it will be used throughout the whole fitting process, but the final fit will be evaluated also in the masked points\n",
    "    grow: number of points to reject around the rejected point.\n",
    "    verbose: print the results or not\n",
    "    \"\"\"\n",
    "\n",
    "    nv,dim=np.shape(p)\n",
    "\n",
    "    #if error vector is not given, assume errors are equal to 0:\n",
    "    if ye==[]: ye=np.zeros(dim)\n",
    "    #if a single number is given for y errors, assume it means the same error is for all points:\n",
    "    if isinstance(ye, (int, float)): ye=np.ones(dim)*ye\n",
    "\n",
    "    if global_mask==None: global_mask=np.ones(dim, dtype=bool)\n",
    "    else: pass\n",
    "\n",
    "    f_initial=fit(p,ye,global_mask)\n",
    "    s_initial=np.std(p[-1]-f_initial)\n",
    "\n",
    "    f=f_initial\n",
    "    s=s_initial\n",
    "\n",
    "    tmp_results=[]\n",
    "\n",
    "    b_old=np.ones(dim, dtype=bool)\n",
    "\n",
    "    for step in range(n):\n",
    "        #check that only sigmas or only min/max are given:\n",
    "        if (sl!=99999 or su!=99999) and (min!=0 or max!=0):\n",
    "            raise RuntimeError('Sigmas and min/max are given. Only one can be used.')\n",
    "\n",
    "        #if sigmas are given:\n",
    "        if sl!=99999 or su!=99999:\n",
    "            b=np.zeros(dim, dtype=bool)\n",
    "            if sl>=99999 and su!=sl: sl=su#check if only one is given. In this case set the other to the same value\n",
    "            if su>=99999 and sl!=su: su=sl\n",
    "\n",
    "            good_values=np.where(((f-p[-1])<(sl*(s+ye))) & ((f-p[-1])>-(su*(s+ye))))#find points that pass the sigma test\n",
    "            b[good_values]=True\n",
    "\n",
    "        #if min/max are given\n",
    "        if min!=0 or max!=0:\n",
    "            b=np.ones(dim, dtype=bool)\n",
    "            if min<1: min=dim*min#detect if min is in number of points or percentage\n",
    "            if max<1: max=dim*max#detect if max is in number of points or percentage\n",
    "\n",
    "            bad_values=np.concatenate(((p[-1]-f).argsort()[-int(max):], (p[-1]-f).argsort()[:int(min)]))\n",
    "            b[bad_values]=False\n",
    "\n",
    "        #check the grow parameter:\n",
    "        if grow>=1 and nv==2:\n",
    "            b_grown=np.ones(dim, dtype=bool)\n",
    "            for ind,val in enumerate(b):\n",
    "                if val==False:\n",
    "                    ind_l=ind-int(grow)\n",
    "                    ind_u=ind+int(grow)+1\n",
    "                    if ind_l<0: ind_l=0\n",
    "                    b_grown[ind_l:ind_u]=False\n",
    "\n",
    "            b=b_grown\n",
    "\n",
    "        tmp_results.append(f)\n",
    "\n",
    "        #check that the minimal number of good points is not too low:\n",
    "        if len(b[b])<min_data:\n",
    "            step=step-1\n",
    "            b=b_old\n",
    "            break\n",
    "\n",
    "        #check if the new b is the same as old one and break if yes:\n",
    "        if np.array_equal(b,b_old):\n",
    "            step=step-1\n",
    "            break\n",
    "\n",
    "        #fit again\n",
    "        f=fit(p,ye,b&global_mask)\n",
    "        s=np.std(p[-1][b]-f[b])\n",
    "        b_old=b\n",
    "\n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('FITTING RESULTS:')\n",
    "        print('Number of iterations requested:    ',n)\n",
    "        print('Number of iterations performed:    ', step+1)\n",
    "        print('Initial standard deviation:        ', s_initial)\n",
    "        print('Final standard deviation:          ', s)\n",
    "        print('Number of rejected points:         ',len(np.invert(b[np.invert(b)])))\n",
    "        print('')\n",
    "\n",
    "    return f,tmp_results,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chebyshev(p,ye,mask):\n",
    "    coef=np.polynomial.chebyshev.chebfit(p[0][mask], p[1][mask], 4)\n",
    "    cont=np.polynomial.chebyshev.chebval(p[0],coef)\n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_default_degrading_wavelength_grid(default_model_wave, synth_res=300000.):\n",
    "    initial_l = dict()\n",
    "    \n",
    "    for ccd in spectrum['available_ccds']:\n",
    "\n",
    "        wave_model_ccd = (default_model_wave > (3+ccd)*1000) & (default_model_wave < (4+ccd)*1000)\n",
    "\n",
    "        synth = np.array(default_model_wave[wave_model_ccd]).T\n",
    "\n",
    "        l_original=synth\n",
    "        #check if the shape of the synthetic spectrum is correct\n",
    "        #if synth.shape[1]!=2: logging.error('Syntehtic spectrum must have shape m x 2.')\n",
    "\n",
    "        #check if the resolving power is high enough\n",
    "        sigma_synth=synth/synth_res\n",
    "        if max(sigma_synth)>=min(spectrum['lsf_ccd'+str(ccd)])*0.95: logging.error('Resolving power of the synthetic spectrum must be higher.')\n",
    "\n",
    "        #check if wavelength calibration of the synthetic spectrum is linear:\n",
    "        if not (synth[1]-synth[0])==(synth[-1]-synth[-2]):\n",
    "            logging.error('Synthetic spectrum must have linear (equidistant) sampling.')\t\t\n",
    "\n",
    "        #current sampling:\n",
    "        sampl=synth[1]-synth[0]\n",
    "        galah_sampl=spectrum['cdelt_ccd'+str(ccd)]\n",
    "\n",
    "        #original sigma\n",
    "        s_original=sigma_synth\n",
    "\n",
    "        #required sigma (resample the resolution map into the wavelength range of the synthetic spectrum)\n",
    "        s_out=np.interp(synth, spectrum['crval_ccd'+str(ccd)]+spectrum['cdelt_ccd'+str(ccd)]*np.arange(len(spectrum['counts_ccd'+str(ccd)])), spectrum['lsf_ccd'+str(ccd)])\n",
    "        \n",
    "        #the sigma of the kernel is:\n",
    "        s=np.sqrt(s_out**2-s_original**2)\n",
    "        \n",
    "        #fit it with the polynomial, so we have a function instead of sampled values:\n",
    "        map_fit=np.poly1d(np.polyfit(synth, s, deg=6))\n",
    "\n",
    "        #create an array with new sampling. The first point is the same as in the spectrum:\n",
    "        l_new=[synth[0]]\n",
    "\n",
    "        #oversampling. If synthetic spectrum sampling is much finer than the size of the kernel, the code would work, but would return badly sampled spectrum. this is because from here on the needed sampling is measured in units of sigma.\n",
    "        oversample=galah_sampl/sampl*10.0\n",
    "\n",
    "        #minimal needed sampling\n",
    "        min_sampl=max(s_original)/sampl/sampl*oversample\n",
    "        \n",
    "        #keep adding samples until end of the wavelength range is reached\n",
    "        while l_new[-1]<synth[-1]+sampl:\n",
    "            # THIS IS THE BOTTLENECK OF THE COMPUTATION\n",
    "            l_new.append(l_new[-1]+map_fit(l_new[-1])/sampl/min_sampl)\n",
    "\n",
    "        initial_l['ccd'+str(ccd)] = np.array(l_new)\n",
    "    return(initial_l)\n",
    "\n",
    "initial_l = calculate_default_degrading_wavelength_grid(default_model_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def synth_resolution_degradation(l, res_map, res_b, synth, synth_res=300000.0, reuse_initial_res_wave_grid=True, initial_l=initial_l):\n",
    "    \"\"\"\n",
    "    Take a synthetic spectrum with a very high  resolution and degrade its resolution to the resolution profile of the observed spectrum. The synthetic spectrum should not be undersampled, or the result of the convolution might be wrong.\n",
    "    Parameters:\n",
    "        synth np array or similar: an array representing the synthetic spectrum. Must have size m x 2. First column is the wavelength array, second column is the flux array. Resolution of the synthetic spectrum must be constant and higher than that of the observed spectrum.\n",
    "        synth_res (float): resolving power of the synthetic spectrum\n",
    "    Returns:\n",
    "        Convolved syntehtic spectrum as a np array of size m x 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    synth=np.array(synth)\n",
    "    l_original=synth[:,0]\n",
    "\n",
    "    #check if the resolving power is high enough\n",
    "    sigma_synth=synth[:,0]/synth_res\n",
    "    if max(sigma_synth)>=min(res_map)*0.95: logging.error('Resolving power of the synthetic spectrum must be higher.')\n",
    "\n",
    "    #check if wavelength calibration of the synthetic spectrum is linear:\n",
    "    if not (synth[:,0][1]-synth[:,0][0])==(synth[:,0][-1]-synth[:,0][-2]):\n",
    "        logging.error('Synthetic spectrum must have linear (equidistant) sampling.')\t\t\n",
    "\n",
    "    #current sampling:\n",
    "    sampl=synth[:,0][1]-synth[:,0][0]\n",
    "    galah_sampl=l[1]-l[0]\n",
    "\n",
    "    #original sigma\n",
    "    s_original=sigma_synth\n",
    "\n",
    "    #oversampling. If synthetic spectrum sampling is much finer than the size of the kernel, the code would work, but would return badly sampled spectrum. this is because from here on the needed sampling is measured in units of sigma.\n",
    "    oversample=galah_sampl/sampl*10.0\n",
    "\n",
    "    if reuse_initial_res_wave_grid == False:        \n",
    "\n",
    "        #required sigma (resample the resolution map into the wavelength range of the synthetic spectrum)\n",
    "        s_out=np.interp(synth[:,0], l, res_map)\n",
    "\n",
    "        #the sigma of the kernel is:\n",
    "        s=np.sqrt(s_out**2-s_original**2)\n",
    "\n",
    "        #fit it with the polynomial, so we have a function instead of sampled values:\n",
    "        map_fit=np.poly1d(np.polyfit(synth[:,0], s, deg=6))\n",
    "\n",
    "        #create an array with new sampling. The first point is the same as in the spectrum:\n",
    "        l_new=[synth[:,0][0]]\n",
    "\n",
    "        #minimal needed sampling\n",
    "        min_sampl=max(s_original)/sampl/sampl*oversample\n",
    "\n",
    "        #keep adding samples until end of the wavelength range is reached\n",
    "        while l_new[-1]<synth[:,0][-1]+sampl:\n",
    "            # THIS IS THE BOTTLENECK OF THE COMPUTATION\n",
    "            l_new.append(l_new[-1]+map_fit(l_new[-1])/sampl/min_sampl)\n",
    "        \n",
    "        l_new = np.array(l_new)\n",
    "    else:\n",
    "        l_new = initial_l\n",
    "        \n",
    "    #interpolate the spectrum to the new sampling:\n",
    "    new_f=np.interp(l_new,synth[:,0],synth[:,1])\n",
    "\n",
    "    kernel_=galah_kern(max(s_original)/sampl*oversample, res_b)\n",
    "\n",
    "    con_f=signal.fftconvolve(new_f,kernel_,mode='same')\n",
    "\n",
    "    return np.array([np.array(l_new),con_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    return z*(z > 0) + 0.01*z*(z < 0)\n",
    "\n",
    "def get_spectrum_from_neural_net(scaled_labels, NN_coeffs):\n",
    "    w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max = NN_coeffs\n",
    "    inside = np.einsum('ij,j->i', w_array_0, scaled_labels) + b_array_0\n",
    "    outside = np.einsum('ij,j->i', w_array_1, leaky_relu(inside)) + b_array_1\n",
    "    spectrum = np.einsum('ij,j->i', w_array_2, leaky_relu(outside)) + b_array_2\n",
    "    return spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_synthetic_spectrum(model_parameters, model_labels, neural_network_model, debug=True):\n",
    "    \n",
    "    model_parameters = np.array(model_parameters)\n",
    "    \n",
    "    if 'teff' in model_labels:\n",
    "        teff = 1000.*model_parameters[model_labels=='teff'][0]\n",
    "    else:\n",
    "        raise ValueError('You have to define Teff as input parameter')\n",
    "\n",
    "    if 'fe_h' in model_labels:\n",
    "        fe_h = model_parameters[model_labels=='fe_h'][0]\n",
    "    else:\n",
    "        raise ValueError('You have to define fe_h as input parameter')\n",
    "\n",
    "    if 'logg' in model_labels:\n",
    "        logg = model_parameters[model_labels=='logg'][0]\n",
    "    else:\n",
    "        logg = spectrum['init_logg']\n",
    "\n",
    "    if 'vmic' in model_labels:\n",
    "        vmic = model_parameters[model_labels=='vmic'][0]\n",
    "    else:\n",
    "        vmic = 1.0\n",
    "\n",
    "    if 'vsini' in model_labels:\n",
    "        vsini = model_parameters[model_labels=='vsini'][0]\n",
    "    else:\n",
    "        vsini = 0.0\n",
    "\n",
    "    if 'li_fe' in model_labels:\n",
    "        li_fe = model_parameters[model_labels=='li_fe'][0]\n",
    "    else:\n",
    "        li_fe = 0.0\n",
    "    \n",
    "    if 'c_fe' in model_labels:\n",
    "        c_fe = model_parameters[model_labels=='c_fe'][0]\n",
    "    else:\n",
    "        c_fe = 0.0\n",
    "\n",
    "    if 'n_fe' in model_labels:\n",
    "        n_fe = model_parameters[model_labels=='n_fe'][0]\n",
    "    else:\n",
    "        n_fe = 0.0\n",
    "\n",
    "    if 'o_fe' in model_labels:\n",
    "        o_fe = model_parameters[model_labels=='o_fe'][0]\n",
    "    else:\n",
    "        o_fe = (-0.4*model_parameters[model_labels=='fe_h'][0]).clip(min=0.0,max=0.4)\n",
    "\n",
    "    if 'na_fe' in model_labels:\n",
    "        na_fe = model_parameters[model_labels=='na_fe'][0]\n",
    "    else:\n",
    "        na_fe = 0.0\n",
    "\n",
    "    if 'mg_fe' in model_labels:\n",
    "        mg_fe = model_parameters[model_labels=='mg_fe'][0]\n",
    "    else:\n",
    "        mg_fe = (-0.4*model_parameters[model_labels=='fe_h'][0]).clip(min=0.0,max=0.4)\n",
    "\n",
    "    if 'al_fe' in model_labels:\n",
    "        al_fe = model_parameters[model_labels=='al_fe'][0]\n",
    "    else:\n",
    "        al_fe = 0.0\n",
    "\n",
    "    if 'si_fe' in model_labels:\n",
    "        si_fe = model_parameters[model_labels=='si_fe'][0]\n",
    "    else:\n",
    "        si_fe = (-0.4*model_parameters[model_labels=='fe_h'][0]).clip(min=0.0,max=0.4)\n",
    "\n",
    "    if 'k_fe' in model_labels:\n",
    "        k_fe = model_parameters[model_labels=='k_fe'][0]\n",
    "    else:\n",
    "        k_fe = 0.0\n",
    "\n",
    "    if 'ca_fe' in model_labels:\n",
    "        ca_fe = model_parameters[model_labels=='ca_fe'][0]\n",
    "    else:\n",
    "        ca_fe = (-0.4*model_parameters[model_labels=='fe_h'][0]).clip(min=0.0,max=0.4)\n",
    "\n",
    "    if 'sc_fe' in model_labels:\n",
    "        sc_fe = model_parameters[model_labels=='sc_fe'][0]\n",
    "    else:\n",
    "        sc_fe = 0.0\n",
    "\n",
    "    if 'ti_fe' in model_labels:\n",
    "        ti_fe = model_parameters[model_labels=='ti_fe'][0]\n",
    "    else:\n",
    "        ti_fe = (-0.4*model_parameters[model_labels=='fe_h'][0]).clip(min=0.0,max=0.4)\n",
    "\n",
    "    if 'v_fe' in model_labels:\n",
    "        v_fe = model_parameters[model_labels=='v_fe'][0]\n",
    "    else:\n",
    "        v_fe = 0.0\n",
    "\n",
    "    if 'cr_fe' in model_labels:\n",
    "        cr_fe = model_parameters[model_labels=='cr_fe'][0]\n",
    "    else:\n",
    "        cr_fe = 0.0\n",
    "\n",
    "    if 'mn_fe' in model_labels:\n",
    "        mn_fe = model_parameters[model_labels=='mn_fe'][0]\n",
    "    else:\n",
    "        mn_fe = 0.0\n",
    "\n",
    "    if 'co_fe' in model_labels:\n",
    "        co_fe = model_parameters[model_labels=='co_fe'][0]\n",
    "    else:\n",
    "        co_fe = 0.0\n",
    "\n",
    "    if 'ni_fe' in model_labels:\n",
    "        ni_fe = model_parameters[model_labels=='ni_fe'][0]\n",
    "    else:\n",
    "        ni_fe = 0.0\n",
    "\n",
    "    if 'cu_fe' in model_labels:\n",
    "        cu_fe = model_parameters[model_labels=='cu_fe'][0]\n",
    "    else:\n",
    "        cu_fe = 0.0\n",
    "\n",
    "    if 'zn_fe' in model_labels:\n",
    "        zn_fe = model_parameters[model_labels=='zn_fe'][0]\n",
    "    else:\n",
    "        zn_fe = 0.0\n",
    "\n",
    "    if 'rb_fe' in model_labels:\n",
    "        rb_fe = model_parameters[model_labels=='rb_fe'][0]\n",
    "    else:\n",
    "        rb_fe = 0.0\n",
    "\n",
    "    if 'sr_fe' in model_labels:\n",
    "        sr_fe = model_parameters[model_labels=='sr_fe'][0]\n",
    "    else:\n",
    "        sr_fe = 0.0\n",
    "\n",
    "    if 'y_fe' in model_labels:\n",
    "        y_fe = model_parameters[model_labels=='y_fe'][0]\n",
    "    else:\n",
    "        y_fe = 0.0\n",
    "\n",
    "    if 'zr_fe' in model_labels:\n",
    "        zr_fe = model_parameters[model_labels=='zr_fe'][0]\n",
    "    else:\n",
    "        zr_fe = 0.0\n",
    "\n",
    "    if 'mo_fe' in model_labels:\n",
    "        mo_fe = model_parameters[model_labels=='mo_fe'][0]\n",
    "    else:\n",
    "        mo_fe = 0.0\n",
    "\n",
    "    if 'ru_fe' in model_labels:\n",
    "        ru_fe = model_parameters[model_labels=='ru_fe'][0]\n",
    "    else:\n",
    "        ru_fe = 0.0\n",
    "\n",
    "    if 'ba_fe' in model_labels:\n",
    "        ba_fe = model_parameters[model_labels=='ba_fe'][0]\n",
    "    else:\n",
    "        ba_fe = 0.0\n",
    "\n",
    "    if 'la_fe' in model_labels:\n",
    "        la_fe = model_parameters[model_labels=='la_fe'][0]\n",
    "    else:\n",
    "        la_fe = 0.0\n",
    "\n",
    "    if 'ce_fe' in model_labels:\n",
    "        ce_fe = model_parameters[model_labels=='ce_fe'][0]\n",
    "    else:\n",
    "        ce_fe = 0.0\n",
    "\n",
    "    if 'nd_fe' in model_labels:\n",
    "        nd_fe = model_parameters[model_labels=='nd_fe'][0]\n",
    "    else:\n",
    "        nd_fe = 0.0\n",
    "\n",
    "    if 'sm_fe' in model_labels:\n",
    "        sm_fe = model_parameters[model_labels=='sm_fe'][0]\n",
    "    else:\n",
    "        sm_fe = 0.0\n",
    "\n",
    "    if 'eu_fe' in model_labels:\n",
    "        eu_fe = model_parameters[model_labels=='eu_fe'][0]\n",
    "    else:\n",
    "        eu_fe = 0.0\n",
    "    \n",
    "    model_labels = np.array([\n",
    "        teff, logg, fe_h, vmic, vsini, li_fe,\n",
    "        c_fe, n_fe, o_fe, na_fe, mg_fe,\n",
    "        al_fe, si_fe, k_fe, ca_fe, sc_fe,\n",
    "        ti_fe, v_fe, cr_fe, mn_fe, co_fe,\n",
    "        ni_fe, cu_fe, zn_fe, rb_fe, sr_fe,\n",
    "        y_fe, zr_fe, mo_fe, ru_fe, ba_fe,\n",
    "        la_fe, ce_fe, nd_fe, sm_fe, eu_fe\n",
    "    ])\n",
    "    \n",
    "    scaled_labels = (model_labels-neural_network_model[-2])/(neural_network_model[-1]-neural_network_model[-2]) - 0.5\n",
    "    model_flux = get_spectrum_from_neural_net(scaled_labels,neural_network_model)\n",
    "\n",
    "    return(\n",
    "        model_flux\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def match_observation_and_model(model_parameters, model_labels, spectrum, neural_network_model, reuse_initial_res_wave_grid=False, debug=True):\n",
    "    \n",
    "    model_parameters = np.array(model_parameters)\n",
    "    if 'vrad' in model_labels:\n",
    "        vrad = model_parameters[model_labels=='vrad'][0]\n",
    "    else:\n",
    "        vrad = spectrum['init_vrad']\n",
    "    cdelt = dict()\n",
    "    crval = dict()\n",
    "        \n",
    "    if debug:\n",
    "        start = time.time()\n",
    "        print(start)\n",
    "    \n",
    "    flux_model = create_synthetic_spectrum(model_parameters, model_labels, neural_network_model, debug)\n",
    "    \n",
    "    if debug:\n",
    "        time_step = time.time()-start\n",
    "        print('reading in cannon model',time_step)\n",
    "    \n",
    "    # at the moment, let's assume cdelt and crval are correct\n",
    "    \n",
    "    for ccd in spectrum['available_ccds']:\n",
    "        \n",
    "        if 'cdelt'+str(ccd) in model_labels:\n",
    "            cdelt['ccd'+str(ccd)] = model_parameters[model_labels=='cdelt'+str(ccd)][0]\n",
    "        else:\n",
    "            cdelt['ccd'+str(ccd)] = 1000*spectrum['cdelt_ccd'+str(ccd)]\n",
    "\n",
    "        if 'crval'+str(ccd) in model_labels:\n",
    "            crval['ccd'+str(ccd)] = model_parameters[model_labels=='crval'+str(ccd)][0]\n",
    "        else:\n",
    "            crval['ccd'+str(ccd)] = spectrum['crval_ccd'+str(ccd)]\n",
    "        \n",
    "        spectrum['wave_ccd'+str(ccd)] = rv_shift(vrad,crval['ccd'+str(ccd)] + cdelt['ccd'+str(ccd)]/1000.*np.arange(len(spectrum['counts_ccd'+str(ccd)])))\n",
    "        \n",
    "        wave_model_ccd = (default_model_wave > (3+ccd)*1000) & (default_model_wave < (4+ccd)*1000)\n",
    "        \n",
    "        # Degrade synthetic spectrum onto LSF\n",
    "        # Note: Synthetic spectra have to be on equidistant wavelength scale!\n",
    "        wave_model_ccd_lsf, flux_model_ccd_lsf = synth_resolution_degradation(\n",
    "            l = spectrum['crval_ccd'+str(ccd)] + spectrum['cdelt_ccd'+str(ccd)]*np.arange(len(spectrum['counts_ccd'+str(ccd)])), \n",
    "            res_map = spectrum['lsf_ccd'+str(ccd)], \n",
    "            res_b = spectrum['lsf_b_ccd'+str(ccd)], \n",
    "            synth = np.array([default_model_wave[wave_model_ccd], flux_model[wave_model_ccd]]).T,\n",
    "            synth_res=300000.0,\n",
    "            reuse_initial_res_wave_grid = reuse_initial_res_wave_grid,\n",
    "            initial_l = initial_l['ccd'+str(ccd)]\n",
    "        )\n",
    "        if debug:\n",
    "            time_step_old = time_step\n",
    "            time_step = time.time()-start\n",
    "            print('degrade flux and sigma ccd'+str(ccd),time_step,time_step-time_step_old)\n",
    "        \n",
    "        # Interpolate model onto right wavelength grid\n",
    "        spectrum['flux_model_ccd'+str(ccd)] = cubic_spline_interpolate(\n",
    "            wave_model_ccd_lsf,\n",
    "            flux_model_ccd_lsf,\n",
    "            spectrum['wave_ccd'+str(ccd)]\n",
    "        )\n",
    "\n",
    "        renormalisation_fit = sclip((spectrum['wave_ccd'+str(ccd)],spectrum['counts_ccd'+str(ccd)]/spectrum['flux_model_ccd'+str(ccd)]),chebyshev,int(3),ye=spectrum['counts_unc_ccd'+str(ccd)],su=5,sl=5,min_data=100,verbose=False)\n",
    "        spectrum['flux_obs_ccd'+str(ccd)] = spectrum['counts_ccd'+str(ccd)]/renormalisation_fit[0]\n",
    "        spectrum['flux_obs_unc_ccd'+str(ccd)] = spectrum['counts_unc_ccd'+str(ccd)]/renormalisation_fit[0]\n",
    "            \n",
    "    # prepare input for likelihood (we will combine sigma2 and s2 later):\n",
    "    # -0.5 * sum((data-model))**2/sigma) + log(sigma)\n",
    "    wave = np.concatenate([spectrum['wave_ccd'+str(ccd)] for ccd in spectrum['available_ccds']])\n",
    "    data = np.concatenate([spectrum['flux_obs_ccd'+str(ccd)] for ccd in spectrum['available_ccds']])\n",
    "    sigma2 = np.concatenate([spectrum['flux_obs_unc_ccd'+str(ccd)] for ccd in spectrum['available_ccds']])**2\n",
    "    model = np.concatenate([spectrum['flux_model_ccd'+str(ccd)] for ccd in spectrum['available_ccds']])\n",
    "\n",
    "    return(wave,data,sigma2,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_best_available_neutral_network_model(teff, logg, fe_h):\n",
    "\n",
    "    model_index = grid_index_tree.query([teff/1000.,logg,fe_h],k=1)[1]\n",
    "\n",
    "    model_teff_logg_feh = str(int(grids['teff_subgrid'][model_index]))+'_'+\"{:.2f}\".format(grids['logg_subgrid'][model_index])+'_'+\"{:.2f}\".format(grids['fe_h_subgrid'][model_index])\n",
    "\n",
    "    closest_model = model_teff_logg_feh\n",
    "\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Searching for closest neutral network')\n",
    "        print('Need: '+str(int(teff)), \"{:.2f}\".format(logg), \"{:.2f}\".format(fe_h))\n",
    "    \n",
    "    model_name = galah_dr4_directory+'spectrum_interpolation/neural_networks/models/galah_dr4_neutral_network_3x3x3_'+model_teff_logg_feh+'_36labels.npz'\n",
    "    \n",
    "    #if sobject_id in [140305003201336,160331004301396,160419003601126,161013003801353]:\n",
    "    #    print('Manual overwrite to grid 1482 (4750_2.00_-2.00)')\n",
    "    #    closest_model = '4750_2.00_-2.00'\n",
    "    #    model_teff_logg_feh = '4750_2.00_-2.00'\n",
    "    #    model_name = galah_dr4_directory+'spectrum_interpolation/neural_networks/models/galah_dr4_neutral_network_3x3x3_'+closest_model+'_36labels.npz'\n",
    "    #    model_index = 1482\n",
    "    \n",
    "    try:\n",
    "        tmp = np.load(model_name)\n",
    "\n",
    "        used_model = closest_model\n",
    "        print('Using 3x3x3 model '+model_teff_logg_feh+' (closest)')\n",
    "\n",
    "        if (spectrum['flag_sp'] & flag_sp_closest_3x3x3_model_not_available) > 0:\n",
    "            spectrum['flag_sp'] -= flag_sp_closest_3x3x3_model_not_available\n",
    "\n",
    "    except:\n",
    "        \n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Could not load 3x3x3 model '+model_teff_logg_feh+' (closest)')\n",
    "        \n",
    "        if (spectrum['flag_sp'] & flag_sp_closest_3x3x3_model_not_available) == 0:\n",
    "            spectrum['flag_sp'] += flag_sp_closest_3x3x3_model_not_available\n",
    "\n",
    "        model_index = grid_avail_index_tree.query([teff/1000.,logg,fe_h],k=1)[1]\n",
    "        model_teff_logg_feh = str(int(grids_avail['teff_subgrid'][model_index]))+'_'+\"{:.2f}\".format(grids_avail['logg_subgrid'][model_index])+'_'+\"{:.2f}\".format(grids_avail['fe_h_subgrid'][model_index])\n",
    "        model_name = galah_dr4_directory+'spectrum_interpolation/neural_networks/models/galah_dr4_neutral_network_3x3x3_'+model_teff_logg_feh+'_36labels.npz'\n",
    "        print('Using closest available old 3x3x3 model '+model_teff_logg_feh+' instead')\n",
    "        tmp = np.load(model_name)\n",
    "\n",
    "        used_model = model_teff_logg_feh\n",
    "\n",
    "    w_array_0 = tmp[\"w_array_0\"]\n",
    "    w_array_1 = tmp[\"w_array_1\"]\n",
    "    w_array_2 = tmp[\"w_array_2\"]\n",
    "    b_array_0 = tmp[\"b_array_0\"]\n",
    "    b_array_1 = tmp[\"b_array_1\"]\n",
    "    b_array_2 = tmp[\"b_array_2\"]\n",
    "    x_min = tmp[\"x_min\"]\n",
    "    x_max = tmp[\"x_max\"]\n",
    "    tmp.close()\n",
    "    neural_network_model = (w_array_0, w_array_1, w_array_2, b_array_0, b_array_1, b_array_2, x_min, x_max)\n",
    "    \n",
    "    model_labels = np.loadtxt(galah_dr4_directory+'spectrum_interpolation/gradient_spectra/'+used_model+'/recommended_fit_labels_'+used_model+'.txt',usecols=(0,),dtype=str)\n",
    "    \n",
    "    # We tested also fitting c_fe for stars below [Fe/H] < -1; but if there is C2, it would be strong!\n",
    "    #if fe_h < -1:\n",
    "    #    if 'c_fe' not in model_labels:\n",
    "    #        if sys.argv[1] == '-f': print('[Fe/H] < -1, adding [C/Fe] to models')\n",
    "    #        model_labels = list(model_labels)\n",
    "    #        if 'li_fe' not in model_labels:\n",
    "    #            model_labels.insert(5,'c_fe')\n",
    "    #        else:\n",
    "    #            model_labels.insert(6,'c_fe')\n",
    "    #        model_labels = np.array(model_labels)\n",
    "\n",
    "    # We tested also fitting c_fe for stars below [Fe/H] < -1; but if there is C2, it would be strong!\n",
    "    # if (fe_h < -1) & (teff < 5500) & (logg < 3.5):\n",
    "    #     if 'al_fe' not in model_labels:\n",
    "    #         if sys.argv[1] == '-f': print('Giant with [Fe/H] < -1, adding [Al/Fe] to models')\n",
    "    #         model_labels = list(model_labels)\n",
    "    #         model_labels.insert(11,'al_fe')\n",
    "    #         model_labels = np.array(model_labels)\n",
    "    # This idea was abandoned, because SME would actually not predict a reasonably strong [Al/Fe] in metal-poor giants\n",
    "                \n",
    "    if 1 not in spectrum['available_ccds']:\n",
    "        print('CCD1 not available, cannot fit Zn.')\n",
    "        for label in ['zn_fe']:\n",
    "            model_labels = np.delete(model_labels, model_labels == label)\n",
    "\n",
    "    if 2 not in spectrum['available_ccds']:\n",
    "        print('CCD2 not available, cannot fit Cu.')\n",
    "        for label in ['cu_fe']:\n",
    "            model_labels = np.delete(model_labels, model_labels == label)\n",
    "\n",
    "    if 3 not in spectrum['available_ccds']:\n",
    "        print('CCD3 not available, cannot fit Li and Eu.')\n",
    "        for label in ['li_fe','eu_fe']:\n",
    "            model_labels = np.delete(model_labels, model_labels == label)\n",
    "\n",
    "    if 4 not in spectrum['available_ccds']:\n",
    "        print('CCD4 not available, cannot fit N, O, K, and Rb.')\n",
    "        for label in ['n_fe','o_fe','k_fe','rb_fe']:\n",
    "            model_labels = np.delete(model_labels, model_labels == label)\n",
    "\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('')\n",
    "        print('Fitting the following labels:')\n",
    "        print(model_labels)\n",
    "        print('')\n",
    "    \n",
    "    return(neural_network_model, closest_model, used_model, model_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adjust_rv(current_rv, wave_input_for_rv, data_input_for_rv, sigma2_input_for_rv, model_input_for_rv, small_rv_window = 20):\n",
    "\n",
    "    text = '\\n Assessing RVs: Red Pipeline = '\n",
    "    if np.isfinite(init_values_table['vrad_red'][sobject_id_initial_index]):\n",
    "        text = text+\"{:.2f}\".format(init_values_table['vrad_red'][sobject_id_initial_index])+', '\n",
    "    else:\n",
    "        text = text+'NaN, '\n",
    "    text = text+' Gaia DR3 = '\n",
    "    if np.isfinite(init_values_table['vrad_gaia'][sobject_id_initial_index]):\n",
    "        text = text+\"{:.2f}\".format(init_values_table['vrad_gaia'][sobject_id_initial_index])+', '\n",
    "    else:\n",
    "        text = text+'NaN'\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(text)\n",
    "    \n",
    "    neg_rv_corr = -1000\n",
    "    pos_rv_corr = 1000\n",
    "    bin_rv_corr = 1001\n",
    "    rv_res = (pos_rv_corr-neg_rv_corr)/(bin_rv_corr-1)\n",
    "\n",
    "    rv_adjustment_array = np.linspace(neg_rv_corr,pos_rv_corr,bin_rv_corr)\n",
    "    rv_adjustment_chi2 = np.zeros(len(rv_adjustment_array))\n",
    "\n",
    "    for index, rv_correction in enumerate(rv_adjustment_array):\n",
    "\n",
    "        wave_shifted = rv_shift(rv_correction - current_rv,wave_input_for_rv)\n",
    "        wave_shifted_flux = np.interp(wave_shifted, wave_input_for_rv, model_input_for_rv, left=-1000, right=-1000)\n",
    "        available_wavelength_points = np.where(wave_shifted_flux!=-1000)[0]\n",
    "\n",
    "        rv_adjustment_chi2[index] = np.median(\n",
    "                np.abs(\n",
    "                    wave_shifted_flux[available_wavelength_points] - data_input_for_rv[available_wavelength_points]\n",
    "                )/sigma2_input_for_rv[available_wavelength_points]\n",
    "            )\n",
    "\n",
    "    rv_adjustment_chi2 = np.min(rv_adjustment_chi2) / rv_adjustment_chi2\n",
    "    suggested_shift_broad = rv_adjustment_array[np.argmax(rv_adjustment_chi2)]\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(\"{:.1f}\".format(suggested_shift_broad)+' on grid covering '+\"{:.1f}\".format(neg_rv_corr)+'..('+\"{:.1f}\".format(rv_res)+')..'+\"{:.1f}\".format(pos_rv_corr))\n",
    "\n",
    "    f, gs = plt.subplots(1,3,figsize=(9,3))\n",
    "\n",
    "    ax = gs[0]\n",
    "    ax.set_xlabel(r'$v_\\mathrm{rad}~/~\\mathrm{km\\,s^{-1}}$')\n",
    "    ax.set_ylabel(r'$1/\\chi^2$')\n",
    "\n",
    "    # Analyse for multiple peaks\n",
    "    peaks,peaks_info=signal.find_peaks(rv_adjustment_chi2, width=2, distance=3, height=0.15, prominence=0.05)\n",
    "    \n",
    "    height_sorted = np.argsort(peaks_info['peak_heights'])[::-1]\n",
    "    peaks = peaks[height_sorted]\n",
    "    peak_heights = peaks_info['peak_heights'][height_sorted]\n",
    "    peak_prominence = peaks_info['prominences'][height_sorted]\n",
    "    \n",
    "    ax.plot(\n",
    "        rv_adjustment_array,\n",
    "        rv_adjustment_chi2,\n",
    "        c = 'k', lw=1\n",
    "    )\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('   ',rv_adjustment_array[peaks],'peaks found by scipy.signal.finds_peaks')\n",
    "\n",
    "    spectrum['rv_peak_nr'] = np.int32(len(peaks))\n",
    "\n",
    "    if len(peaks) > 0:\n",
    "        spectrum['rv_peak_1'] = float(rv_adjustment_array[peaks[0]])\n",
    "        spectrum['rv_peak_1_h'] = float(peak_heights[0])\n",
    "        spectrum['rv_peak_1_p'] = float(peak_prominence[0])\n",
    "    else:\n",
    "        print('No peaks found. Assuming that initial RV must have been close to correct one')\n",
    "        print('Looking around '+\"{:.2f}\".format(current_rv))\n",
    "        suggested_shift_broad = current_rv\n",
    "        spectrum['rv_peak_1'] = np.NaN\n",
    "        spectrum['rv_peak_1_h'] = np.NaN\n",
    "        spectrum['rv_peak_1_p'] = np.NaN\n",
    "    if len(peaks) > 1:\n",
    "        spectrum['rv_peak_2'] = float(rv_adjustment_array[peaks[1]])\n",
    "        spectrum['rv_peak_2_h'] = float(peak_heights[1])\n",
    "        spectrum['rv_peak_2_p'] = float(peak_prominence[1])\n",
    "        print('   ','Multiple peaks found! Suggest binary analysis and save 2 highest peaks')\n",
    "    else:\n",
    "        spectrum['rv_peak_2'] = np.NaN\n",
    "        spectrum['rv_peak_2_h'] = np.NaN\n",
    "        spectrum['rv_peak_2_p'] = np.NaN\n",
    "    for peak in peaks:\n",
    "        ax.axvline(rv_adjustment_array[peak], c = 'orange', ls='dashed')\n",
    "    if len(peaks) <= 3:\n",
    "        ax.text(0.1,0.8,str(len(peaks))+' Peak(s): \\n '+' & '.join([str(int(peak)) for peak in rv_adjustment_array[peaks]]) ,transform=ax.transAxes,bbox=dict(boxstyle='round', facecolor='w', alpha=0.85))\n",
    "    else:\n",
    "        ax.text(0.1,0.9,str(len(peaks))+' Peak(s)',transform=ax.transAxes,bbox=dict(boxstyle='round', facecolor='w', alpha=0.85))\n",
    "\n",
    "    ax = gs[1]\n",
    "    ax.set_xlabel(r'$v_\\mathrm{rad}~/~\\mathrm{km\\,s^{-1}}$')\n",
    "    ax.set_ylabel(r'$1/\\chi^2$')\n",
    "    ax.plot(\n",
    "        rv_adjustment_array,\n",
    "        rv_adjustment_chi2,\n",
    "        c = 'k', lw = 1\n",
    "    )\n",
    "    if np.isfinite(init_values_table['vrad_red'][sobject_id_initial_index]):\n",
    "        ax.axvline(init_values_table['vrad_red'][sobject_id_initial_index], c = 'r', label = 'Red Pipe. \\n '+\"{:.2f}\".format(init_values_table['vrad_red'][sobject_id_initial_index]))\n",
    "    if np.isfinite(init_values_table['vrad_gaia'][sobject_id_initial_index]):\n",
    "        ax.axvline(init_values_table['vrad_gaia'][sobject_id_initial_index], c = 'C0', ls='dashed', label = '$Gaia$ DR3 \\n '+\"{:.2f}\".format(init_values_table['vrad_gaia'][sobject_id_initial_index]))\n",
    "    ax.legend(loc='upper left', handlelength = 1)\n",
    "\n",
    "    neg_rv_corr = suggested_shift_broad - small_rv_window\n",
    "    pos_rv_corr = suggested_shift_broad + small_rv_window\n",
    "    bin_rv_corr = 1001\n",
    "    rv_res = (pos_rv_corr-neg_rv_corr)/(bin_rv_corr-1)\n",
    "\n",
    "    rv_adjustment_array = np.linspace(neg_rv_corr,pos_rv_corr,bin_rv_corr)\n",
    "    rv_adjustment_chi2 = np.zeros(len(rv_adjustment_array))\n",
    "\n",
    "    for index, rv_correction in enumerate(rv_adjustment_array):\n",
    "\n",
    "        wave_shifted = rv_shift(rv_correction - current_rv,wave_input_for_rv)\n",
    "        wave_shifted_flux = np.interp(wave_shifted, wave_input_for_rv, model_input_for_rv, left=-1000, right=-1000)\n",
    "        available_wavelength_points = np.where(wave_shifted_flux!=-1000)[0]\n",
    "\n",
    "        rv_adjustment_chi2[index] = np.median(\n",
    "                np.abs(\n",
    "                    wave_shifted_flux[available_wavelength_points] - data_input_for_rv[available_wavelength_points]\n",
    "                )/sigma2_input_for_rv[available_wavelength_points]\n",
    "            )\n",
    "\n",
    "    rv_adjustment_chi2 = np.min(rv_adjustment_chi2)/rv_adjustment_chi2\n",
    "\n",
    "    suggested_shift_fine = rv_adjustment_array[np.argmax(rv_adjustment_chi2)]\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(\"{:.2f}\".format(suggested_shift_fine)+' on grid covering '+\"{:.2f}\".format(neg_rv_corr)+'..('+\"{:.2f}\".format(rv_res)+')..'+\"{:.2f}\".format(pos_rv_corr))\n",
    "\n",
    "    ax = gs[2]\n",
    "    ax.set_xlabel(r'$v_\\mathrm{rad}~/~\\mathrm{km\\,s^{-1}}$')\n",
    "    ax.set_ylabel(r'$1/\\chi^2$')\n",
    "    ax.plot(\n",
    "        rv_adjustment_array,\n",
    "        rv_adjustment_chi2,\n",
    "        c = 'k', lw = 1\n",
    "    )\n",
    "    if np.isfinite(init_values_table['vrad_red'][sobject_id_initial_index]):\n",
    "        ax.axvline(init_values_table['vrad_red'][sobject_id_initial_index], c = 'r')#, label = 'Red Pipeline')\n",
    "    if np.isfinite(init_values_table['vrad_gaia'][sobject_id_initial_index]):\n",
    "        ax.axvline(init_values_table['vrad_gaia'][sobject_id_initial_index], c = 'C0', ls='dashed')#, label = '$Gaia$ DR3')\n",
    "    plt.tight_layout(w_pad=0)\n",
    "\n",
    "    def gauss(x, H, A, x0, sigma):\n",
    "        return H + abs(A) * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "    def gauss_fit(x, y):\n",
    "        mean = sum(x * y) / sum(y)\n",
    "        sigma = np.sqrt(sum(y * (x - mean) ** 2) / sum(y))\n",
    "        popt, pcov = curve_fit(gauss, x, y, p0=[min(y), 1., mean, sigma])\n",
    "        return popt, pcov\n",
    "\n",
    "    gauss_popt, gauss_pcov = gauss_fit(rv_adjustment_array, rv_adjustment_chi2)\n",
    "    ax.plot(rv_adjustment_array,gauss(rv_adjustment_array, *gauss_popt), c='orange', label='Fit: '+\"{:.2f}\".format(gauss_popt[2])+'$ \\pm $'+\"{:.2f}\".format(np.sqrt(np.diag(gauss_pcov)[2])))\n",
    "    ax.legend(loc='lower center')\n",
    "    ax.axvline(gauss_popt[2], c = 'orange', label = 'Fit')\n",
    "    ax.set_xlim(suggested_shift_fine - 1.75 * small_rv_window, suggested_shift_fine + 1.75 * small_rv_window)\n",
    "\n",
    "    file_directory = galah_dr4_directory+'analysis_products/'+str(spectrum['sobject_id'])[:6]+'/'+str(spectrum['sobject_id'])+'/'\n",
    "    Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.savefig(file_directory+str(spectrum['sobject_id'])+'_plxcom_fit_rv.png',overwrite=True,bbox_inches='tight')\n",
    "\n",
    "    new_rv = gauss_popt[2]\n",
    "    new_e_rv = np.sqrt(np.diag(gauss_pcov)[2])\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(\"{:.2f}\".format(gauss_popt[2])+' ± '+\"{:.2f}\".format(np.sqrt(np.diag(gauss_pcov)[2]))+' km/s based on Gaussian fit. Updating to this value')\n",
    "\n",
    "    if np.isfinite(init_values_table['vrad_gaia'][sobject_id_initial_index]):\n",
    "        if abs(gauss_popt[2] - init_values_table['vrad_gaia'][sobject_id_initial_index]) > 500:\n",
    "            print('Difference between Gaia and this RV is > 500 km/s. Using Gaia instead')\n",
    "            new_rv = init_values_table['vrad_gaia'][sobject_id_initial_index]\n",
    "            new_e_rv = 1000.\n",
    "        \n",
    "    # show plot if working interactively\n",
    "    if sys.argv[1] == '-f': plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return(new_rv, new_e_rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age_mass(teff, logg, loglum, m_h, e_teff = 100, e_logg = 0.5, e_loglum = 0.1, e_m_h = 0.2):\n",
    "\n",
    "    e_loglum = e_loglum * loglum\n",
    "    \n",
    "    # Make sure that [Fe/H] stays within parsec grid limits\n",
    "    unique_m_h = np.unique(parsec['m_h'])\n",
    "    if m_h < unique_m_h[0]:\n",
    "        m_h = unique_m_h[0] + 0.001\n",
    "        print('adjust m_h input to ',m_h)\n",
    "    if m_h > unique_m_h[-1]:\n",
    "        m_h = unique_m_h[-1] - 0.001\n",
    "        print('adjust m_h input to ',m_h)\n",
    "        \n",
    "    # Make sure we have at least 2 [Fe/H] dimensions to integrate over\n",
    "    lower_boundary_m_h = np.argmin(np.abs(unique_m_h - (m_h - e_m_h)))\n",
    "    upper_boundary_m_h = np.argmin(np.abs(unique_m_h - (m_h + e_m_h)))\n",
    "    if lower_boundary_m_h == upper_boundary_m_h:\n",
    "        if lower_boundary_m_h == 0:\n",
    "            upper_boundary_m_h = 1\n",
    "        if lower_boundary_m_h == len(unique_m_h)-1:\n",
    "            lower_boundary_m_h = len(unique_m_h)-2\n",
    "    \n",
    "    # find all relevant isochrones points\n",
    "    relevant_isochrone_points = (\n",
    "        (parsec['logT'] > np.log10(teff - e_teff)) & \n",
    "        (parsec['logT'] < np.log10(teff + e_teff)) &\n",
    "        (parsec['logg'] > logg - e_logg) & \n",
    "        (parsec['logg'] < logg + e_logg) &\n",
    "        (parsec['logL'] > loglum - e_loglum) & \n",
    "        (parsec['logL'] < loglum + e_loglum) &\n",
    "        (parsec['m_h']  >= unique_m_h[lower_boundary_m_h]) & \n",
    "        (parsec['m_h']  <= unique_m_h[upper_boundary_m_h])\n",
    "    )\n",
    "    # if len(parsec['logT'][relevant_isochrone_points]) < 10:\n",
    "    #     print('Only '+str(len(parsec['logT'][relevant_isochrone_points]))+' isochrones points available')\n",
    "    \n",
    "    # \n",
    "    model_points = np.array([\n",
    "        10**parsec['logT'][relevant_isochrone_points],\n",
    "        parsec['logg'][relevant_isochrone_points],\n",
    "        parsec['logL'][relevant_isochrone_points],\n",
    "        parsec['m_h'][relevant_isochrone_points]\n",
    "    ]).T\n",
    "    \n",
    "    # find normalising factor\n",
    "    norm = np.log(np.sqrt((2.*np.pi)**4.*np.prod(np.array([e_teff, e_logg, e_loglum ,e_m_h])**2)))\n",
    "    \n",
    "    # sum up lnProb and weight ages/masses by \n",
    "    lnProb = - np.sum(((model_points - [teff, logg, loglum, m_h])/[e_teff, e_logg, e_loglum, e_m_h])**2, axis=1) - norm    \n",
    "    age = np.sum(10**parsec['logAge'][relevant_isochrone_points] * np.exp(lnProb)/10**9)\n",
    "    mass = np.sum(parsec['mass'][relevant_isochrone_points] * np.exp(lnProb))\n",
    "    \n",
    "    # Normalise by probability\n",
    "    Prob_sum = np.sum(np.exp(lnProb))\n",
    "    age /= Prob_sum\n",
    "    mass /= Prob_sum\n",
    "    \n",
    "    return(age, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracket by +/-nn values over (irregular) grid. If idx True, then indices \n",
    "# are returned instead\n",
    "def bracket(inval,grval,nn,idx=False):\n",
    "    \n",
    "    norep = np.sort(np.array(list(dict.fromkeys(list(grval)))))\n",
    "    \n",
    "    x1    = np.where(norep<=inval)\n",
    "    x2    = np.where(norep>inval)\n",
    "    \n",
    "    if idx==False:\n",
    "        lo = norep[x1][-nn::]\n",
    "        up = norep[x2][0:nn]        \n",
    "    else:\n",
    "        lo = x1[0][-nn::]\n",
    "        up = x2[0][0:nn]\n",
    "        \n",
    "    return(lo,up)\n",
    "\n",
    "# linear interpolation for 2 points, Akima for more. Returns nan if \n",
    "# not possible or if extrapolated. The MARCS grid of BC used here is ordered\n",
    "# such that gridt is monotonic. If not, sorting is necessary.\n",
    "def mal(val,gridt,gridbc,dset):\n",
    "    if len(dset[0])>2:\n",
    "        mfun = Akima1DInterpolator(gridt[dset],gridbc[dset])\n",
    "        itp  = mfun(val)\n",
    "    if len(dset[0])==2:\n",
    "        mfun = interp1d(gridt[dset],gridbc[dset],bounds_error=False) \n",
    "        itp  = mfun(val)        \n",
    "    if len(dset[0])<2:\n",
    "        itp = np.nan\n",
    "    return(itp)\n",
    "\n",
    "\n",
    "# read input tables of BCs for several values of E(B-V)\n",
    "files = ['../auxiliary_information/BC_Tables/grid/STcolors_2MASS_GaiaDR2_EDR3_Rv3.1_EBV_0.00.dat']\n",
    "gebv   = [0.0]\n",
    "gri_bc = []\n",
    "\n",
    "kk=0\n",
    "for f in files:\n",
    "\n",
    "    grid = Table.read(f,format='ascii')\n",
    "    if kk==0:\n",
    "        gteff, gfeh, glogg = grid['Teff'],grid['feh'],grid['logg']\n",
    "\n",
    "    bc_g2  = grid['mbol']-grid['G2']\n",
    "    bc_bp2 = grid['mbol']-grid['BP2']\n",
    "    bc_rp2 = grid['mbol']-grid['RP2']\n",
    "\n",
    "    bc_g3  = grid['mbol']-grid['G3']\n",
    "    bc_bp3 = grid['mbol']-grid['BP3']\n",
    "    bc_rp3 = grid['mbol']-grid['RP3']\n",
    "\n",
    "    bc_j   = grid['mbol']-grid['J']\n",
    "    bc_h   = grid['mbol']-grid['H']\n",
    "    bc_k   = grid['mbol']-grid['Ks']\n",
    "\n",
    "    tmp = np.transpose([bc_g2,bc_bp2,bc_rp2,bc_g3,bc_bp3,bc_rp3,bc_j,bc_h,bc_k])\n",
    "    gri_bc.append(tmp)\n",
    "\n",
    "    kk=kk+1\n",
    "\n",
    "gebv   = np.array(gebv)\n",
    "gri_bc = np.array(gri_bc)\n",
    "\n",
    "\n",
    "# compute Bolometric Corrections for stars of known input parameters\n",
    "def bcstar(teff,logg,feh,alpha_fe):\n",
    "    \n",
    "#     teff = np.min([np.max([teff,np.min(grid['Teff'])]),np.max(grid['Teff'])])\n",
    "#     if teff < 3900:\n",
    "#         logg = np.min([np.max([logg,np.min(grid['logg'])]),5.5])\n",
    "#     else:\n",
    "#         logg = np.min([np.max([logg,np.min(grid['logg'])]),5.0])\n",
    "#     feh = np.min([np.max([feh,np.min(grid['feh'])]),np.max(grid['feh'])])\n",
    "    \n",
    "    frange = [8]\n",
    "    flist = ['BC_Ks']\n",
    "    rmi = [8]\n",
    "\n",
    "    itp_bc = np.nan\n",
    "    arr_bc  = np.nan\n",
    "\n",
    "    fold      = [feh]\n",
    "        \n",
    "    # take +/-3 steps in [Fe/H] grid\n",
    "    snip = np.concatenate(bracket(fold,gfeh,3))\n",
    "    itp1 = np.zeros((len(snip)))+np.nan\n",
    "    \n",
    "    for k in range(len(snip)):\n",
    "\n",
    "        x0   = np.where((gfeh==snip[k]) & (np.abs(glogg-logg)<1.1))\n",
    "        lg0  = np.array(list(dict.fromkeys(list(glogg[x0]))))\n",
    "        itp0 = np.zeros((len(lg0)))+np.nan\n",
    "\n",
    "        # at given logg and feh, range of Teff to interpolate across\n",
    "        for j in range(len(lg0)):\n",
    "            ok      = np.where((np.abs(gteff-teff)<1000) & \\\n",
    "                               (gfeh==snip[k]) & (glogg==lg0[j]))\n",
    "\n",
    "            itp0[j] = mal(teff,gteff,gri_bc[0,:,8],ok)\n",
    "\n",
    "        # remove any nan, in case. Either of itp[?,:,:] is enough\n",
    "        k0 = np.where(np.isnan(itp0[:])==False)\n",
    "        # interpolate in logg at correct Teff\n",
    "        itp1[k] = mal(logg,lg0,itp0[:],k0)\n",
    "        \n",
    "    k1  = np.where(np.isnan(itp1[:])==False)\n",
    "    \n",
    "    bc_ks = mal(fold,snip,itp1[:],k1)\n",
    "\n",
    "    if np.isnan(bc_ks):\n",
    "        \n",
    "        bc_grid = np.genfromtxt('../auxiliary_information/BC_Tables/grid/STcolors_2MASS_GaiaDR2_EDR3_Rv3.1_EBV_0.00.dat',names=True)\n",
    "        file = open('../auxiliary_information/BC_Tables/grid/bc_grid_kdtree_ebv_0.00.pickle','rb')\n",
    "        bc_kdtree = pickle.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        bc_distance_matches, bc_closest_matches = bc_kdtree.query(np.array([np.log10(teff),logg,feh,alpha_fe]).T,k=8)\n",
    "        bc_ks = np.average(bc_grid['mbol'][bc_closest_matches] - bc_grid['Ks'][bc_closest_matches],weights=bc_distance_matches,axis=-1)\n",
    "        \n",
    "    else:\n",
    "        bc_ks = bc_ks[0]\n",
    "        \n",
    "    return(bc_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logg_parallax(teff, logg_in, fe_h, e_teff = 100, e_logg = 0.25, e_m_h = 0.2):\n",
    "    \n",
    "    if fe_h < -1:\n",
    "        alpha_fe = 0.4\n",
    "    elif fe_h > 0:\n",
    "        alpha_fe = 0.0\n",
    "    else:\n",
    "        alpha_fe = -0.4 *fe_h\n",
    "    \n",
    "    m_h = fe_h + np.log10(10**alpha_fe * 0.694 + 0.306)\n",
    "    \n",
    "    bc_ks = bcstar(teff, logg_in, fe_h, alpha_fe)\n",
    "    \n",
    "    loglbol = - 0.4 * (extra_info['ks_m'] - 5.0*np.log10(extra_info['r_med']/10.) + bc_ks - extra_info['a_ks'] - 4.75)#[0]\n",
    "    # Take into account uncertainties of Ks, distance, and adds uncertainties of +- 0.05 mag for A(Ks) and BC(Ks)\n",
    "    loglbol_lo = - 0.4 * (extra_info['ks_m'] + extra_info['ks_msigcom'] - 5.0*np.log10(extra_info['r_lo']/10.) + (bc_ks + 0.05) - (extra_info['a_ks'] - 0.05) - 4.75)#[0]\n",
    "    loglbol_hi = - 0.4 * (extra_info['ks_m'] - extra_info['ks_msigcom'] - 5.0*np.log10(extra_info['r_hi']/10.) + (bc_ks - 0.05) - (extra_info['a_ks'] + 0.05) - 4.75)#[0]\n",
    "    \n",
    "    e_loglum = 0.5*(loglbol_hi-loglbol_lo) / loglbol\n",
    "        \n",
    "    age, mass = calculate_age_mass(teff, logg_in, loglbol, m_h, e_teff, e_logg, e_loglum, e_m_h)\n",
    "    if np.isnan(mass):\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Mass could not be estimated, trying again with 2x errors')\n",
    "        age, mass = calculate_age_mass(teff, logg_in, loglbol, m_h, e_teff*2, e_logg*2, e_loglum*2, e_m_h*2)\n",
    "        if np.isnan(mass):\n",
    "            if sys.argv[1] == '-f':\n",
    "                print('Mass could not be estimated, trying again with 3x the errors')\n",
    "            age, mass = calculate_age_mass(teff, logg_in, loglbol, m_h, e_teff*3, e_logg*3, e_loglum*3, e_m_h*3)\n",
    "            if np.isnan(mass):\n",
    "                if sys.argv[1] == '-f':\n",
    "                    print('Mass could not be estimated, assuming 1Mbol')\n",
    "                mass = 1.0\n",
    "                age = np.NaN\n",
    "        \n",
    "    return(4.438 + np.log10(mass) + 4*np.log10(teff/5772.) - loglbol, mass, age, bc_ks, 10**loglbol, loglbol_lo, loglbol_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_logg_mass_age_bc_ks_lbol(teff, logg_in, fe_h):\n",
    "    logg_out, mass, age, bc_ks, lbol, loglbol_lo, loglbol_hi = calculate_logg_parallax(teff, logg_in, fe_h)        \n",
    "    iteration = 0\n",
    "    while (abs(logg_out - logg_in) > 0.01) & (iteration < 4):\n",
    "        logg_in = logg_out\n",
    "        logg_out, mass, age, bc_ks, lbol, loglbol_lo, loglbol_hi = calculate_logg_parallax(teff, logg_in, fe_h)\n",
    "        iteration += 1\n",
    "    if sys.argv[1] == '-f':\n",
    "        print(r'log(lbol), $'+\"{:.2f}\".format(np.log10(lbol))+'_{-'+\"{:.2f}\".format(np.log10(lbol)-loglbol_lo)+'}^{+'+\"{:.2f}\".format(loglbol_hi-np.log10(lbol))+'}$')\n",
    "\n",
    "    if logg_out < -0.5:\n",
    "        logg_out = -0.49\n",
    "        print('logg too low. Resetting to -0.5')\n",
    "    if logg_out > 5.5 :\n",
    "        logg_out = 5.49\n",
    "        print('logg too high. Resetting to 5.5')\n",
    "        \n",
    "    return(mass, age, bc_ks, lbol, logg_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising labels \n",
    "For computational reasons, we fix RV here and fit only the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise important global parameters\n",
    "\n",
    "spectrum['opt_loop'] = 0\n",
    "converged = False\n",
    "maximum_loops = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimise_labels(input_model_parameters, input_model, input_model_name, input_model_labels, input_wave, input_data, input_sigma, input_unmasked, iteration):\n",
    "    \n",
    "    # Make sure we have some reasonable initial abundances\n",
    "    # Start by setting abundances of hot stars > 6000K back to 0\n",
    "    if input_model_parameters[0] > 6.:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Star has Teff > 6000 K, nulling abundances (exepct for Li)')\n",
    "        # We will actually apply that further down\n",
    "    if input_model_parameters[0] > 5.5:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Star has Teff > 5500 K, nulling CNO')\n",
    "        # We will actually apply that further down\n",
    "        \n",
    "    if input_model_parameters[3] < 0.:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Negative vmic detected. Setting to 0.5 km/s')\n",
    "        input_model_parameters[3] = 0.5\n",
    "\n",
    "    if input_model_parameters[4] < 0.:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Negative vsini detected. Setting to 1.0 km/s')\n",
    "        input_model_parameters[4] = 1\n",
    "\n",
    "    for parameter_index, value in enumerate(input_model_parameters):\n",
    "        # Some abundance values may be extreme at the beginning - let's bring them back within the narrow training edges in this case\n",
    "        if input_model_labels[parameter_index] not in ['teff', 'logg', 'fe_h', 'vmic', 'vsini']:\n",
    "            if input_model_parameters[0] > 6.:\n",
    "                if input_model_labels[parameter_index] not in ['li_fe']:\n",
    "                    # Null abundances (except Li), if Teff > 6000K\n",
    "                    input_model_parameters[parameter_index] = 0.0\n",
    "            if input_model_parameters[0] > 5.5:\n",
    "                if input_model_labels[parameter_index] in ['c_fe','n_fe','o_fe']:\n",
    "                    # Null CNO abundances, if Teff > 5500K\n",
    "                    input_model_parameters[parameter_index] = 0.0\n",
    "            if input_model_labels[parameter_index] in ['li_fe']:\n",
    "                # print('[Li/Fe]',input_model_parameters[parameter_index])\n",
    "                # print('A(Li)',input_model_parameters[parameter_index] + input_model_parameters[2] + 1.05)\n",
    "                if input_model_parameters[parameter_index] + input_model_parameters[2] + 1.05 < 0.0:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value of A(Li) < 0 detected, setting back to 1.05')\n",
    "                    input_model_parameters[parameter_index] = - input_model_parameters[2]\n",
    "                if input_model_parameters[parameter_index] + input_model_parameters[2] + 1.05 > 4.0:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value of A(Li) > 4 detected, setting back to 3.26')\n",
    "                    input_model_parameters[parameter_index] = (3.26 - 1.05) - input_model_parameters[2] \n",
    "            elif input_model_labels[parameter_index] in ['n_fe','o_fe','y_fe','ba_fe','la_fe','ce_fe','nd_fe']:\n",
    "                if input_model_parameters[parameter_index] < -0.5:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value for '+input_model_labels[parameter_index]+' < -0.5 detected, setting back to -0.5')\n",
    "                    input_model_parameters[parameter_index] = -0.5\n",
    "                if input_model_parameters[parameter_index] > 1.0:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value for '+input_model_labels[parameter_index]+' > 1.0 detected, setting back to +1.0')\n",
    "                    input_model_parameters[parameter_index] = 1.0\n",
    "            # Note: we have explicitely shifted c_fe in the latter catergory to avoid issues with too strong detections of c_fe\n",
    "            else:\n",
    "                if input_model_parameters[parameter_index] < -0.5:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value for '+input_model_labels[parameter_index]+' < -0.5 detected, setting back to -0.5')\n",
    "                    input_model_parameters[parameter_index] = -0.5\n",
    "                if input_model_parameters[parameter_index] > 0.5:\n",
    "                    if sys.argv[1] == '-f':\n",
    "                        print('Extreme value for '+input_model_labels[parameter_index]+' > 0.5 detected, setting back to +0.5')\n",
    "                    input_model_parameters[parameter_index] = 0.5\n",
    "    \n",
    "    if sys.argv[1] == '-f':\n",
    "        print()\n",
    "        \n",
    "    def get_flux_only(input_wave,*model_parameters):\n",
    "        \"\"\"\n",
    "        This will be used as interpolation routine to give back a synthetic flux based on the curve_fit parameters\n",
    "        \"\"\"\n",
    "        (wave,data,data_sigma2,model_flux) = match_observation_and_model(model_parameters, input_model_labels, spectrum, input_model, True, False)\n",
    "\n",
    "        return(model_flux[input_unmasked])\n",
    "    \n",
    "    bounds=[]\n",
    "    for label in input_model_labels:\n",
    "        if label == 'teff':\n",
    "            bounds.append((3.0,8.0))\n",
    "        elif label == 'logg':\n",
    "            bounds.append((-0.5,5.5))\n",
    "        elif label == 'fe_h':\n",
    "            bounds.append((-4.0,1.0))\n",
    "        elif label == 'vmic':\n",
    "            bounds.append((0.25,4.0))\n",
    "        elif label == 'vsini':\n",
    "            bounds.append((1.0,40.0))\n",
    "        elif label == 'li_fe':\n",
    "            bounds.append((-0.5 - input_model_parameters[2] - 1.05, 4.5 - input_model_parameters[2] - 1.05))\n",
    "        elif label in ['c_fe','n_fe','o_fe','y_fe','ba_fe','la_fe','ce_fe','nd_fe']:\n",
    "            bounds.append((-1.5,2.0))\n",
    "        elif label in ['al_fe']:\n",
    "            if input_model_parameters[2] < -0.5:\n",
    "                print('[Fe/H] < -0.75. Allowing [Al/Fe] to go up to 1.5')\n",
    "                bounds.append((-1.0,1.5))\n",
    "            else:\n",
    "                bounds.append((-1.0,1.0))\n",
    "        elif label in ['na_fe']:\n",
    "            if input_model_parameters[2] < -0.5:\n",
    "                print('[Fe/H] < -0.75. Allowing [Na/Fe] to go up to 1.25')\n",
    "                bounds.append((-1.0,1.25))\n",
    "            else:\n",
    "                bounds.append((-1.0,1.0))\n",
    "        else:\n",
    "            bounds.append((-1.0,1.0))\n",
    "    bounds = np.array(bounds)\n",
    "    \n",
    "    # The MASTERPIECE: CURVE FIT\n",
    "    output_model_parameters, output_model_covariances = curve_fit(get_flux_only,input_wave[input_unmasked],\n",
    "        input_data[input_unmasked],\n",
    "        p0 = input_model_parameters,\n",
    "        sigma=np.sqrt(input_sigma[input_unmasked]),\n",
    "        absolute_sigma=True,\n",
    "        bounds = tuple([bounds[:,0],bounds[:,1]]),\n",
    "        maxfev=10000,\n",
    "        xtol=1e-4\n",
    "    )\n",
    "    \n",
    "    (output_wave,output_data,output_sigma,output_flux) = match_observation_and_model(output_model_parameters, input_model_labels, spectrum, input_model, True, False)\n",
    "    \n",
    "    # Test what should be the next model for a new round of label optimisation\n",
    "    new_output_model, new_closest_model, new_output_model_name, new_output_model_labels = find_best_available_neutral_network_model(\n",
    "        1000*output_model_parameters[input_model_labels == 'teff'][0],\n",
    "        spectrum['init_logg'],\n",
    "        output_model_parameters[input_model_labels == 'fe_h'][0]\n",
    "    )\n",
    "    \n",
    "    # Test if the output model parameters teff/logg/fe_h are within the input model\n",
    "    grid_teff = float(input_model_name[:4])\n",
    "    grid_logg = float(input_model_name[5:9])\n",
    "    grid_fe_h = float(input_model_name[10:])\n",
    "    if grid_teff <= 4000:\n",
    "        teff_low = (1000*output_model_parameters[input_model_labels == 'teff'][0] - grid_teff) > -100#/2. * 1.2\n",
    "    else:\n",
    "        teff_low = (1000*output_model_parameters[input_model_labels == 'teff'][0] - grid_teff) > -250#/2. * 1.2\n",
    "    if grid_teff <= 3900:\n",
    "        teff_high = (1000*output_model_parameters[input_model_labels == 'teff'][0] - grid_teff) < 100#/2. * 1.2\n",
    "    else:\n",
    "        teff_high = (1000*output_model_parameters[input_model_labels == 'teff'][0] - grid_teff) < 250#/2. * 1.2\n",
    "        \n",
    "    logg_low = (spectrum['init_logg'] - grid_logg) > -0.5#/2. * 1.1\n",
    "    logg_high = (spectrum['init_logg'] - grid_logg) < 0.5#/2. * 1.1\n",
    "    \n",
    "    if grid_fe_h <= -1.00:\n",
    "        fe_h_low = (output_model_parameters[input_model_labels == 'fe_h'][0] - grid_fe_h) > -0.5#/2. * 1.2\n",
    "    else:\n",
    "        fe_h_low = (output_model_parameters[input_model_labels == 'fe_h'][0] - grid_fe_h) > -0.25#/2. * 1.2\n",
    "    if grid_fe_h <= -1.5:\n",
    "        fe_h_high = (output_model_parameters[input_model_labels == 'fe_h'][0] - grid_fe_h) < 0.5#/2. * 1.2\n",
    "    else:\n",
    "        fe_h_high = (output_model_parameters[input_model_labels == 'fe_h'][0] - grid_fe_h) < 0.25#/2. * 1.2\n",
    "\n",
    "    # If yes: we converged on a model within 10% of grid edges or it's the same as before\n",
    "    if (\n",
    "        (\n",
    "            np.all([teff_low,teff_high,logg_low,logg_high,fe_h_low,fe_h_high]) |\n",
    "            (input_model_name == new_output_model_name) \n",
    "        ) & \n",
    "        (iteration != 0)\n",
    "    ):\n",
    "        converged = True\n",
    "    else:\n",
    "        converged = False\n",
    "    \n",
    "    # Decide what parameters to return\n",
    "    # Casse 1) return input model and its optimised parameters\n",
    "    # 1a) not first iteration, but same model or \n",
    "    # 1b) final iteration\n",
    "    if converged | (iteration == maximum_loops - 1):\n",
    "        \n",
    "        # If the a new iteration would use the same model or we are in the last iteration:\n",
    "\n",
    "        # output_flux -> stays the same\n",
    "        # output_model_parameters -> stays the same\n",
    "        # output_model_covariances -> stays the same\n",
    "        output_model = input_model\n",
    "        output_model_name = input_model_name\n",
    "        output_model_labels = input_model_labels\n",
    "        # output_wave -> stays the same\n",
    "        # output_data -> stays the same \n",
    "        # output_sigma -> stays the same\n",
    "\n",
    "    # Case 2) return next_iteration model and its parameter array adjusted to it\n",
    "    # for iteration 0 or where not converged, but also not yet final iteration\n",
    "    else:\n",
    "        # This is neither the final iteration, nor did the model stay the same\n",
    "        \n",
    "        # output_flux -> stays the same\n",
    "        # output_model_covariances -> stays the same\n",
    "        # output_wave -> stays the same\n",
    "        # output_data -> stays the same \n",
    "        # output_sigma -> stays the same\n",
    "\n",
    "        # Test if output and input model have same labels\n",
    "        same_model_labels = True\n",
    "        if len(input_model_labels) == len(new_output_model_labels):\n",
    "            for label_index, label in enumerate(input_model_labels):\n",
    "                if new_output_model_labels[label_index] != label:\n",
    "                    same_model_labels = False\n",
    "        else:\n",
    "            same_model_labels = False\n",
    "        if same_model_labels:\n",
    "            if sys.argv[1] == '-f':\n",
    "                print('Model_labels are the same! Continuing with same model_parameters')\n",
    "        else:\n",
    "            if sys.argv[1] == '-f':\n",
    "                print('Model_labels changed! Updating model_parameters')\n",
    "            new_model_parameters = []\n",
    "            # Match old labels if possible, otherwise add [X/Fe] = 0\n",
    "            for label in new_output_model_labels:\n",
    "                if label in input_model_labels:\n",
    "                    new_model_parameters.append(output_model_parameters[input_model_labels==label][0])\n",
    "                else:\n",
    "                    new_model_parameters.append(0) # If label not available for any [X/Fe], set it to 0\n",
    "\n",
    "            output_model_parameters = np.array(new_model_parameters)\n",
    "            \n",
    "        # update the next iteration model\n",
    "        output_model = new_output_model\n",
    "        output_model_name = new_output_model_name\n",
    "        output_model_labels = new_output_model_labels\n",
    "    \n",
    "    return(converged, output_flux, output_model_parameters, output_model_covariances, output_model, output_model_name, output_model_labels, output_wave, output_data, output_sigma, new_closest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We loop up to maximum_loops times over the major iteration step\n",
    "while (spectrum['opt_loop'] < maximum_loops) & (converged == False):\n",
    "    \n",
    "    if sys.argv[1] == '-f':\n",
    "        print('\\n *** STARTING MAJOR LOOP '+str(spectrum['opt_loop'])+' *** \\n')\n",
    "    \n",
    "    if spectrum['opt_loop'] == 0:\n",
    "        mass, age, bc_ks, lbol, logg = iterate_logg_mass_age_bc_ks_lbol(1000.*spectrum['init_teff'], spectrum['init_logg'], spectrum['init_fe_h'])\n",
    "    else:\n",
    "        mass, age, bc_ks, lbol, logg = iterate_logg_mass_age_bc_ks_lbol(1000*model_parameters_opt[model_labels_opt == 'teff'][0], spectrum['init_logg'], model_parameters_opt[model_labels_opt == 'fe_h'][0])\n",
    "    \n",
    "    if sys.argv[1] == '-f':\n",
    "        print('mass     ,',\"{:.2f}\".format(mass))\n",
    "        print('age      ,',\"{:.2f}\".format(age))\n",
    "        print('bc_ks    ,',\"{:.2f}\".format(bc_ks)+'\\n')\n",
    "\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Updated logg from '+\"{:.2f}\".format(spectrum['init_logg'])+' to '+\"{:.2f}\".format(logg)+'\\n')\n",
    "\n",
    "    spectrum['init_logg'] = logg\n",
    "    \n",
    "    # Major loop 0:\n",
    "    if spectrum['opt_loop'] == 0:\n",
    "\n",
    "        # Find best model for given initial Teff/logg/[Fe/H]\n",
    "        neural_network_model_opt, closest_model, model_name_opt, model_labels_opt = find_best_available_neutral_network_model(\n",
    "            1000*spectrum['init_teff'],\n",
    "            spectrum['init_logg'],\n",
    "            spectrum['init_fe_h']\n",
    "        )\n",
    "        \n",
    "        # Feed initial values into array\n",
    "        model_parameters_opt = [spectrum['init_'+label] for label in model_labels_opt]\n",
    "        \n",
    "        # Create model flux for finding best mask for this optimisation loop\n",
    "        (wave_opt,data_opt,sigma2_opt,model_flux_opt) = match_observation_and_model(model_parameters_opt, model_labels_opt, spectrum, neural_network_model_opt, True, False)\n",
    "        unmasked_opt = (\n",
    "            (\n",
    "                # Not too large difference between obs and synthesis\n",
    "                (~((np.abs(data_opt-model_flux_opt)/np.sqrt(sigma2_opt) > 5) & (np.abs(data_opt-model_flux_opt) > 0.4))) & \n",
    "                # Not in unreliable synthesis region\n",
    "                (~np.any(np.array([((wave_opt >= mask_beginning) & (wave_opt <= mask_end)) for (mask_beginning, mask_end) in zip(masks['mask_begin'],masks['mask_end'])]),axis=0))\n",
    "            ) |\n",
    "            # or is in vital line wavelengths (unless they are > 1 away!)\n",
    "            (\n",
    "                (np.abs(data_opt-model_flux_opt) < 1.0) &\n",
    "                np.any(np.array([((wave_opt >= line_beginning) & (wave_opt <= line_end)) for (line_beginning, line_end) in zip(vital_lines['line_begin'],vital_lines['line_end'])]),axis=0)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Initial Nr. Wavelength Points: '+str(len(np.where(unmasked_opt==True)[0]))+' ('+str(int(np.round(100*len(np.where(unmasked_opt==True)[0])/len(unmasked_opt))))+'%)')\n",
    "    # For Major loops > 0: We already have a model flux to use for the RV optimisation\n",
    "    \n",
    "    if spectrum['fit_global_rv'] == True:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Fitting global RV')\n",
    "        # Optimise RV based on initial or previous RV\n",
    "        try:\n",
    "            spectrum['init_vrad'],spectrum['init_e_vrad'] = adjust_rv(spectrum['init_vrad'], wave_opt, data_opt, sigma2_opt, model_flux_opt,small_rv_window = np.max([20.,2*spectrum['init_vsini']]))\n",
    "        except:\n",
    "            spectrum['init_vrad'],spectrum['init_e_vrad'] = adjust_rv(spectrum['init_vrad'], wave_opt, data_opt, sigma2_opt, model_flux_opt,small_rv_window = 200.)\n",
    "    else:\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Keeping global RV fixed at ',spectrum['init_vrad'])\n",
    "        \n",
    "    # Find new mask based on optimised RV\n",
    "    (wave_opt,data_opt,sigma2_opt,model_flux_opt) = match_observation_and_model(model_parameters_opt, model_labels_opt, spectrum, neural_network_model_opt, True, False)\n",
    "    unmasked_opt = (\n",
    "            (\n",
    "                # Not too large difference between obs and synthesis\n",
    "                (~((np.abs(data_opt-model_flux_opt)/np.sqrt(sigma2_opt) > 5) & (np.abs(data_opt-model_flux_opt) > 0.3))) & \n",
    "                # Not in unreliable synthesis region\n",
    "                (~np.any(np.array([((wave_opt >= mask_beginning) & (wave_opt <= mask_end)) for (mask_beginning, mask_end) in zip(masks['mask_begin'],masks['mask_end'])]),axis=0))\n",
    "            ) |\n",
    "            # or is in vital line wavelengths (unless they are > 1 away!)\n",
    "            (\n",
    "                (np.abs(data_opt-model_flux_opt) < 1.0) &\n",
    "                np.any(np.array([((wave_opt >= line_beginning) & (wave_opt <= line_end)) for (line_beginning, line_end) in zip(vital_lines['line_begin'],vital_lines['line_end'])]),axis=0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Loop '+str(spectrum['opt_loop'])+' Nr. Wavelength Points: '+str(len(np.where(unmasked_opt==True)[0]))+' ('+str(int(np.round(100*len(np.where(unmasked_opt==True)[0])/len(unmasked_opt))))+'%) \\n')\n",
    "\n",
    "    # Call optimisation routine\n",
    "    converged, model_flux_opt, model_parameters_opt, model_covariances_opt, neural_network_model_opt, model_name_opt, model_labels_opt, wave_opt, data_opt, sigma2_opt, closest_model = optimise_labels(model_parameters_opt, neural_network_model_opt, model_name_opt, model_labels_opt, wave_opt, data_opt, sigma2_opt, unmasked_opt, spectrum['opt_loop'])\n",
    "\n",
    "    if (converged != True) & (spectrum['opt_loop'] < maximum_loops - 1):\n",
    "        if sys.argv[1] == '-f':\n",
    "            print('Not converged at the end of loop '+str(spectrum['opt_loop'])+'. Will start another loop \\n')\n",
    "    elif (converged == True):\n",
    "        print('Converged at the end of loop '+str(spectrum['opt_loop'])+'. \\n')\n",
    "    else:\n",
    "        print('Not converged at the end of final loop '+str(spectrum['opt_loop'])+'! \\n')\n",
    "        success = False\n",
    "        \n",
    "    mass, age, bc_ks, lbol, logg = iterate_logg_mass_age_bc_ks_lbol(1000*model_parameters_opt[model_labels_opt == 'teff'][0], spectrum['init_logg'], model_parameters_opt[model_labels_opt == 'fe_h'][0])\n",
    "    if sys.argv[1] == '-f':\n",
    "        print('Updated logg from '+\"{:.2f}\".format(spectrum['init_logg'])+' to '+\"{:.2f}\".format(logg))\n",
    "    spectrum['init_logg'] = logg\n",
    "    \n",
    "    print('Mass ',\"{:.2f}\".format(mass),'Age ',\"{:.2f}\".format(age),'BC(Ks) ',\"{:.2f}\".format(bc_ks),'Lbol ',\"{:.2f}\".format(lbol),'log(Lbol)',\"{:.2f}\".format(np.log10(lbol)))\n",
    "\n",
    "    print(\n",
    "        'Teff='+str(int(1000*model_parameters_opt[model_labels_opt == 'teff'][0]))+'K, '+ \\\n",
    "        'logg='+str(np.round(spectrum['init_logg'],decimals=2))+', '+ \\\n",
    "        '[Fe/H]='+str(np.round(model_parameters_opt[model_labels_opt == 'fe_h'][0],decimals=2))+', '+ \\\n",
    "        'vmic='+str(np.round(model_parameters_opt[model_labels_opt == 'vmic'][0],decimals=2))+'km/s, '+ \\\n",
    "        'vsini='+str(np.round(model_parameters_opt[model_labels_opt == 'vsini'][0],decimals=1))+'km/s'\n",
    "    )\n",
    "\n",
    "    spectrum['opt_loop'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end: plot full spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if success:\n",
    "    info_line_1 = str(spectrum['sobject_id'])+': successful in '+str(spectrum['opt_loop'])+' loops, Model '+model_name_opt\n",
    "else:\n",
    "    info_line_1 = str(spectrum['sobject_id'])+': not successful, Model '+model_name_opt\n",
    "\n",
    "if (spectrum['flag_sp'] & flag_sp_closest_3x3x3_model_not_available) > 0:\n",
    "    if (spectrum['flag_sp'] & flag_sp_closest_extra6_model_not_available) > 0:\n",
    "        info_line_1 = info_line_1+' (extrapol. 27+7)'\n",
    "    else:\n",
    "        info_line_1 = info_line_1+' (extrapol. 27)'\n",
    "elif (spectrum['flag_sp'] & flag_sp_closest_extra6_model_not_available) > 0:\n",
    "    info_line_1 = info_line_1+' (extrapol. 7)'\n",
    "\n",
    "\n",
    "if success:\n",
    "    info_line_2 = 'Teff='+str(int(1000*model_parameters_opt[model_labels_opt == 'teff'][0]))+'K, '+ \\\n",
    "        'logg='+str(np.round(spectrum['init_logg'],decimals=2))+', '+ \\\n",
    "        '[Fe/H]='+str(np.round(model_parameters_opt[model_labels_opt == 'fe_h'][0],decimals=2))+', '+ \\\n",
    "        'vmic='+str(np.round(model_parameters_opt[model_labels_opt == 'vmic'][0],decimals=2))+'km/s, '+ \\\n",
    "        'vsini='+str(np.round(model_parameters_opt[model_labels_opt == 'vsini'][0],decimals=1))+'km/s'\n",
    "else:\n",
    "    info_line_2 = 'Teff='+str(int(1000*spectrum['init_teff']))+'K, '+ \\\n",
    "        'logg='+str(np.round(spectrum['init_logg'],decimals=2))+', '+ \\\n",
    "        '[Fe/H]='+str(np.round(spectrum['init_fe_h'],decimals=2))+', '+ \\\n",
    "        'vmic='+str(np.round(spectrum['init_vmic'],decimals=2))+'km/s, '+ \\\n",
    "        'vsini='+str(np.round(spectrum['init_vsini'],decimals=1))+'km/s'\n",
    "\n",
    "if spectrum['fit_global_rv'] == True:\n",
    "    info_line_3 = 'RV Global Fit: '+str(np.round(spectrum['init_vrad'],decimals=2))+'±'+str(np.round(spectrum['init_e_vrad'],decimals=2))\n",
    "else:\n",
    "    info_line_3 = 'RV Global Fix'\n",
    "\n",
    "info_line_3 = info_line_3+', Ind. RV (single) = '+\"{:.2f}\".format(rv_mean)+r' ± '+\"{:.2f}\".format(rv_sigma)+' km/s'\n",
    "\n",
    "info_line_3 = info_line_3+', Gaia DR3 = '\n",
    "if np.isfinite(init_values_table['vrad_gaia'][sobject_id_initial_index]):\n",
    "    info_line_3 = info_line_3+\"{:.2f}\".format(init_values_table['vrad_gaia'][sobject_id_initial_index])+', '\n",
    "else:\n",
    "    info_line_3 = info_line_3+'NaN'\n",
    "    \n",
    "fig = plot_spectrum(\n",
    "    wave_opt,\n",
    "    [\n",
    "        data_opt,\n",
    "        model_flux_opt\n",
    "    ],\n",
    "    np.sqrt(sigma2_opt),\n",
    "    ~unmasked_opt,\n",
    "    info_line_1,\n",
    "    info_line_2,\n",
    "    info_line_3\n",
    ")\n",
    "\n",
    "file_directory = galah_dr4_directory+'analysis_products/'+str(spectrum['sobject_id'])[:6]+'/'+str(spectrum['sobject_id'])+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig.savefig(file_directory+str(spectrum['sobject_id'])+'_plxcom_fit_comparison.pdf',overwrite=True,bbox_inches='tight')\n",
    "\n",
    "# show plot if working interactively\n",
    "if sys.argv[1] == '-f':\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save spectrum\n",
    "save_spectrum = Table()\n",
    "save_spectrum['wave'] = wave_opt\n",
    "save_spectrum['sob'] = data_opt\n",
    "save_spectrum['uob'] = np.sqrt(sigma2_opt)\n",
    "save_spectrum['smod'] = model_flux_opt\n",
    "save_spectrum['mob'] = unmasked_opt\n",
    "\n",
    "file_directory = galah_dr4_directory+'analysis_products/'+str(spectrum['sobject_id'])[:6]+'/'+str(spectrum['sobject_id'])+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_spectrum.write(file_directory+str(spectrum['sobject_id'])+'_plxcom_fit_spectrum.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save covariances\n",
    "np.savez(\n",
    "    file_directory+str(spectrum['sobject_id'])+'_plxcom_fit_covariances.npz',\n",
    "    model_labels = model_labels_opt,\n",
    "    model_name = model_name_opt,\n",
    "    model_parameters = model_parameters_opt,\n",
    "    model_covariances = model_covariances_opt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(file_directory+str(spectrum['sobject_id'])+'_plxcom_sobject_ids.txt',spectrum['sobject_ids'],fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup our output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     53
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = Table()\n",
    "file_directory = galah_dr4_directory+'analysis_products/'+str(spectrum['sobject_id'])[:6]+'/'+str(spectrum['sobject_id'])+'/'\n",
    "Path(file_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "col = Table.Column(\n",
    "    name='sobject_id',\n",
    "    data = [spectrum['sobject_id']],\n",
    "    description='GALAH unique identifier; spectrum stacked via tmass_id',\n",
    "    unit='')\n",
    "output.add_column(col)\n",
    "col = Table.Column(\n",
    "    name='tmass_id',\n",
    "    data = [spectrum['tmass_id']],\n",
    "    description='2MASS ID (same for stacked spectra)',\n",
    "    unit='')\n",
    "output.add_column(col)\n",
    "col = Table.Column(\n",
    "    name='fit_global_rv',\n",
    "    data = [spectrum['fit_global_rv']],\n",
    "    description='Global RV fixed or fitted',\n",
    "    unit='')\n",
    "output.add_column(col)\n",
    "\n",
    "for label in ['gaia_edr3_source_id']:\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = [np.int64(spectrum[label])],\n",
    "        description=description[label],\n",
    "        unit=units[label])\n",
    "    output.add_column(col)\n",
    "\n",
    "col = Table.Column(\n",
    "    name='flag_sp_fit',\n",
    "    data = [np.int32(spectrum['flag_sp'])],\n",
    "    description=description['flag_sp'],\n",
    "    unit='')\n",
    "output.add_column(col)\n",
    "\n",
    "if spectrum['fit_global_rv']:\n",
    "    rv_value = np.float32(spectrum['init_vrad'])\n",
    "    e_rv_value = np.float32(spectrum['init_e_vrad'])\n",
    "else:\n",
    "    rv_value = rv_mean\n",
    "    e_rv_value = rv_sigma\n",
    "\n",
    "col = Table.Column(\n",
    "    name='rv_gauss',\n",
    "    data = [rv_value],\n",
    "    description=description['rv_gauss'],\n",
    "    unit=units['rv_gauss'])\n",
    "output.add_column(col)\n",
    "col = Table.Column(\n",
    "    name='e_rv_gauss',\n",
    "    data = [e_rv_value],\n",
    "    description='Fitting uncertainty for '+description['rv_gauss'],\n",
    "    unit=units['rv_gauss'])\n",
    "output.add_column(col)\n",
    "\n",
    "diagonal_covariance_entries_sqrt = np.sqrt(np.diag(model_covariances_opt))\n",
    "\n",
    "# These are the labels that our interpolation routine was trained on\n",
    "model_interpolation_labels = np.array(['teff', 'logg', 'fe_h', 'vmic', 'vsini', 'li_fe', 'c_fe', 'n_fe', 'o_fe', 'na_fe', 'mg_fe', 'al_fe', 'si_fe', 'k_fe', 'ca_fe', 'sc_fe', 'ti_fe', 'v_fe', 'cr_fe', 'mn_fe', 'co_fe', 'ni_fe', 'cu_fe', 'zn_fe', 'rb_fe', 'sr_fe', 'y_fe', 'zr_fe', 'mo_fe', 'ru_fe', 'ba_fe', 'la_fe', 'ce_fe', 'nd_fe', 'sm_fe', 'eu_fe'])\n",
    "\n",
    "# flag_x_fe_values:\n",
    "flag_x_fe_value_no_detection = 1\n",
    "flag_x_fe_value_not_measured = 2\n",
    "flag_x_fe_value_no_success = 4\n",
    "\n",
    "# Let's loop through all the parameters that are part of the spectrum_interpolation routine\n",
    "for label in model_interpolation_labels:\n",
    "    \n",
    "    # For each of the labels, we start with an unflagged value of 0\n",
    "    flag_x_fe = 0\n",
    "\n",
    "    if label == 'logg':\n",
    "        col = Table.Column(\n",
    "            name=label,\n",
    "            data = [spectrum['init_logg']],\n",
    "            description=description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "        col = Table.Column(\n",
    "            name='cov_e_'+label,\n",
    "            data = [np.float32(np.NaN)],\n",
    "            description='Diagonal Covariance Error (raw) for '+description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "        col = Table.Column(\n",
    "            name='flag_'+label,\n",
    "            data = [np.int32(flag_x_fe)],\n",
    "            description='Quality flag for '+description[label],\n",
    "            unit='')\n",
    "        output.add_column(col)    \n",
    "\n",
    "    # We know that some labels do not influence the spectrum shape at all\n",
    "    elif label not in model_labels_opt:\n",
    "\n",
    "        # If that is the case, we do not measure them, and raise the flag_x_fe\n",
    "        flag_x_fe += flag_x_fe_value_not_measured\n",
    "\n",
    "        # To have the same output format, fill the needed columns with NaN / flag_x_fe\n",
    "        col = Table.Column(\n",
    "            name=label,\n",
    "            data = [np.float32(np.NaN)],\n",
    "            description=description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "        col = Table.Column(\n",
    "            name='cov_e_'+label,\n",
    "            data = [np.float32(np.NaN)],\n",
    "            description='Diagonal Covariance Error (raw) for '+description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "        col = Table.Column(\n",
    "            name='flag_'+label,\n",
    "            data = [np.int32(flag_x_fe)],\n",
    "            description='Quality flag for '+description[label],\n",
    "            unit='')\n",
    "        output.add_column(col)\n",
    "\n",
    "    # Let's check out the labels that have been fitted\n",
    "    else:\n",
    "        label_index = np.where(label == model_labels_opt)[0][0]\n",
    "\n",
    "        label_value = model_parameters_opt[label_index]\n",
    "        if label == 'teff':\n",
    "            label_value *= 1000\n",
    "\n",
    "        col = Table.Column(\n",
    "            name=label,\n",
    "            data = [np.float32(label_value)],\n",
    "            description=description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "    \n",
    "        label_value = diagonal_covariance_entries_sqrt[label_index]\n",
    "        if label == 'teff':\n",
    "            label_value *= 1000\n",
    "\n",
    "        col = Table.Column(\n",
    "            name='cov_e_'+label,\n",
    "            data = [np.float32(label_value)],\n",
    "            description='Diagonal Covariance Error (raw) for '+description[label],\n",
    "            unit=units[label])\n",
    "        output.add_column(col)\n",
    "\n",
    "        # For [Fe/H] and [X/Fe], we do an additional test, if the lines are actually sufficiently detected\n",
    "        if ((label == 'fe_h') | (label[-3:] == '_fe')):\n",
    "\n",
    "            # Replace the particular value for [X/Fe] or [Fe/H] with the lowest value of the training routine\n",
    "            model_parameters_low_xfe = np.array(model_parameters_opt)\n",
    "            model_parameters_low_xfe[label_index] = (neural_network_model_opt[-2])[(label == model_interpolation_labels)][0]\n",
    "\n",
    "            # Create the spectrum with lowest [X/Fe] or [Fe/H]\n",
    "            (wave_low_xfe,data_low_xfe,data_sigma2_low_xfe,model_low_xfe) = match_observation_and_model(\n",
    "                model_parameters_low_xfe, \n",
    "                model_labels_opt, \n",
    "                spectrum,\n",
    "                neural_network_model_opt,\n",
    "                True, \n",
    "                False\n",
    "            )\n",
    "\n",
    "            # Let's calculate the absolute difference between the spectra\n",
    "            absolute_difference = np.abs(model_flux_opt[unmasked_opt].clip(min=0.0,max=1.0)-model_low_xfe[unmasked_opt].clip(min=0.0,max=1.0))\n",
    "\n",
    "            # Let's compare the absolute difference to the measurement noise\n",
    "            difference_with_respect_to_noise = absolute_difference / np.sqrt(sigma2_opt[unmasked_opt])\n",
    "\n",
    "            # If the difference is not larger than 3xnoise, we raise a flag that we do not have a detection\n",
    "            if not np.max(difference_with_respect_to_noise) > 3:\n",
    "                flag_x_fe += flag_x_fe_value_no_detection\n",
    "                \n",
    "            if not success:\n",
    "                flag_x_fe += flag_x_fe_value_no_success\n",
    "\n",
    "            col = Table.Column(\n",
    "                name='flag_'+label,\n",
    "                data = [np.int32(flag_x_fe)],\n",
    "                description='Quality flag for '+description[label],\n",
    "                unit='')\n",
    "            output.add_column(col)\n",
    "\n",
    "for data, label in zip([mass,age,bc_ks,extra_info['a_ks'],lbol,extra_info['r_med'],extra_info['r_lo'],extra_info['r_hi']],['mass','age','bc_ks','a_ks','lbol','r_med','r_lo','r_hi']):\n",
    "    col = Table.Column(\n",
    "        name=label,\n",
    "        data = np.float32(data),\n",
    "        description=label,\n",
    "        unit='')\n",
    "    output.add_column(col)\n",
    "\n",
    "# Let's safe the default model that we use to allow to reproduce the spectra\n",
    "col = Table.Column(\n",
    "    name='model_name',\n",
    "    data = [model_name_opt],\n",
    "    description=description['model_name'],\n",
    "    unit=units['model_name'])\n",
    "output.add_column(col)\n",
    "\n",
    "col = Table.Column(\n",
    "    name='closest_model',\n",
    "    data = [closest_model],\n",
    "    description=description['closest_model'],\n",
    "    unit=units['closest_model'])\n",
    "output.add_column(col)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "col = Table.Column(\n",
    "    name='comp_time',\n",
    "    data = [float(end_time)],\n",
    "    description='Computational time used on this sobject_id',\n",
    "    unit='s')\n",
    "output.add_column(col)\n",
    "\n",
    "col = Table.Column(\n",
    "    name='opt_loop',\n",
    "    data = [spectrum['opt_loop']],\n",
    "    description='Nr of optimisation loops',\n",
    "    unit='')\n",
    "output.add_column(col)\n",
    "\n",
    "# And save!\n",
    "output.write(file_directory+str(spectrum['sobject_id'])+'_plxcom_fit_results.fits',overwrite=True)\n",
    "\n",
    "print('Duration: '+str(np.round(end_time,decimals=1))+' for sobject_id '+str(spectrum['sobject_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
