{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic Tools\n",
    "import numpy as np\n",
    "from astropy.table import Table,vstack\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as op\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The Payne, see https://github.com/tingyuansen/The_Payne for more details\n",
    "from The_Payne import training\n",
    "from The_Payne import utils\n",
    "from The_Payne import spectral_model\n",
    "\n",
    "# # That's how we would do it:\n",
    "# # training_labels, training_spectra, validation_labels, validation_spectra = utils.load_training_data()\n",
    "# \"\"\"\n",
    "# Changes that need to be made to training.py in The_Payne if no CUDA is available\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     dtype = torch.cuda.FloatTensor\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# else:\n",
    "#     dtype = torch.FloatTensor\n",
    "#     torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    \n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "        \n",
    "# if torch.cuda.is_available():\n",
    "#     perm = perm.cuda()\n",
    "# \"\"\"\n",
    "# # That's how we do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISATION\n",
    "try:\n",
    "    grid_index = int(sys.argv[1])\n",
    "    print('Using Grid index ',grid_index)\n",
    "    \n",
    "    try:\n",
    "        number_grid_points = int(sys.argv[2])\n",
    "    except:\n",
    "        number_grid_points = 3*3*3\n",
    "    print('Using '+str(number_grid_points)+' grid points')\n",
    "\n",
    "except:\n",
    "    grid_index = 1931\n",
    "    print('Using default grid index ',grid_index)\n",
    "\n",
    "    # 3 dimensional points == 27\n",
    "    number_grid_points = 3*3*3\n",
    "\n",
    "    # middle point + surrounding +- ones == 7\n",
    "    # number_grid_points = 1+2+2+2\n",
    "    \n",
    "    print('Using '+str(number_grid_points)+' grid points')\n",
    "    \n",
    "if number_grid_points not in [7,27]:\n",
    "    raise ValueError('number_grid_points not valid (needs to be 7 or 27)')\n",
    "    \n",
    "grids = Table.read('../../spectrum_grids/galah_dr4_model_trainingset_gridpoints.fits')\n",
    "teff_logg_feh_name = str(int(grids['teff_subgrid'][grid_index]))+'_'+\"{:.2f}\".format(grids['logg_subgrid'][grid_index])+'_'+\"{:.2f}\".format(grids['fe_h_subgrid'][grid_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find upper and lower Teff points\n",
    "teff_middle = int(grids['teff_subgrid'][grid_index])\n",
    "if teff_middle <= 4000:\n",
    "    teff_lower = teff_middle - 100\n",
    "else:\n",
    "    teff_lower = teff_middle - 250\n",
    "if teff_middle <= 3900:\n",
    "    teff_higher = teff_middle + 100\n",
    "else:\n",
    "    teff_higher = teff_middle + 250\n",
    "\n",
    "# Find upper and lower logg points\n",
    "logg_middle = grids['logg_subgrid'][grid_index]\n",
    "logg_lower = logg_middle - 0.5\n",
    "logg_higher = logg_middle + 0.5\n",
    "\n",
    "# Find upper and lower fe_h points\n",
    "fe_h_middle = grids['fe_h_subgrid'][grid_index]\n",
    "if fe_h_middle <= -0.75:\n",
    "    fe_h_lower = fe_h_middle - 0.5\n",
    "else:\n",
    "    fe_h_lower = fe_h_middle - 0.25\n",
    "if fe_h_middle <= -1.5:\n",
    "    fe_h_higher = fe_h_middle + 0.5\n",
    "else:\n",
    "    fe_h_higher = fe_h_middle + 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_3x3x3_indices(grid_index):\n",
    "    \n",
    "    grid_indices_3x3x3 = []\n",
    "\n",
    "    for teff in [teff_middle,teff_lower,teff_higher]:\n",
    "        for logg in [logg_middle,logg_lower,logg_higher]:\n",
    "            for fe_h in [fe_h_middle,fe_h_lower,fe_h_higher]:\n",
    "                grid_indices_3x3x3.append(str(int(teff))+'_'+\"{:.2f}\".format(logg)+'_'+\"{:.2f}\".format(fe_h))\n",
    "                \n",
    "    return(grid_indices_3x3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1_2_2_2_indices(grid_index):\n",
    "    \n",
    "    grid_indices_1_2_2_2 = []\n",
    "\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_middle))+'_'+\"{:.2f}\".format(logg_middle)+'_'+\"{:.2f}\".format(fe_h_middle))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_middle))+'_'+\"{:.2f}\".format(logg_middle)+'_'+\"{:.2f}\".format(fe_h_lower))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_middle))+'_'+\"{:.2f}\".format(logg_middle)+'_'+\"{:.2f}\".format(fe_h_higher))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_middle))+'_'+\"{:.2f}\".format(logg_lower)+'_'+\"{:.2f}\".format(fe_h_middle))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_middle))+'_'+\"{:.2f}\".format(logg_higher)+'_'+\"{:.2f}\".format(fe_h_middle))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_lower))+'_'+\"{:.2f}\".format(logg_middle)+'_'+\"{:.2f}\".format(fe_h_middle))\n",
    "    grid_indices_1_2_2_2.append(str(int(teff_higher))+'_'+\"{:.2f}\".format(logg_middle)+'_'+\"{:.2f}\".format(fe_h_middle))\n",
    "\n",
    "    return(grid_indices_1_2_2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the appropriate function to find the subsets\n",
    "if number_grid_points == 27:\n",
    "    subset_names = find_3x3x3_indices(grid_index)\n",
    "elif number_grid_points == 7:\n",
    "    subset_names = find_1_2_2_2_indices(grid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "available = []\n",
    "not_available = []\n",
    "for subset_index, subset_name in enumerate(subset_names):\n",
    "    try:\n",
    "        training_set = Table.read('../training_input/'+subset_name+'/galah_dr4_trainingset_'+subset_name+'_incl_vsini.fits')\n",
    "        print(subset_name)\n",
    "        available.append(subset_name)\n",
    "    except:\n",
    "        print(subset_name+' n/a, using middle grid '+teff_logg_feh_name)\n",
    "        available.append(teff_logg_feh_name)\n",
    "subset_names = available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c943a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_labels = []\n",
    "training_set_flux = []\n",
    "\n",
    "wavelength_file = '../training_input/galah_dr4_3dbin_wavelength_array.pickle'\n",
    "wavelength_file_opener = open(wavelength_file,'rb')\n",
    "wavelength_array = pickle.load(wavelength_file_opener)\n",
    "wavelength_file_opener.close()\n",
    "\n",
    "for subset_index, subset_name in enumerate(subset_names):\n",
    "    \n",
    "    training_labels_subset_index = Table.read('../training_input/'+subset_name+'/galah_dr4_trainingset_'+subset_name+'_incl_vsini.fits')\n",
    "\n",
    "    if subset_index == 0:\n",
    "        labels = tuple(training_labels_subset_index.keys()[2:-1])\n",
    "\n",
    "    else:\n",
    "        # If we go with the 3x3x3 version, we will only use those with teff_lower <= teff <= teff_higher\n",
    "        if number_grid_points == 27:\n",
    "            # If we use the 3x3x3 subsets, but some subset was not available,\n",
    "            # we have to make sure to only include the stars that were not sampled.\n",
    "            if (subset_name == teff_logg_feh_name):\n",
    "\n",
    "                # check teff limits\n",
    "                if subset_index in np.arange(0,9):\n",
    "                    teff_limits = (\n",
    "                        (training_labels_subset_index['teff'] > teff_lower) &\n",
    "                        (training_labels_subset_index['teff'] < teff_higher)\n",
    "                    )\n",
    "                if subset_index in np.arange(9,18):\n",
    "                    teff_limits = (\n",
    "                        (training_labels_subset_index['teff'] > teff_lower) &\n",
    "                        (training_labels_subset_index['teff'] < teff_middle)\n",
    "                    )\n",
    "                if subset_index in np.arange(18,27):\n",
    "                    teff_limits = (\n",
    "                        (training_labels_subset_index['teff'] > teff_middle) &\n",
    "                        (training_labels_subset_index['teff'] < teff_higher)\n",
    "                    )\n",
    "\n",
    "                # check logg limits\n",
    "                if subset_index in [0,1,2,9,10,11,18,19,20]:\n",
    "                    logg_limits = (\n",
    "                        (training_labels_subset_index['logg'] > logg_lower) &\n",
    "                        (training_labels_subset_index['logg'] < logg_higher)\n",
    "                    )\n",
    "                if subset_index in [3,4,5,12,13,14,21,22,23]:\n",
    "                    logg_limits = (\n",
    "                        (training_labels_subset_index['logg'] > logg_lower) &\n",
    "                        (training_labels_subset_index['logg'] < logg_middle)\n",
    "                    )\n",
    "                if subset_index in [6,7,8,15,16,17,24,25,26]:\n",
    "                    logg_limits = (\n",
    "                        (training_labels_subset_index['logg'] > logg_middle) &\n",
    "                        (training_labels_subset_index['logg'] < logg_higher)\n",
    "                    )\n",
    "\n",
    "                # check fe_h limits\n",
    "                if subset_index in np.arange(1,28,3):\n",
    "                    fe_h_limits = (\n",
    "                        (training_labels_subset_index['fe_h'] > fe_h_lower) &\n",
    "                        (training_labels_subset_index['fe_h'] < fe_h_higher)\n",
    "                    )\n",
    "                if subset_index in np.arange(1,28,3):\n",
    "                    fe_h_limits = (\n",
    "                        (training_labels_subset_index['fe_h'] > fe_h_lower) &\n",
    "                        (training_labels_subset_index['fe_h'] < fe_h_middle)\n",
    "                    )\n",
    "                if subset_index in np.arange(2,28,3):\n",
    "                    fe_h_limits = (\n",
    "                        (training_labels_subset_index['fe_h'] > fe_h_middle) &\n",
    "                        (training_labels_subset_index['fe_h'] < fe_h_higher)\n",
    "                    )\n",
    "\n",
    "                within_teff_logg_fe_h_limits = (teff_limits & logg_limits & fe_h_limits)\n",
    "\n",
    "                training_labels_subset_index = training_labels_subset_index[within_teff_logg_fe_h_limits]\n",
    "\n",
    "            else:\n",
    "                within_teff_logg_fe_h_limits = (\n",
    "                    (training_labels_subset_index['teff'] > teff_lower) &\n",
    "                    (training_labels_subset_index['teff'] < teff_higher) &\n",
    "                    (training_labels_subset_index['logg'] > logg_lower) &\n",
    "                    (training_labels_subset_index['logg'] < logg_higher) &\n",
    "                    (training_labels_subset_index['fe_h'] > fe_h_lower) &\n",
    "                    (training_labels_subset_index['fe_h'] < fe_h_higher)\n",
    "                )\n",
    "\n",
    "    training_labels.append(np.array([training_labels_subset_index[label] for label in labels]).T)\n",
    "\n",
    "    flux_ivar_file = '../training_input/'+subset_name+'/galah_dr4_trainingset_'+subset_name+'_incl_vsini_flux_ivar.pickle'\n",
    "    flux_ivar_file_opener = open(flux_ivar_file,'rb')\n",
    "    \n",
    "    if (number_grid_points == 27) & (subset_index != 0):\n",
    "        training_set_flux.append(pickle.load(flux_ivar_file_opener)[within_teff_logg_fe_h_limits])\n",
    "    else:\n",
    "        training_set_flux.append(pickle.load(flux_ivar_file_opener))\n",
    "    \n",
    "    flux_ivar_file_opener.close()\n",
    "\n",
    "training_labels = np.concatenate((training_labels))\n",
    "training_set_flux = np.concatenate((training_set_flux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tuple(training_set.keys()[2:-1])\n",
    "\n",
    "print('Labels to be fitted: ',len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10566b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the appropriate function to find the subsets\n",
    "if number_grid_points == 27:\n",
    "    model_file = 'galah_dr4_neutral_network_3x3x3_'+teff_logg_feh_name+'_'+str(len(labels))+'labels'\n",
    "elif number_grid_points == 7:\n",
    "    model_file = 'galah_dr4_neutral_network_1plus6_'+teff_logg_feh_name+'_'+str(len(labels))+'labels'\n",
    "else:\n",
    "    raise ValueError('number_grid_points not valid')\n",
    "    \n",
    "print('Will create neural network to be stored at ')\n",
    "print('models/'+model_file+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-huntington",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if number_grid_points == 27:\n",
    "    print('Randomly sampling to get 90% training and 10% test sample')\n",
    "    train, test = train_test_split(np.arange(np.shape(training_set_flux)[0]), test_size=0.10, random_state=int(teff_middle)+int(10*logg_middle)+int(100*fe_h_middle))\n",
    "\n",
    "elif number_grid_points == 7:\n",
    "    # We will split the training set into 252 x N spectra to train (90%) and 28 x N spectra to test (10%).\n",
    "    # To have a representative test set, we use the last 14 xN spectra of the narrowly and broadly random samples\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # loop through the size-280 sets\n",
    "    for index in range(int(np.shape(training_labels)[0] / 280)):\n",
    "        # we train the 72 spectra testing the boundaries and the next 104-14 narrowly sampled values\n",
    "        train_indices.append(np.arange(280*index,280*(index+1)-104-14))\n",
    "        # we test with the last 14 entries of the narrowly sampled ones\n",
    "        test_indices.append(np.arange(280*(index+1)-104-14,280*(index+1)-104))\n",
    "        # we train on the next broadly sampled ones up until the last 14 entries\n",
    "        train_indices.append(np.arange(280*(index+1)-104,280*(index+1)-14))\n",
    "        # we test with the last 14 entries of the broadly sampled ones\n",
    "        test_indices.append(np.arange(280*(index+1)-14,280*(index+1)))\n",
    "\n",
    "    train = np.concatenate((train_indices))\n",
    "    test = np.concatenate((test_indices))\n",
    "\n",
    "print('Training with '+str(len(train))+' spectra ('+str(int(100*len(train)/(len(train)+len(test))))+'%)')\n",
    "print('Testing with '+str(len(test))+' spectra ('+str(int(100*len(test)/(len(train)+len(test))))+'%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-exclusive",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot distribution of training set, if interactive\n",
    "if sys.argv[1] == '-f':\n",
    "    plt.show()\n",
    "    \n",
    "    for each in range(36):\n",
    "        f, gs = plt.subplots(1,3,figsize=(10,3))\n",
    "        ax = gs[0]\n",
    "        ax.scatter(\n",
    "            training_labels[train,0],\n",
    "            training_labels[train,each],\n",
    "            s=1\n",
    "        )\n",
    "        ax.set_xlabel(labels[0])\n",
    "        ax.set_xlim(ax.get_xlim()[::-1])\n",
    "        ax.set_ylabel(labels[each])\n",
    "        if each == 1:\n",
    "            ax.set_ylim(ax.get_ylim()[::-1])\n",
    "\n",
    "        ax = gs[1]\n",
    "        ax.scatter(\n",
    "            training_labels[train,1],\n",
    "            training_labels[train,each],\n",
    "            s=1\n",
    "        )\n",
    "        ax.set_xlabel(labels[1])\n",
    "        ax.set_xlim(ax.get_xlim()[::-1])\n",
    "        ax.set_ylabel(labels[each])\n",
    "        if each == 1:\n",
    "            ax.set_ylim(ax.get_ylim()[::-1])\n",
    "\n",
    "        ax = gs[2]\n",
    "        ax.scatter(\n",
    "            training_labels[train,2],\n",
    "            training_labels[train,each],\n",
    "            s=1\n",
    "        )\n",
    "        ax.set_xlabel(labels[2])\n",
    "        ax.set_ylabel(labels[each])\n",
    "        if each == 1:\n",
    "            ax.set_ylim(ax.get_ylim()[::-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.neural_net(\n",
    "    training_labels = training_labels[train,:], \n",
    "    training_spectra = training_set_flux[train,:],\n",
    "    validation_labels = training_labels[test,:], \n",
    "    validation_spectra = training_set_flux[test,:],\n",
    "    num_neurons=300,\n",
    "    learning_rate=1e-4,\n",
    "    num_steps=1e4,\n",
    "    batch_size=128,\n",
    "    num_pixel=np.shape(training_set_flux[0])[0],\n",
    "    training_loss_name = 'loss_functions/'+model_file+'_loss.npz',\n",
    "    payne_model_name = 'models/'+model_file+'.npz'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a776dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load('loss_functions/'+model_file+'_loss.npz') # the output array also stores the training and validation loss\n",
    "training_loss = tmp[\"training_loss\"]\n",
    "validation_loss = tmp[\"validation_loss\"]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(np.arange(training_loss.size)*100, training_loss, 'k', lw=0.5, label = 'Training set')\n",
    "plt.plot(np.arange(training_loss.size)*100, validation_loss, 'r', lw=0.5, label = 'Validation set')\n",
    "plt.legend(loc = 'best', frameon = False, fontsize= 18)\n",
    "plt.yscale('log')\n",
    "#plt.ylim([5,100])\n",
    "plt.xlabel(\"Step\", size=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.savefig('loss_functions/'+model_file+'_loss.png',dpi=200,bbox_inches='tight')\n",
    "if sys.argv[1] == '-f':\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
