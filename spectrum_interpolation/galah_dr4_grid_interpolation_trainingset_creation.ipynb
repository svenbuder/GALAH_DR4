{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# galah_dr4_grid_interpolation_trainingset_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "from scipy.io import readsav\n",
    "from scipy.ndimage.filters import convolve\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all available grids\n",
    "grids = Table.read('../spectrum_grids/galah_dr4_model_trainingset_gridpoints.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one grid_index\n",
    "try:\n",
    "    grid_index = int(sys.argv[1])\n",
    "    print('Using Grid index ',grid_index)\n",
    "except:\n",
    "    print('Interactive mode')\n",
    "    grid_index = 2536\n",
    "    print('Using Grid index ',grid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    teff_logg_feh_name = str(int(grids['teff_subgrid'][grid_index]))+'_'+\"{:.2f}\".format(grids['logg_subgrid'][grid_index])+'_'+\"{:.2f}\".format(grids['fe_h_subgrid'][grid_index])\n",
    "    training_set_vsini0 = Table.read('../spectrum_grids/3d_bin_subgrids/'+teff_logg_feh_name+'/galah_dr4_trainingset_'+teff_logg_feh_name+'.fits')\n",
    "    synthesis_files = '../spectrum_grids/3d_bin_subgrids/'+teff_logg_feh_name\n",
    "    print('Grid index '+str(grid_index)+' corresponds to '+teff_logg_feh_name)\n",
    "except:\n",
    "    raise ValueError('There are only '+str(len(grids))+' entries within the grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we define how to broaden a spectrum with a certain vsini value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def integrate_flux(mu, inten, deltav, vsini, vrt, osamp=1):\n",
    "    \"\"\"\n",
    "    Produces a flux profile by integrating intensity profiles (sampled\n",
    "    at various mu angles) over the visible stellar surface.\n",
    "    Intensity profiles are weighted by the fraction of the projected\n",
    "    stellar surface they represent, apportioning the area between\n",
    "    adjacent MU points equally. Additional weights (such as those\n",
    "    used in a Gauss-Legendre quadrature) can not meaningfully be\n",
    "    used in this scheme.  About twice as many points are required\n",
    "    with this scheme to achieve the precision of Gauss-Legendre\n",
    "    quadrature.\n",
    "    DELTAV, VSINI, and VRT must all be in the same units (e.g. km/s).\n",
    "    If specified, OSAMP should be a positive integer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : array(float) of size (nmu,)\n",
    "        cosine of the angle between the outward normal and\n",
    "        the line of sight for each intensity spectrum in INTEN.\n",
    "    inten : array(float) of size(nmu, npts)\n",
    "        intensity spectra at specified values of MU.\n",
    "    deltav : float\n",
    "        velocity spacing between adjacent spectrum points\n",
    "        in INTEN (same units as VSINI and VRT).\n",
    "    vsini : float\n",
    "        maximum radial velocity, due to solid-body rotation.\n",
    "    vrt : float\n",
    "        radial-tangential macroturbulence parameter, i.e.\n",
    "        np.sqrt(2) times the standard deviation of a Gaussian distribution\n",
    "        of turbulent velocities. The same distribution function describes\n",
    "        the radial motions of one component and the tangential motions of\n",
    "        a second component. Each component covers half the stellar surface.\n",
    "        See 'The Observation and Analysis of Stellar Photospheres', Gray.\n",
    "    osamp : int, optional\n",
    "        internal oversampling factor for convolutions.\n",
    "        By default convolutions are done using the input points (OSAMP=1),\n",
    "        but when OSAMP is set to higher integer values, the input spectra\n",
    "        are first oversampled by cubic spline interpolation.\n",
    "    Returns\n",
    "    -------\n",
    "    value : array(float) of size (npts,)\n",
    "        Disk integrated flux profile.\n",
    "    Note\n",
    "    ------------\n",
    "        If you use this algorithm in work that you publish, please cite\n",
    "        Valenti & Anderson 1996, PASP, currently in preparation.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    -----------\n",
    "    Feb-88  GM\n",
    "        Created ANA version.\n",
    "    13-Oct-92 JAV\n",
    "        Adapted from G. Marcy's ANA routi!= of the same name.\n",
    "    03-Nov-93 JAV\n",
    "        Switched to annular convolution technique.\n",
    "    12-Nov-93 JAV\n",
    "        Fixed bug. Intensity compo!=nts not added when vsini=0.\n",
    "    14-Jun-94 JAV\n",
    "        Reformatted for \"public\" release. Heavily commented.\n",
    "        Pass deltav instead of 2.998d5/deltav. Added osamp\n",
    "        keyword. Added rebinning logic at end of routine.\n",
    "        Changed default osamp from 3 to 1.\n",
    "    20-Feb-95 JAV\n",
    "        Added mu as an argument to handle arbitrary mu sampling\n",
    "        and remove ambiguity in intensity profile ordering.\n",
    "        Interpret VTURB as np.sqrt(2)*sigma instead of just sigma.\n",
    "        Replaced call_external with call to spl_{init|interp}.\n",
    "    03-Apr-95 JAV\n",
    "        Multiply flux by pi to give observed flux.\n",
    "    24-Oct-95 JAV\n",
    "        Force \"nmk\" padding to be at least 3 pixels.\n",
    "    18-Dec-95 JAV\n",
    "        Renamed from dskint() to rtint(). No longer make local\n",
    "        copy of intensities. Use radial-tangential instead\n",
    "        of isotropic Gaussian macroturbulence.\n",
    "    26-Jan-99 JAV\n",
    "        For NMU=1 and VSINI=0, assume resolved solar surface#\n",
    "        apply R-T macro, but supress vsini broadening.\n",
    "    01-Apr-99 GMH\n",
    "        Use annuli weights, rather than assuming ==ual area.\n",
    "    07-Mar-12 JAV\n",
    "        Force vsini and vmac to be scalars.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make local copies of various input variables, which will be altered below.\n",
    "    # Force vsini and especially vmac to be scalars. Otherwise mu dependence fails.\n",
    "\n",
    "    if np.size(vsini) > 1:\n",
    "        vsini = vsini[0]\n",
    "    if np.size(vrt) > 1:\n",
    "        vrt = vrt[0]\n",
    "\n",
    "    # Determine oversampling factor.\n",
    "    os = round(np.clip(osamp, 1, None))  # force integral value > 1\n",
    "\n",
    "    # Convert input MU to projected radii, R, of annuli for a star of unit radius\n",
    "    #  (which is just sine, rather than cosine, of the angle between the outward\n",
    "    #  normal and the line of sight).\n",
    "    rmu = np.sqrt(1 - mu ** 2)  # use simple trig identity\n",
    "\n",
    "    # Sort the projected radii and corresponding intensity spectra into ascending\n",
    "    #  order (i.e. from disk center to the limb), which is equivalent to sorting\n",
    "    #  MU in descending order.\n",
    "    isort = np.argsort(rmu)\n",
    "    rmu = rmu[isort]  # reorder projected radii\n",
    "    nmu = np.size(mu)  # number of radii\n",
    "    if nmu == 1:\n",
    "        if vsini != 0:\n",
    "            logger.warning(\n",
    "                \"Vsini is non-zero, but only one projected radius (mu value) is set. No rotational broadening will be performed.\"\n",
    "            )\n",
    "            vsini = 0  # ignore vsini if only 1 mu\n",
    "\n",
    "    # Calculate projected radii for boundaries of disk integration annuli.  The n+1\n",
    "    # boundaries are selected such that r(i+1) exactly bisects the area between\n",
    "    # rmu(i) and rmu(i+1). The in!=rmost boundary, r(0) is set to 0 (disk center)\n",
    "    # and the outermost boundary, r(nmu) is set to 1 (limb).\n",
    "    if nmu > 1 or vsini != 0:  # really want disk integration\n",
    "        r = np.sqrt(\n",
    "            0.5 * (rmu[:-1] ** 2 + rmu[1:] ** 2)\n",
    "        )  # area midpoints between rmu\n",
    "        r = np.concatenate(([0], r, [1]))\n",
    "\n",
    "        # Calculate integration weights for each disk integration annulus.  The weight\n",
    "        # is just given by the relative area of each annulus, normalized such that\n",
    "        # the sum of all weights is unity.  Weights for limb darkening are included\n",
    "        # explicitly in the intensity profiles, so they aren't needed here.\n",
    "        wt = r[1:] ** 2 - r[:-1] ** 2  # weights = relative areas\n",
    "    else:\n",
    "        wt = np.array([1.0])  # single mu value, full weight\n",
    "\n",
    "    # Generate index vectors for input and oversampled points. Note that the\n",
    "    # oversampled indicies are carefully chosen such that every \"os\" finely\n",
    "    # sampled points fit exactly into one input bin. This makes it simple to\n",
    "    # \"integrate\" the finely sampled points at the end of the routine.\n",
    "    npts = inten.shape[1]  # number of points\n",
    "    xpix = np.arange(npts, dtype=float)  # point indices\n",
    "    nfine = os * npts  # number of oversampled points\n",
    "    xfine = (0.5 / os) * (\n",
    "        2 * np.arange(nfine, dtype=float) - os + 1\n",
    "    )  # oversampled points indices\n",
    "\n",
    "    # Loop through annuli, constructing and convolving with rotation kernels.\n",
    "\n",
    "    yfine = np.empty(nfine)  # init oversampled intensities\n",
    "    flux = np.zeros(nfine)  # init flux vector\n",
    "    for imu in range(nmu):  # loop thru integration annuli\n",
    "\n",
    "        #  Use external cubic spline routine (adapted from Numerical Recipes) to make\n",
    "        #  an oversampled version of the intensity profile for the current annulus.\n",
    "        ypix = inten[isort[imu]]  # extract intensity profile\n",
    "        if os == 1:\n",
    "            # just copy (use) original profile\n",
    "            yfine = ypix\n",
    "        else:\n",
    "            # spline onto fine wavelength scale\n",
    "            yfine = interp1d(xpix, ypix, kind=\"cubic\")(xfine)\n",
    "\n",
    "        # Construct the convolution kernel which describes the distribution of\n",
    "        # rotational velocities present in the current annulus. The distribution has\n",
    "        # been derived analytically for annuli of arbitrary thickness in a rigidly\n",
    "        # rotating star. The kernel is constructed in two pieces: o!= piece for\n",
    "        # radial velocities less than the maximum velocity along the inner edge of\n",
    "        # the annulus, and one piece for velocities greater than this limit.\n",
    "        if vsini > 0:\n",
    "            # nontrivial case\n",
    "            r1 = r[imu]  # inner edge of annulus\n",
    "            r2 = r[imu + 1]  # outer edge of annulus\n",
    "            dv = deltav / os  # oversampled velocity spacing\n",
    "            maxv = vsini * r2  # maximum velocity in annulus\n",
    "            nrk = 2 * int(maxv / dv) + 3  ## oversampled kernel point\n",
    "            # velocity scale for kernel\n",
    "            v = dv * (np.arange(nrk, dtype=float) - ((nrk - 1) / 2))\n",
    "            rkern = np.zeros(nrk)  # init rotational kernel\n",
    "            j1 = np.abs(v) < vsini * r1  # low velocity points\n",
    "            rkern[j1] = np.sqrt((vsini * r2) ** 2 - v[j1] ** 2) - np.sqrt(\n",
    "                (vsini * r1) ** 2 - v[j1] ** 2\n",
    "            )  # generate distribution\n",
    "\n",
    "            j2 = (np.abs(v) >= vsini * r1) & (np.abs(v) <= vsini * r2)\n",
    "            rkern[j2] = np.sqrt(\n",
    "                (vsini * r2) ** 2 - v[j2] ** 2\n",
    "            )  # generate distribution\n",
    "\n",
    "            rkern = rkern / np.sum(rkern)  # normalize kernel\n",
    "\n",
    "            # Convolve the intensity profile with the rotational velocity kernel for this\n",
    "            # annulus. Pad each end of the profile with as many points as are in the\n",
    "            # convolution kernel. This reduces Fourier ringing. The convolution may also\n",
    "            # be do!= with a routi!= called \"externally\" from IDL, which efficiently\n",
    "            # shifts and adds.\n",
    "            if nrk > 3:\n",
    "                yfine = convolve(yfine, rkern, mode=\"nearest\")\n",
    "\n",
    "        # Calculate projected sigma for radial and tangential velocity distributions.\n",
    "        muval = mu[isort[imu]]  # current value of mu\n",
    "        sigma = os * vrt / np.sqrt(2) / deltav  # standard deviation in points\n",
    "        sigr = sigma * muval  # reduce by current mu value\n",
    "        sigt = sigma * np.sqrt(1.0 - muval ** 2)  # reduce by np.sqrt(1-mu**2)\n",
    "\n",
    "        # Figure out how many points to use in macroturbulence kernel.\n",
    "        nmk = int(10 * sigma)\n",
    "        nmk = np.clip(nmk, 3, (nfine - 3) // 2)\n",
    "\n",
    "        # Construct radial macroturbulence kernel with a sigma of mu*VRT/np.sqrt(2).\n",
    "        if sigr > 0:\n",
    "            xarg = np.linspace(-nmk, nmk, 2 * nmk + 1) / sigr\n",
    "            xarg = np.clip(-0.5 * xarg ** 2, -20, None)\n",
    "            mrkern = np.exp(xarg)  # compute the gaussian\n",
    "            mrkern = mrkern / np.sum(mrkern)  # normalize the profile\n",
    "        else:\n",
    "            mrkern = np.zeros(2 * nmk + 1)  # init with 0d0\n",
    "            mrkern[nmk] = 1.0  # delta function\n",
    "\n",
    "        # Construct tangential kernel with a sigma of np.sqrt(1-mu**2)*VRT/np.sqrt(2).\n",
    "        if sigt > 0:\n",
    "            xarg = np.linspace(-nmk, nmk, 2 * nmk + 1) / sigt\n",
    "            xarg = np.clip(-0.5 * xarg ** 2, -20, None)\n",
    "            mtkern = np.exp(xarg)  # compute the gaussian\n",
    "            mtkern = mtkern / np.sum(mtkern)  # normalize the profile\n",
    "        else:\n",
    "            mtkern = np.zeros(2 * nmk + 1)  # init with 0d0\n",
    "            mtkern[nmk] = 1.0  # delta function\n",
    "\n",
    "        # Sum the radial and tangential components, weighted by surface area.\n",
    "        area_r = 0.5  # assume equal areas\n",
    "        area_t = 0.5  # ar+at must equal 1\n",
    "        mkern = area_r * mrkern + area_t * mtkern  # add both components\n",
    "\n",
    "        # Convolve the total flux profiles, again padding the spectrum on both ends to\n",
    "        # protect against Fourier ringing.\n",
    "        yfine = convolve(\n",
    "            yfine, mkern, mode=\"nearest\"\n",
    "        )  # add the padding and convolve\n",
    "\n",
    "        # Add contribution from current annulus to the running total.\n",
    "        flux = flux + wt[imu] * yfine  # add profile to running total\n",
    "\n",
    "    flux = np.reshape(flux, (npts, os))  # convert to an array\n",
    "    flux = np.pi * np.sum(flux, axis=1) / os  # sum, normalize\n",
    "    return flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def broaden_spectrum(wint_seg, sint_seg, wave_seg, cmod_seg, vsini=0, vmac=0, debug=False):\n",
    "\n",
    "    nw = len(wint_seg)\n",
    "    clight = 299792.5\n",
    "    mu = (np.sqrt(0.5*(2*np.arange(7)+1)/np.float(7)))[::-1]\n",
    "    nmu = 7\n",
    "    wmid = 0.5 * (wint_seg[nw-1] + wint_seg[0])\n",
    "    wspan = wint_seg[nw-1] - wint_seg[0]\n",
    "    jmin = np.argmin(wint_seg[1:nw-1] - wint_seg[0:nw-2])\n",
    "    vstep1 = min(wint_seg[1:nw-1] - wint_seg[0:nw-2])\n",
    "    vstep2 = 0.1 * wspan / (nw-1) / wmid * clight\n",
    "    vstep3 = 0.05\n",
    "    vstep = np.max([vstep1,vstep2,vstep3])\n",
    "\n",
    "    # Generate model wavelength scale X, with uniform wavelength step.\n",
    "    nx = int(np.floor(np.log10(wint_seg[nw-1] / wint_seg[0])/ np.log10(1.0+vstep / clight))+1)\n",
    "    if nx % 2 == 0: nx += 1\n",
    "    resol_out = 1.0/((wint_seg[nw-1] / wint_seg[0])**(1.0/(nx-1.0))-1.0)\n",
    "    vstep = clight / resol_out\n",
    "    x_seg = wint_seg[0] * (1.0 + 1.0 / resol_out)**np.arange(nx)\n",
    "\n",
    "    # Interpolate intensity spectra onto new model wavelength scale.  \n",
    "    yi_seg = np.empty((nmu, nx))\n",
    "\n",
    "    for imu in range(nmu):\n",
    "        yi_seg[imu] = np.interp(x_seg, wint_seg, sint_seg[imu])\n",
    "\n",
    "    y_seg = integrate_flux(mu, yi_seg, vstep, np.abs(vsini), np.abs(vmac))\n",
    "\n",
    "    dispersion = vstep1\n",
    "    wave_equi = np.arange(x_seg[0],x_seg[-1]+dispersion,dispersion)\n",
    "\n",
    "    c_seg = np.interp(wave_equi,wave_seg,cmod_seg)\n",
    "    y_seg = np.interp(wave_equi,x_seg,y_seg)\n",
    "\n",
    "    if debug:\n",
    "        print(vstep1,len(wave_equi))\n",
    "\n",
    "    return(wave_equi,y_seg/c_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsini_values = np.array([1.5, 3.0, 6.0, 9.0, 12.0, 18.0]) # km/s\n",
    "if grids['teff_subgrid'][grid_index] >= 5000:\n",
    "    vsini_values = np.array([1.5, 3.0, 6.0, 9.0, 12.0, 18.0, 24.0]) # km/s\n",
    "if grids['teff_subgrid'][grid_index] >= 6000:\n",
    "    vsini_values = np.array([1.5, 3.0, 6.0, 9.0, 12.0, 18.0, 24.0, 36.0]) # km/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Spectra and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_spectrum_broad = dict()\n",
    "for ccd in [1,2,3,4]:\n",
    "    null_spectrum = readsav(synthesis_files+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_0_'+str(ccd)+'.out').results[0]\n",
    "    null_spectrum_broad['wave_null_ccd'+str(ccd)],null_spectrum_broad['spectrum_null_ccd'+str(ccd)] = broaden_spectrum(\n",
    "            null_spectrum.wint,\n",
    "            null_spectrum.sint,\n",
    "            null_spectrum.wave,\n",
    "            null_spectrum.cmod,\n",
    "            vsini = vsini_values[-1]\n",
    "        )\n",
    "print('The synthetic spectra come with keywords ',null_spectrum.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(training_set_vsini0.keys()[2:-1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_labels = []\n",
    "for label in labels:\n",
    "    if label == 'teff':\n",
    "        fancy_labels.append(r'$T_\\mathrm{eff}~/~\\mathrm{K}$')\n",
    "    elif label == 'logg':\n",
    "        fancy_labels.append(r'$\\log (g~/~\\mathrm{cm\\,s^{-2}})$')\n",
    "    elif label == 'fe_h':\n",
    "        fancy_labels.append(r'$\\mathrm{[Fe/H]}$')\n",
    "    elif label == 'vmic':\n",
    "        fancy_labels.append(r'$v_\\mathrm{mic}~/~\\mathrm{km\\,s^{-1}}$')\n",
    "    elif label == 'vsini':\n",
    "        fancy_labels.append(r'$v \\sin i~/~\\mathrm{km\\,s^{-1}}$')\n",
    "    elif label[-3:] == '_fe':\n",
    "        fancy_labels.append('$\\mathrm{['+label[0].upper()+label[1:-3]+'/Fe]}$')\n",
    "    else:\n",
    "        print('No entry for '+label)\n",
    "print(fancy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_spectra_up = Table()\n",
    "gradient_spectra_up['wave'] = np.concatenate(([null_spectrum_broad['wave_null_ccd'+str(ccd)] for ccd in [1,2,3,4]]))\n",
    "gradient_spectra_up['median'] = np.concatenate(([null_spectrum_broad['spectrum_null_ccd'+str(ccd)] for ccd in [1,2,3,4]]))\n",
    "\n",
    "gradient_spectra_down = Table()\n",
    "gradient_spectra_down['wave'] = np.concatenate(([null_spectrum_broad['wave_null_ccd'+str(ccd)] for ccd in [1,2,3,4]]))\n",
    "gradient_spectra_down['median'] = np.concatenate(([null_spectrum_broad['spectrum_null_ccd'+str(ccd)] for ccd in [1,2,3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_index, label in enumerate(labels):\n",
    "    \n",
    "    gradient_up = []\n",
    "    gradient_down = []\n",
    "    \n",
    "    for ccd in [1,2,3,4]:\n",
    "        spectra_available = False\n",
    "        try:\n",
    "            increased_spectrum = readsav(synthesis_files+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_'+str(2+label_index)+'_'+str(ccd)+'.out').results[0]\n",
    "            decreased_spectrum = readsav(synthesis_files+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_'+str(37+label_index)+'_'+str(ccd)+'.out').results[0]\n",
    "            spectra_available = True\n",
    "        except:\n",
    "            try:\n",
    "                increased_spectrum = readsav(synthesis_files+'/galah_dr4_cannon_trainingset_'+teff_logg_feh_name+'_'+str(2+label_index)+'_'+str(ccd)+'.out').results[0]\n",
    "                decreased_spectrum = readsav(synthesis_files+'/galah_dr4_cannon_trainingset_'+teff_logg_feh_name+'_'+str(37+label_index)+'_'+str(ccd)+'.out').results[0]\n",
    "                spectra_available = True\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        if spectra_available:\n",
    "            wave_increase, spectrum_increase = broaden_spectrum(\n",
    "                increased_spectrum.wint,\n",
    "                increased_spectrum.sint,\n",
    "                increased_spectrum.wave,\n",
    "                increased_spectrum.cmod,\n",
    "                vsini = vsini_values[-1]\n",
    "            )\n",
    "\n",
    "            wave_decrease, spectrum_decrease = broaden_spectrum(\n",
    "                decreased_spectrum.wint,\n",
    "                decreased_spectrum.sint,\n",
    "                decreased_spectrum.wave,\n",
    "                decreased_spectrum.cmod,\n",
    "                vsini = vsini_values[-1]\n",
    "            )\n",
    "\n",
    "            gradient_up.append(spectrum_increase - null_spectrum_broad['spectrum_null_ccd'+str(ccd)])\n",
    "            gradient_down.append(spectrum_decrease - null_spectrum_broad['spectrum_null_ccd'+str(ccd)])\n",
    "        else:\n",
    "            if label == 'teff':\n",
    "                print('No gradient spectrum for Teff available (possible for grid edges e.g. 8000K) - fixing by returning 1s')\n",
    "                gradient_up.append(np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "                gradient_down.append(-np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "            \n",
    "            elif label == 'logg':\n",
    "                print('No gradient spectrum for logg available (possible for grid edges e.g. 5.0) - fixing by returning 1s')\n",
    "                gradient_up.append(np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "                gradient_down.append(-np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "            \n",
    "            elif label == 'fe_h':\n",
    "                print('No gradient spectrum for fe_h available (possible for grid edges e.g. luminous giants) - fixing by returning 1s')\n",
    "                gradient_up.append(np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "                gradient_down.append(-np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "            \n",
    "            elif label == 'o_fe':\n",
    "                print('No gradient spectrum for ofe available (possible for cool stars) - fixing by returning 1s')\n",
    "                gradient_up.append(np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "                gradient_down.append(-np.ones(len(null_spectrum_broad['wave_null_ccd'+str(ccd)])))\n",
    "            \n",
    "            else:\n",
    "                print(label)\n",
    "            \n",
    "    gradient_spectra_up[label] = np.concatenate((gradient_up))\n",
    "    gradient_spectra_down[label] = np.concatenate((gradient_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_beta = (gradient_spectra_up['wave'] >= 4860.90 - 1) & (gradient_spectra_up['wave'] <= 4861.77 + 1)\n",
    "h_alpha = (gradient_spectra_up['wave'] >= 6562.00 - 1) & (gradient_spectra_up['wave'] <= 6563.60 + 1)\n",
    "usual_galah_wavelength_range = (\n",
    "    ((gradient_spectra_up['wave'] > 4710) & (gradient_spectra_up['wave'] < 4905)) |\n",
    "    ((gradient_spectra_up['wave'] > 5645) & (gradient_spectra_up['wave'] < 5880)) |\n",
    "    ((gradient_spectra_up['wave'] > 6470) & (gradient_spectra_up['wave'] < 6750)) |\n",
    "    ((gradient_spectra_up['wave'] > 7670) & (gradient_spectra_up['wave'] < 7900))\n",
    ")\n",
    "usual_galah_range_without_balmer_cores = (~h_beta) & (~h_alpha) & usual_galah_wavelength_range\n",
    "\n",
    "total = len(gradient_spectra_up)\n",
    "total_usual = len(gradient_spectra_up[usual_galah_range_without_balmer_cores])\n",
    "print('Total points: '+str(total)+', within GALAH range (exluding Balmer cores): '+str(total_usual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_masks = Table()\n",
    "\n",
    "percentage_used = []\n",
    "\n",
    "Path('gradient_spectra/'+teff_logg_feh_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for label_index, label in enumerate(labels):\n",
    "    print(label, training_set_vsini0[label][2+label_index]-training_set_vsini0[label][0])\n",
    "       \n",
    "    threshold1 = 0.0001\n",
    "    threshold2 = 0.001\n",
    "    \n",
    "\n",
    "    below_threshold1 = len(np.where(np.max([np.abs(gradient_spectra_up[label][usual_galah_range_without_balmer_cores]),np.abs(gradient_spectra_down[label][usual_galah_range_without_balmer_cores])],axis=0) >= threshold1)[0])\n",
    "    below_threshold2 = len(np.where(np.max([np.abs(gradient_spectra_up[label][usual_galah_range_without_balmer_cores]),np.abs(gradient_spectra_down[label][usual_galah_range_without_balmer_cores])],axis=0) >= threshold2)[0])\n",
    "    \n",
    "    print(str(threshold1)+':   ',\"{:.1f}\".format(100*below_threshold1/total_usual)+'%',below_threshold1)\n",
    "    print(str(threshold2)+':   ',\"{:.1f}\".format(100*below_threshold2/total_usual)+'%',below_threshold2)\n",
    "    \n",
    "    percentage_used.append([fancy_labels[label_index], r'$\\pm$'+str(training_set_vsini0[label][2+label_index]-training_set_vsini0[label][0]), \"{:.1f}\".format(100*below_threshold1/total_usual),\"{:.1f}\".format(100*below_threshold2/total_usual)])\n",
    "    \n",
    "    above_threshold1 = (np.max([np.abs(gradient_spectra_up[label]),np.abs(gradient_spectra_down[label])],axis=0) >= threshold1) & usual_galah_range_without_balmer_cores\n",
    "    above_threshold2 = (np.max([np.abs(gradient_spectra_up[label]),np.abs(gradient_spectra_down[label])],axis=0) >= threshold2) & usual_galah_range_without_balmer_cores\n",
    "\n",
    "    grid_masks[label] = above_threshold2\n",
    "    \n",
    "    f, gs = plt.subplots(1,4,figsize=(15,2.5),sharey=True)\n",
    "    for ccd in [1,2,3,4]:\n",
    "        plot_label = '_nolegend_'\n",
    "        if ccd == 2:\n",
    "            plot_label = r'$\\Delta f$ for $\\Delta$'+fancy_labels[label_index]+' = '+str(training_set_vsini0[label][2+label_index]-training_set_vsini0[label][0])\n",
    "        in_ccd = (gradient_spectra_up['wave'] > (3+ccd)*1000) & (gradient_spectra_up['wave'] < (4+ccd)*1000)\n",
    "        ax=gs[ccd-1]\n",
    "        if ccd == 1:\n",
    "            ax.axvspan(4860.90, 4861.77, color='purple',alpha=0.3)\n",
    "        if ccd == 3:\n",
    "            ax.axvspan(6562.00, 6563.60, color='purple',alpha=0.3)\n",
    "        ax.plot(\n",
    "            gradient_spectra_up['wave'][in_ccd],\n",
    "            gradient_spectra_up[label][in_ccd],\n",
    "            c='k',lw=0.5,label = plot_label\n",
    "        )\n",
    "        plot_label = '_nolegend_'\n",
    "        if ccd == 3:\n",
    "            plot_label = r'$-\\Delta f$ for $\\Delta$'+fancy_labels[label_index]+' = '+str(training_set_vsini0[label][37+label_index]-training_set_vsini0[label][0])\n",
    "        ax.plot(\n",
    "            gradient_spectra_down['wave'][in_ccd],\n",
    "            -gradient_spectra_down[label][in_ccd],\n",
    "            c='C0',lw=0.5,label = plot_label\n",
    "        )\n",
    "        ax.set_xlabel(r'Wavelength [$\\AA$]')\n",
    "        if ccd==1:\n",
    "            ax.set_ylabel(r'$\\Delta f~/~\\mathrm{norm.}$')\n",
    "        plot_label = '_nolegend_'\n",
    "        if ccd == 4:\n",
    "            plot_label = r'$\\vert\\Delta f\\vert$ above '+str(0.0001)\n",
    "        ax.scatter(\n",
    "            gradient_spectra_up['wave'][(above_threshold1 & in_ccd)],\n",
    "            np.zeros(len(np.where(above_threshold1 & in_ccd==True)[0])),\n",
    "            c='red',s=2,label=plot_label\n",
    "        )\n",
    "        plot_label = '_nolegend_'\n",
    "        if ccd == 4:\n",
    "            plot_label = r'$\\vert\\Delta f\\vert$ above '+str(0.001)\n",
    "        ax.scatter(\n",
    "            gradient_spectra_up['wave'][(above_threshold2 & in_ccd)],\n",
    "            np.zeros(len(np.where(above_threshold2 & in_ccd==True)[0])),\n",
    "            c='orange',s=2,label=plot_label\n",
    "        )\n",
    "        ax.set_ylim(\n",
    "            np.min([np.min(gradient_spectra_up[label]),-3*threshold1]),\n",
    "            np.max([np.max(gradient_spectra_up[label]),3*threshold1])\n",
    "        )\n",
    "        if ccd in [2,3,4]:\n",
    "            if label not in ['teff']:\n",
    "                ax.legend(loc='lower center')\n",
    "            else:\n",
    "                ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gradient_spectra/'+teff_logg_feh_name+'/gradient_spectrum_'+teff_logg_feh_name+'_'+label+'.png',dpi=200,bbox_inches='tight')\n",
    "    if grid_index in [1931]:\n",
    "        if sys.argv[1] == '-f':\n",
    "            plt.show()\n",
    "        try:\n",
    "            plt.savefig('../galah_dr4_paper/figures/gradient_spectrum_'+teff_logg_feh_name+'_'+label+'.png',dpi=200,bbox_inches='tight')\n",
    "        except:\n",
    "            pass\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_index in [1931]:\n",
    "    table_text = [\n",
    "    [r'\\begin{table}[!ht]'],\n",
    "    [r'    \\centering'],\n",
    "    [r'    \\caption{Example of mask estimation for \\textit{The Cannon}/\\textit{The Payne} model creation. Listed are percentages of the spectrum that respond to an in-/decrease of each label above 0.001 and 0.0001 of the normalised flux.}'],\n",
    "    [r'    \\label{tab:cannon_mask_percentage}'],\n",
    "    [r'    \\begin{tabular}{cccc}'],\n",
    "    [r'    \\hline \\hline'],\n",
    "    [r'    Label &  Label change & $\\vert \\Delta f \\vert > 0.001~/~\\%$ & $\\vert \\Delta f \\vert > 0.0001~/~\\%$ \\\\'],\n",
    "    [r'    \\hline']\n",
    "    ]\n",
    "    for each in percentage_used:\n",
    "        table_text.append([r'    '+each[0]+' & '+each[1]+' & '+each[3]+' & '+each[2]+r' \\\\'])\n",
    "    table_text.append([r'    \\hline'])\n",
    "    table_text.append([r'    \\end{tabular}'])\n",
    "    table_text.append([r'\\end{table}'])\n",
    "\n",
    "    try:\n",
    "        np.savetxt('../galah_dr4_paper/tables/mask_percentage_1931.tex',np.array(table_text),fmt='%s')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('training_input/'+teff_logg_feh_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gradient_spectra_up.write('gradient_spectra/'+teff_logg_feh_name+'/'+teff_logg_feh_name+'_gradient_spectra_up.fits',overwrite=True)\n",
    "gradient_spectra_down.write('gradient_spectra/'+teff_logg_feh_name+'/'+teff_logg_feh_name+'_gradient_spectra_down.fits',overwrite=True)\n",
    "grid_masks.write('training_input/'+teff_logg_feh_name+'/'+teff_logg_feh_name+'_masks.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainingset flux and ivar at different vsini values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the full trainingset (including vsini sampled from vsini_values)\n",
    "\n",
    "full_trainingset = Table()\n",
    "for label in training_set_vsini0.keys()[:6]:\n",
    "    full_trainingset[label] = np.concatenate((np.array([training_set_vsini0[label] for vsini in vsini_values])))\n",
    "full_trainingset['vsini'] = np.concatenate((np.array([vsini*np.ones(len(training_set_vsini0['spectrum_index'])) for vsini in vsini_values])))\n",
    "for label in training_set_vsini0.keys()[6:]:\n",
    "    full_trainingset[label] = np.concatenate((np.array([training_set_vsini0[label] for vsini in vsini_values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the wavelength array, if not yet available\n",
    "\n",
    "wavelength_array = np.concatenate(([null_spectrum_broad['wave_null_ccd'+str(ccd)] for ccd in [1,2,3,4]]))\n",
    "wavelength_file = 'training_input/galah_dr4_3dbin_wavelength_array.pickle'\n",
    "if not os.path.isfile(wavelength_file):\n",
    "    wavelength_file_opener = open(wavelength_file,'wb')\n",
    "    pickle.dump((wavelength_array),wavelength_file_opener)\n",
    "    wavelength_file_opener.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_normalised_spectra(spectrum_index, vsini):\n",
    "    \n",
    "    normalised_flux_for_index = []\n",
    "    #normalised_ivar_for_index = []\n",
    "    \n",
    "    spectrum_available = True\n",
    "    \n",
    "    try:\n",
    "        for ccd in [1,2,3,4]:\n",
    "\n",
    "            try:\n",
    "                synthetic_spectrum = readsav(synthesis_files+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_'+str(spectrum_index)+'_'+str(ccd)+'.out').results[0]\n",
    "            except:\n",
    "                synthetic_spectrum = readsav(synthesis_files+'/galah_dr4_cannon_trainingset_'+teff_logg_feh_name+'_'+str(spectrum_index)+'_'+str(ccd)+'.out').results[0]\n",
    "\n",
    "            wave_broadened,flux_broadened = broaden_spectrum(\n",
    "                synthetic_spectrum.wint,\n",
    "                synthetic_spectrum.sint,\n",
    "                synthetic_spectrum.wave,\n",
    "                synthetic_spectrum.cmod,\n",
    "                vsini=vsini)\n",
    "\n",
    "            normalised_flux_for_index.append(flux_broadened)\n",
    "\n",
    "        normalised_flux_for_index = np.concatenate((normalised_flux_for_index))\n",
    "\n",
    "    except:\n",
    "        normalised_flux_for_index = 0\n",
    "        spectrum_available = False\n",
    "        \n",
    "    return(normalised_flux_for_index, spectrum_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def populate_normalised_flux_and_ivar_matrix(index):\n",
    "        \n",
    "    vsini = full_trainingset['vsini'][index]\n",
    "    spectrum_index = full_trainingset['spectrum_index'][index]\n",
    "    \n",
    "    normalised_flux_for_index, spectrum_available = prepare_normalised_spectra(spectrum_index,vsini=vsini)\n",
    "    return(normalised_flux_for_index, spectrum_available)\n",
    "    \n",
    "normalized_flux = np.ones((np.shape(full_trainingset)[0],np.shape(wavelength_array)[0]))\n",
    "spectra_available = np.ones(np.shape(full_trainingset)[0],dtype=bool)\n",
    "\n",
    "start = time.time()\n",
    "now = time.time()\n",
    "\n",
    "for index in range(len(full_trainingset)):\n",
    "    normalised_flux_for_index,spectrum_available = populate_normalised_flux_and_ivar_matrix(index)\n",
    "    if spectrum_available:\n",
    "        normalized_flux[index] = normalised_flux_for_index\n",
    "    else:\n",
    "        spectra_available[index] = spectrum_available\n",
    "    \n",
    "    print(index,spectrum_available,time.time()-now,time.time()-start)\n",
    "    now = time.time()\n",
    "    \n",
    "now = time.time()\n",
    "print(index,time.time()-now,time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(full_trainingset[spectra_available]).write('training_input/'+teff_logg_feh_name+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_incl_vsini.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_ivar_file = 'training_input/'+teff_logg_feh_name+'/galah_dr4_trainingset_'+teff_logg_feh_name+'_incl_vsini_flux_ivar.pickle'\n",
    "\n",
    "flux_ivar_file_opener = open(flux_ivar_file,'wb')\n",
    "pickle.dump((normalized_flux[spectra_available]),flux_ivar_file_opener)\n",
    "flux_ivar_file_opener.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if grid_index not in [\n",
    "        1832,1833,1834,\n",
    "        1844,1845,1846,\n",
    "        1918,1919,1920,\n",
    "        1930,1931,1932,\n",
    "        2001,2002,2003,\n",
    "        2013,2014,2015\n",
    "    ]:\n",
    "        os.system('rm -rf /avatar/buder/GALAH_DR4/spectrum_grids/3d_bin_subgrids/'+teff_logg_feh_name)\n",
    "        print('Removed /avatar/buder/GALAH_DR4/spectrum_grids/3d_bin_subgrids/'+teff_logg_feh_name)\n",
    "    os.system('ipython galah_dr4_grid_interpolation_recommend_labels.py '+str(grid_index))\n",
    "    print('Recommended labels for '+str(grid_index))\n",
    "except:\n",
    "    print('Could not remove /avatar/buder/GALAH_DR4/spectrum_grids/3d_bin_subgrids/'+teff_logg_feh_name)\n",
    "    print('Could not recommend labels to fit for '+str(grid_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
