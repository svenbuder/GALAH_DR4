{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use selected training set to create input for fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic Tools\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import readsav\n",
    "from scipy.ndimage.filters import convolve\n",
    "import time\n",
    "\n",
    "# The Cannon\n",
    "# import thecannon as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "galah_elements = [\n",
    "        'Li','C','O',\n",
    "        'Na','Mg','Al','Si',\n",
    "        'K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn',\n",
    "        'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "        'Ba','La','Ce','Nd','Sm','Eu'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apogee = Table.read('/Users/svenbuder/Surveys/APOGEE_DR16.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: hdu= was not specified but multiple tables are present, reading in first available table (hdu=1) [astropy.io.fits.connect]\n"
     ]
    }
   ],
   "source": [
    "synthesis_files = '/Users/svenbuder/GALAH_DR4/spectrum_grids/marcs2014/specout/'\n",
    "wavelength_file = 'training_sets/solar_twin_5steps_training_marcs2014_wavelength.pickle'\n",
    "\n",
    "training_set = Table.read('../../spectrum_grids/marcs2014/marcs2014_scaledsolar_210720.fits')\n",
    "training_set['INDEX'][np.where((training_set['TEFF'] == 5250) & (training_set['FEH'] == -3))[0]]\n",
    "# #2159,2216,3967\n",
    "\n",
    "example_0p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_2159_ccd1_smod_sint.sav').results[0]\n",
    "example_2p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_2216_ccd1_smod_sint.sav').results[0]\n",
    "example_4p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_3967_ccd1_smod_sint.sav').results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gaussbroad(w, s, hwhm):\n",
    "    \"\"\"\n",
    "    Smooths a spectrum by convolution with a gaussian of specified hwhm.\n",
    "    Parameters\n",
    "    -------\n",
    "    w : array[n]\n",
    "        wavelength scale of spectrum to be smoothed\n",
    "    s : array[n]\n",
    "        spectrum to be smoothed\n",
    "    hwhm : float\n",
    "        half width at half maximum of smoothing gaussian.\n",
    "    Returns\n",
    "    -------\n",
    "    sout: array[n]\n",
    "        the gaussian-smoothed spectrum.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    --------\n",
    "        Dec-90 GB,GM\n",
    "            Rewrote with fourier convolution algorithm.\n",
    "        Jul-91 AL\n",
    "            Translated from ANA to IDL.\n",
    "        22-Sep-91 JAV\n",
    "            Relaxed constant dispersion check# vectorized, 50% faster.\n",
    "        05-Jul-92 JAV\n",
    "            Converted to function, handle nonpositive hwhm.\n",
    "        Oct-18 AW\n",
    "            Python version\n",
    "    \"\"\"\n",
    "\n",
    "    # Warn user if hwhm is negative.\n",
    "    if hwhm < 0:\n",
    "        logger.warning(\"Forcing negative smoothing width to zero.\")\n",
    "\n",
    "    # Return input argument if half-width is nonpositive.\n",
    "    if hwhm <= 0:\n",
    "        return s  # true: no broadening\n",
    "\n",
    "    # Calculate (uniform) dispersion.\n",
    "    nw = len(w)  ## points in spectrum\n",
    "    wrange = w[-1] - w[0]\n",
    "    dw = wrange / (nw - 1)  # wavelength change per pixel\n",
    "\n",
    "    # Make smoothing gaussian# extend to 4 sigma.\n",
    "    # 4.0 / sqrt(2.0*alog(2.0)) = 3.3972872 and sqrt(alog(2.0))=0.83255461\n",
    "    # sqrt(alog(2.0)/pi)=0.46971864 (*1.0000632 to correct for >4 sigma wings)\n",
    "    if hwhm >= 5 * wrange:\n",
    "        return np.full(nw, np.sum(s) / nw)\n",
    "    nhalf = int(3.3972872 * hwhm / dw)  ## points in half gaussian\n",
    "    ng = 2 * nhalf + 1  ## points in gaussian (odd!)\n",
    "    wg = dw * (\n",
    "        np.arange(ng, dtype=float) - (ng - 1) / 2\n",
    "    )  # wavelength scale of gaussian\n",
    "    xg = (0.83255461 / hwhm) * wg  # convenient absisca\n",
    "    gpro = (0.46974832 * dw / hwhm) * np.exp(-xg * xg)  # unit area gaussian w/ FWHM\n",
    "    gpro = gpro / np.sum(gpro)\n",
    "\n",
    "    # Pad spectrum ends to minimize impact of Fourier ringing.\n",
    "    sout = convolve(s, gpro, mode=\"nearest\")\n",
    "\n",
    "    return sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def apply_gauss_broad(wave, smod, ipres=30000, debug=True):\n",
    "    ccd_time = time.perf_counter()\n",
    "    # Apply Gaussian Instrument Broadening\n",
    "    if ipres == 0.0:\n",
    "        hwhm = 0\n",
    "    else:\n",
    "        hwhm = 0.5 * wave[0] / ipres\n",
    "    if hwhm > 0: smod = gaussbroad(wave, smod, hwhm)\n",
    "\n",
    "    if debug:\n",
    "        ccd_time = time.perf_counter() - ccd_time\n",
    "        print('Gaussbroad time: ',ccd_time)\n",
    "\n",
    "    return(smod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ccd1 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_0_ccd1_smod_sint.sav').results[0]\n",
    "example_ccd2 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_0_ccd2_smod_sint.sav').results[0]\n",
    "example_ccd3 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_0_ccd3_smod_sint.sav').results[0]\n",
    "example_ccd4 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_0_ccd4_smod_sint.sav').results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a common wavelength grid that we will use for all spectra\n",
    "# This one was used for GALAH DR2\n",
    "wavelengths_for_each_ccd = dict()\n",
    "wavelengths_for_each_ccd['CCD1'] = example_ccd1.wave[1:-1]\n",
    "wavelengths_for_each_ccd['CCD2'] = example_ccd2.wave[1:-1]\n",
    "wavelengths_for_each_ccd['CCD3'] = example_ccd3.wave[1:-1]\n",
    "wavelengths_for_each_ccd['CCD4'] = example_ccd4.wave[1:-1]\n",
    "\n",
    "wavelength_array = np.concatenate(([wavelengths_for_each_ccd['CCD'+ccd] for ccd in ['1','2','3','4']]))\n",
    "#wavelength_array = np.concatenate(([wavelengths_for_each_ccd['CCD'+ccd] for ccd in ['3','4']]))\n",
    "\n",
    "wavelength_file_opener = open(wavelength_file,'wb')\n",
    "pickle.dump((wavelength_array),wavelength_file_opener)\n",
    "wavelength_file_opener.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a matrix that we will later fill with the normalised flux values\n",
    "normalized_flux = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "normalized_ivar = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_normalised_spectra(index, wavelengths_for_each_ccd, spectrum_path = '/Users/svenbuder/galah_solar_twins/dr3_spectra/hermes'):\n",
    "    \n",
    "    # For each stars, there are 4 spectra for the 4 different CCDs.\n",
    "    # We will interpolate the fluxes and uncertainties/inverse variances onto a common grid\n",
    "    normalised_flux_for_index = []\n",
    "    normalised_ivar_for_index = []\n",
    "    # For that we first interpolate over the individual CCDs\n",
    "    #for ccd in ['3','4']:\n",
    "    for ccd in ['1','2','3','4']:\n",
    "    #for ccd in ['2','3']:\n",
    "        \n",
    "        #synthetic_spectrum = readsav(synthesis_files+'/solar_twin_grid_210831_'+str(index)+'_ccd'+ccd+'_smod_sint.sav').results[0]\n",
    "        synthetic_spectrum = readsav(synthesis_files+'/marcs2014_scaledsolar_210720_'+str(index)+'_ccd'+ccd+'_smod_sint.sav').results[0]\n",
    "        \n",
    "        #broadened_smod = apply_gauss_broad(synthetic_spectrum.wave,synthetic_spectrum.smod,ipres=25000,debug=False)\n",
    "        broadened_smod = synthetic_spectrum.smod\n",
    "        \n",
    "        interpolated_broadened_smod = np.interp(wavelengths_for_each_ccd['CCD'+ccd],synthetic_spectrum.wave,broadened_smod)\n",
    "        \n",
    "        normalised_flux_for_index.append(interpolated_broadened_smod)\n",
    "        # We use synthetic spectra, so SNR == infinity\n",
    "        # Let's assume SNR = 1000, so std == 0.001, so 1/var = 1/std**2 = 1,000,000.\n",
    "        normalised_ivar_for_index.append(1000000.*np.ones_like(interpolated_broadened_smod))\n",
    "        \n",
    "    normalised_flux_for_index = np.concatenate((normalised_flux_for_index))\n",
    "    normalised_ivar_for_index = np.concatenate((normalised_ivar_for_index))\n",
    "    \n",
    "    return(normalised_flux_for_index,normalised_ivar_for_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def populate_normalised_flux_and_ivar_matrix(training_set, matrix_index, wavelengths_for_each_ccd):\n",
    "    index = training_set['INDEX'][matrix_index]\n",
    "    #try:\n",
    "    normalised_flux_for_index, normalised_ivar_for_index = load_normalised_spectra(index,wavelengths_for_each_ccd=wavelengths_for_each_ccd)\n",
    "    #except:\n",
    "    #    print('Failed to load spectrum for index '+str(matrix_index)+', that is, index '+str(index))\n",
    "    normalized_flux[matrix_index] = normalised_flux_for_index\n",
    "    normalized_ivar[matrix_index] = normalised_ivar_for_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250.0 4.5 0.0 75\n"
     ]
    }
   ],
   "source": [
    "training_set_all = Table.read('../../spectrum_grids/marcs2014/marcs2014_scaledsolar_210720.fits')\n",
    "\n",
    "# 2160 models with\n",
    "# teff 2500..(100/250)..8000 K including 5 at a time\n",
    "# logg = -0.5..(0.5)..5.0/5.5 dex\n",
    "# feh = -3.0..(0.5/0.25)..1.0 dex\n",
    "\n",
    "teff_points = np.unique(training_set_all['TEFF'])\n",
    "logg_points = np.unique(training_set_all['LOGG'])\n",
    "fe_h_points = np.unique(training_set_all['FEH'])\n",
    "\n",
    "cannon_index = []\n",
    "cannon_teff = []\n",
    "cannon_logg = []\n",
    "cannon_feh = []\n",
    "\n",
    "i = 0\n",
    "for teff in teff_points[2:-3]:\n",
    "    for logg in logg_points[1:-2]:\n",
    "        for fe_h in fe_h_points[4:-3]:\n",
    "            teff_i = np.where(teff==teff_points)[0][0]\n",
    "            logg_i = np.where(logg==logg_points)[0][0]\n",
    "            fe_h_i = np.where(fe_h==fe_h_points)[0][0]\n",
    "            \n",
    "            grid = (\n",
    "                    (training_set_all['TEFF'] >= teff_points[teff_i-2]) &\n",
    "                    (training_set_all['TEFF'] <= teff_points[teff_i+2]) &\n",
    "                    (training_set_all['LOGG'] >= logg_points[logg_i-1]) &\n",
    "                    (training_set_all['LOGG'] <= logg_points[logg_i+1]) &\n",
    "                    (training_set_all['FEH'] >= fe_h_points[fe_h_i-2]) &\n",
    "                    (training_set_all['FEH'] <= fe_h_points[fe_h_i+2]) &\n",
    "                    ((training_set_all['INDEX'] < 107) | (training_set_all['INDEX'] > 109)) &\n",
    "                    ((training_set_all['INDEX'] < 119) | (training_set_all['INDEX'] > 122)) &\n",
    "                    ((training_set_all['INDEX'] < 131) | (training_set_all['INDEX'] > 134)) &\n",
    "                    ((training_set_all['INDEX'] < 143) | (training_set_all['INDEX'] > 146)) &\n",
    "                    ((training_set_all['INDEX'] < 203) | (training_set_all['INDEX'] > 205)) &\n",
    "                    ((training_set_all['INDEX'] < 212) | (training_set_all['INDEX'] > 213)) &\n",
    "                    ((training_set_all['INDEX'] < 3980) | (training_set_all['INDEX'] > 3980)) \n",
    "                \n",
    "                )\n",
    "            \n",
    "            grid_size = len(training_set_all[grid])\n",
    "            \n",
    "            if grid_size > 10:\n",
    "\n",
    "                cannon_index.append(i)\n",
    "                cannon_teff.append(teff)\n",
    "                cannon_logg.append(logg)\n",
    "                cannon_feh.append(fe_h)\n",
    "\n",
    "                #if (teff == 5500) & (logg == 4.5) & (fe_h == 0.00):\n",
    "                #    print(i)\n",
    "                \n",
    "                if i in [1813]:#550,1000,1206,1656,1660,1664,1668,1669,1670,1671,1672,1673,1744,2000,2050]:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #if (grid_size > 0) & (grid_size < 50):\n",
    "                    print(teff,logg,fe_h,grid_size)\n",
    "\n",
    "                    training_set = copy.deepcopy(training_set_all[grid])\n",
    "                    training_set.write('training_sets/subgrid_'+str(i)+'_training_set_marcs2014.fits',overwrite=True)\n",
    "\n",
    "                    flux_ivar_file = 'training_sets/subgrid_'+str(i)+'_training_marcs2014_flux_ivar.pickle'\n",
    "\n",
    "                    # Let's create a matrix that we will later fill with the normalised flux values\n",
    "                    normalized_flux = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "                    normalized_ivar = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "\n",
    "                    [populate_normalised_flux_and_ivar_matrix(training_set, matrix_index=index, wavelengths_for_each_ccd=wavelengths_for_each_ccd) for index in range(np.shape(training_set)[0])];\n",
    "\n",
    "                    flux_ivar_file_opener = open(flux_ivar_file,'wb')\n",
    "                    pickle.dump((normalized_flux,normalized_ivar),flux_ivar_file_opener)\n",
    "                    flux_ivar_file_opener.close()\n",
    "\n",
    "                i+= 1\n",
    "\n",
    "cannon_grid = Table()\n",
    "cannon_grid['index'] = np.array(cannon_index)\n",
    "cannon_grid['teff'] = np.array(cannon_teff)\n",
    "cannon_grid['logg'] = np.array(cannon_logg)\n",
    "cannon_grid['fe_h'] = np.array(cannon_feh)\n",
    "cannon_grid.write('Cannon_subgrid.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
