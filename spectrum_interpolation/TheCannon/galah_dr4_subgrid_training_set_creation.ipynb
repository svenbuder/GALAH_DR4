{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use selected training set to create input for fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility with Python 3\n",
    "from __future__ import (absolute_import, division, print_function)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Basic Tools\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import readsav\n",
    "from scipy.ndimage.filters import convolve\n",
    "import time\n",
    "\n",
    "# The Cannon\n",
    "# import thecannon as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "galah_elements = [\n",
    "    'Li','C','O',\n",
    "    'Na','Mg','Al','Si',\n",
    "    'K','Ca','Sc','Ti','V','Cr','Mn','Co','Ni','Cu','Zn',\n",
    "    'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "    'Ba','La','Ce','Nd','Sm','Eu'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: hdu= was not specified but multiple tables are present, reading in first available table (hdu=1) [astropy.io.fits.connect]\n"
     ]
    }
   ],
   "source": [
    "synthesis_files = '/Users/svenbuder/GALAH_DR4/spectrum_grids/marcs2014/specout/'\n",
    "wavelength_file = 'training_sets/solar_twin_5steps_training_marcs2014_wavelength.pickle'\n",
    "\n",
    "training_set = Table.read('../../spectrum_grids/marcs2014/marcs2014_scaledsolar_210720.fits')\n",
    "training_set['INDEX'][np.where((training_set['TEFF'] == 5250) & (training_set['FEH'] == -3))[0]]\n",
    "# #2159,2216,3967\n",
    "\n",
    "example_0p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_2159_ccd1_smod_sint.sav').results[0]\n",
    "example_2p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_2216_ccd1_smod_sint.sav').results[0]\n",
    "example_4p5 = readsav(synthesis_files+'marcs2014_scaledsolar_210720_3967_ccd1_smod_sint.sav').results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gaussbroad(w, s, hwhm):\n",
    "    \"\"\"\n",
    "    Smooths a spectrum by convolution with a gaussian of specified hwhm.\n",
    "    Parameters\n",
    "    -------\n",
    "    w : array[n]\n",
    "        wavelength scale of spectrum to be smoothed\n",
    "    s : array[n]\n",
    "        spectrum to be smoothed\n",
    "    hwhm : float\n",
    "        half width at half maximum of smoothing gaussian.\n",
    "    Returns\n",
    "    -------\n",
    "    sout: array[n]\n",
    "        the gaussian-smoothed spectrum.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    --------\n",
    "        Dec-90 GB,GM\n",
    "            Rewrote with fourier convolution algorithm.\n",
    "        Jul-91 AL\n",
    "            Translated from ANA to IDL.\n",
    "        22-Sep-91 JAV\n",
    "            Relaxed constant dispersion check# vectorized, 50% faster.\n",
    "        05-Jul-92 JAV\n",
    "            Converted to function, handle nonpositive hwhm.\n",
    "        Oct-18 AW\n",
    "            Python version\n",
    "    \"\"\"\n",
    "\n",
    "    # Warn user if hwhm is negative.\n",
    "    if hwhm < 0:\n",
    "        logger.warning(\"Forcing negative smoothing width to zero.\")\n",
    "\n",
    "    # Return input argument if half-width is nonpositive.\n",
    "    if hwhm <= 0:\n",
    "        return s  # true: no broadening\n",
    "\n",
    "    # Calculate (uniform) dispersion.\n",
    "    nw = len(w)  ## points in spectrum\n",
    "    wrange = w[-1] - w[0]\n",
    "    dw = wrange / (nw - 1)  # wavelength change per pixel\n",
    "\n",
    "    # Make smoothing gaussian# extend to 4 sigma.\n",
    "    # 4.0 / sqrt(2.0*alog(2.0)) = 3.3972872 and sqrt(alog(2.0))=0.83255461\n",
    "    # sqrt(alog(2.0)/pi)=0.46971864 (*1.0000632 to correct for >4 sigma wings)\n",
    "    if hwhm >= 5 * wrange:\n",
    "        return np.full(nw, np.sum(s) / nw)\n",
    "    nhalf = int(3.3972872 * hwhm / dw)  ## points in half gaussian\n",
    "    ng = 2 * nhalf + 1  ## points in gaussian (odd!)\n",
    "    wg = dw * (\n",
    "        np.arange(ng, dtype=float) - (ng - 1) / 2\n",
    "    )  # wavelength scale of gaussian\n",
    "    xg = (0.83255461 / hwhm) * wg  # convenient absisca\n",
    "    gpro = (0.46974832 * dw / hwhm) * np.exp(-xg * xg)  # unit area gaussian w/ FWHM\n",
    "    gpro = gpro / np.sum(gpro)\n",
    "\n",
    "    # Pad spectrum ends to minimize impact of Fourier ringing.\n",
    "    sout = convolve(s, gpro, mode=\"nearest\")\n",
    "\n",
    "    return sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def apply_gauss_broad(wave, smod, ipres=30000, debug=True):\n",
    "    ccd_time = time.perf_counter()\n",
    "    # Apply Gaussian Instrument Broadening\n",
    "    if ipres == 0.0:\n",
    "        hwhm = 0\n",
    "    else:\n",
    "        hwhm = 0.5 * wave[0] / ipres\n",
    "    if hwhm > 0: smod = gaussbroad(wave, smod, hwhm)\n",
    "\n",
    "    if debug:\n",
    "        ccd_time = time.perf_counter() - ccd_time\n",
    "        print('Gaussbroad time: ',ccd_time)\n",
    "\n",
    "    return(smod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def integrate_flux(mu, inten, deltav, vsini, vrt, osamp=1):\n",
    "    \"\"\"\n",
    "    Produces a flux profile by integrating intensity profiles (sampled\n",
    "    at various mu angles) over the visible stellar surface.\n",
    "    Intensity profiles are weighted by the fraction of the projected\n",
    "    stellar surface they represent, apportioning the area between\n",
    "    adjacent MU points equally. Additional weights (such as those\n",
    "    used in a Gauss-Legendre quadrature) can not meaningfully be\n",
    "    used in this scheme.  About twice as many points are required\n",
    "    with this scheme to achieve the precision of Gauss-Legendre\n",
    "    quadrature.\n",
    "    DELTAV, VSINI, and VRT must all be in the same units (e.g. km/s).\n",
    "    If specified, OSAMP should be a positive integer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : array(float) of size (nmu,)\n",
    "        cosine of the angle between the outward normal and\n",
    "        the line of sight for each intensity spectrum in INTEN.\n",
    "    inten : array(float) of size(nmu, npts)\n",
    "        intensity spectra at specified values of MU.\n",
    "    deltav : float\n",
    "        velocity spacing between adjacent spectrum points\n",
    "        in INTEN (same units as VSINI and VRT).\n",
    "    vsini : float\n",
    "        maximum radial velocity, due to solid-body rotation.\n",
    "    vrt : float\n",
    "        radial-tangential macroturbulence parameter, i.e.\n",
    "        np.sqrt(2) times the standard deviation of a Gaussian distribution\n",
    "        of turbulent velocities. The same distribution function describes\n",
    "        the radial motions of one component and the tangential motions of\n",
    "        a second component. Each component covers half the stellar surface.\n",
    "        See 'The Observation and Analysis of Stellar Photospheres', Gray.\n",
    "    osamp : int, optional\n",
    "        internal oversampling factor for convolutions.\n",
    "        By default convolutions are done using the input points (OSAMP=1),\n",
    "        but when OSAMP is set to higher integer values, the input spectra\n",
    "        are first oversampled by cubic spline interpolation.\n",
    "    Returns\n",
    "    -------\n",
    "    value : array(float) of size (npts,)\n",
    "        Disk integrated flux profile.\n",
    "    Note\n",
    "    ------------\n",
    "        If you use this algorithm in work that you publish, please cite\n",
    "        Valenti & Anderson 1996, PASP, currently in preparation.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    History\n",
    "    -----------\n",
    "    Feb-88  GM\n",
    "        Created ANA version.\n",
    "    13-Oct-92 JAV\n",
    "        Adapted from G. Marcy's ANA routi!= of the same name.\n",
    "    03-Nov-93 JAV\n",
    "        Switched to annular convolution technique.\n",
    "    12-Nov-93 JAV\n",
    "        Fixed bug. Intensity compo!=nts not added when vsini=0.\n",
    "    14-Jun-94 JAV\n",
    "        Reformatted for \"public\" release. Heavily commented.\n",
    "        Pass deltav instead of 2.998d5/deltav. Added osamp\n",
    "        keyword. Added rebinning logic at end of routine.\n",
    "        Changed default osamp from 3 to 1.\n",
    "    20-Feb-95 JAV\n",
    "        Added mu as an argument to handle arbitrary mu sampling\n",
    "        and remove ambiguity in intensity profile ordering.\n",
    "        Interpret VTURB as np.sqrt(2)*sigma instead of just sigma.\n",
    "        Replaced call_external with call to spl_{init|interp}.\n",
    "    03-Apr-95 JAV\n",
    "        Multiply flux by pi to give observed flux.\n",
    "    24-Oct-95 JAV\n",
    "        Force \"nmk\" padding to be at least 3 pixels.\n",
    "    18-Dec-95 JAV\n",
    "        Renamed from dskint() to rtint(). No longer make local\n",
    "        copy of intensities. Use radial-tangential instead\n",
    "        of isotropic Gaussian macroturbulence.\n",
    "    26-Jan-99 JAV\n",
    "        For NMU=1 and VSINI=0, assume resolved solar surface#\n",
    "        apply R-T macro, but supress vsini broadening.\n",
    "    01-Apr-99 GMH\n",
    "        Use annuli weights, rather than assuming ==ual area.\n",
    "    07-Mar-12 JAV\n",
    "        Force vsini and vmac to be scalars.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make local copies of various input variables, which will be altered below.\n",
    "    # Force vsini and especially vmac to be scalars. Otherwise mu dependence fails.\n",
    "\n",
    "    if np.size(vsini) > 1:\n",
    "        vsini = vsini[0]\n",
    "    if np.size(vrt) > 1:\n",
    "        vrt = vrt[0]\n",
    "\n",
    "    # Determine oversampling factor.\n",
    "    os = round(np.clip(osamp, 1, None))  # force integral value > 1\n",
    "\n",
    "    # Convert input MU to projected radii, R, of annuli for a star of unit radius\n",
    "    #  (which is just sine, rather than cosine, of the angle between the outward\n",
    "    #  normal and the line of sight).\n",
    "    rmu = np.sqrt(1 - mu ** 2)  # use simple trig identity\n",
    "\n",
    "    # Sort the projected radii and corresponding intensity spectra into ascending\n",
    "    #  order (i.e. from disk center to the limb), which is equivalent to sorting\n",
    "    #  MU in descending order.\n",
    "    isort = np.argsort(rmu)\n",
    "    rmu = rmu[isort]  # reorder projected radii\n",
    "    nmu = np.size(mu)  # number of radii\n",
    "    if nmu == 1:\n",
    "        if vsini != 0:\n",
    "            logger.warning(\n",
    "                \"Vsini is non-zero, but only one projected radius (mu value) is set. No rotational broadening will be performed.\"\n",
    "            )\n",
    "            vsini = 0  # ignore vsini if only 1 mu\n",
    "\n",
    "    # Calculate projected radii for boundaries of disk integration annuli.  The n+1\n",
    "    # boundaries are selected such that r(i+1) exactly bisects the area between\n",
    "    # rmu(i) and rmu(i+1). The in!=rmost boundary, r(0) is set to 0 (disk center)\n",
    "    # and the outermost boundary, r(nmu) is set to 1 (limb).\n",
    "    if nmu > 1 or vsini != 0:  # really want disk integration\n",
    "        r = np.sqrt(\n",
    "            0.5 * (rmu[:-1] ** 2 + rmu[1:] ** 2)\n",
    "        )  # area midpoints between rmu\n",
    "        r = np.concatenate(([0], r, [1]))\n",
    "\n",
    "        # Calculate integration weights for each disk integration annulus.  The weight\n",
    "        # is just given by the relative area of each annulus, normalized such that\n",
    "        # the sum of all weights is unity.  Weights for limb darkening are included\n",
    "        # explicitly in the intensity profiles, so they aren't needed here.\n",
    "        wt = r[1:] ** 2 - r[:-1] ** 2  # weights = relative areas\n",
    "    else:\n",
    "        wt = np.array([1.0])  # single mu value, full weight\n",
    "\n",
    "    # Generate index vectors for input and oversampled points. Note that the\n",
    "    # oversampled indicies are carefully chosen such that every \"os\" finely\n",
    "    # sampled points fit exactly into one input bin. This makes it simple to\n",
    "    # \"integrate\" the finely sampled points at the end of the routine.\n",
    "    npts = inten.shape[1]  # number of points\n",
    "    xpix = np.arange(npts, dtype=float)  # point indices\n",
    "    nfine = os * npts  # number of oversampled points\n",
    "    xfine = (0.5 / os) * (\n",
    "        2 * np.arange(nfine, dtype=float) - os + 1\n",
    "    )  # oversampled points indices\n",
    "\n",
    "    # Loop through annuli, constructing and convolving with rotation kernels.\n",
    "\n",
    "    yfine = np.empty(nfine)  # init oversampled intensities\n",
    "    flux = np.zeros(nfine)  # init flux vector\n",
    "    for imu in range(nmu):  # loop thru integration annuli\n",
    "\n",
    "        #  Use external cubic spline routine (adapted from Numerical Recipes) to make\n",
    "        #  an oversampled version of the intensity profile for the current annulus.\n",
    "        ypix = inten[isort[imu]]  # extract intensity profile\n",
    "        if os == 1:\n",
    "            # just copy (use) original profile\n",
    "            yfine = ypix\n",
    "        else:\n",
    "            # spline onto fine wavelength scale\n",
    "            yfine = interp1d(xpix, ypix, kind=\"cubic\")(xfine)\n",
    "\n",
    "        # Construct the convolution kernel which describes the distribution of\n",
    "        # rotational velocities present in the current annulus. The distribution has\n",
    "        # been derived analytically for annuli of arbitrary thickness in a rigidly\n",
    "        # rotating star. The kernel is constructed in two pieces: o!= piece for\n",
    "        # radial velocities less than the maximum velocity along the inner edge of\n",
    "        # the annulus, and one piece for velocities greater than this limit.\n",
    "        if vsini > 0:\n",
    "            # nontrivial case\n",
    "            r1 = r[imu]  # inner edge of annulus\n",
    "            r2 = r[imu + 1]  # outer edge of annulus\n",
    "            dv = deltav / os  # oversampled velocity spacing\n",
    "            maxv = vsini * r2  # maximum velocity in annulus\n",
    "            nrk = 2 * int(maxv / dv) + 3  ## oversampled kernel point\n",
    "            # velocity scale for kernel\n",
    "            v = dv * (np.arange(nrk, dtype=float) - ((nrk - 1) / 2))\n",
    "            rkern = np.zeros(nrk)  # init rotational kernel\n",
    "            j1 = np.abs(v) < vsini * r1  # low velocity points\n",
    "            rkern[j1] = np.sqrt((vsini * r2) ** 2 - v[j1] ** 2) - np.sqrt(\n",
    "                (vsini * r1) ** 2 - v[j1] ** 2\n",
    "            )  # generate distribution\n",
    "\n",
    "            j2 = (np.abs(v) >= vsini * r1) & (np.abs(v) <= vsini * r2)\n",
    "            rkern[j2] = np.sqrt(\n",
    "                (vsini * r2) ** 2 - v[j2] ** 2\n",
    "            )  # generate distribution\n",
    "\n",
    "            rkern = rkern / np.sum(rkern)  # normalize kernel\n",
    "\n",
    "            # Convolve the intensity profile with the rotational velocity kernel for this\n",
    "            # annulus. Pad each end of the profile with as many points as are in the\n",
    "            # convolution kernel. This reduces Fourier ringing. The convolution may also\n",
    "            # be do!= with a routi!= called \"externally\" from IDL, which efficiently\n",
    "            # shifts and adds.\n",
    "            if nrk > 3:\n",
    "                yfine = convolve(yfine, rkern, mode=\"nearest\")\n",
    "\n",
    "        # Calculate projected sigma for radial and tangential velocity distributions.\n",
    "        muval = mu[isort[imu]]  # current value of mu\n",
    "        sigma = os * vrt / np.sqrt(2) / deltav  # standard deviation in points\n",
    "        sigr = sigma * muval  # reduce by current mu value\n",
    "        sigt = sigma * np.sqrt(1.0 - muval ** 2)  # reduce by np.sqrt(1-mu**2)\n",
    "\n",
    "        # Figure out how many points to use in macroturbulence kernel.\n",
    "        nmk = int(10 * sigma)\n",
    "        nmk = np.clip(nmk, 3, (nfine - 3) // 2)\n",
    "\n",
    "        # Construct radial macroturbulence kernel with a sigma of mu*VRT/np.sqrt(2).\n",
    "        if sigr > 0:\n",
    "            xarg = np.linspace(-nmk, nmk, 2 * nmk + 1) / sigr\n",
    "            xarg = np.clip(-0.5 * xarg ** 2, -20, None)\n",
    "            mrkern = np.exp(xarg)  # compute the gaussian\n",
    "            mrkern = mrkern / np.sum(mrkern)  # normalize the profile\n",
    "        else:\n",
    "            mrkern = np.zeros(2 * nmk + 1)  # init with 0d0\n",
    "            mrkern[nmk] = 1.0  # delta function\n",
    "\n",
    "        # Construct tangential kernel with a sigma of np.sqrt(1-mu**2)*VRT/np.sqrt(2).\n",
    "        if sigt > 0:\n",
    "            xarg = np.linspace(-nmk, nmk, 2 * nmk + 1) / sigt\n",
    "            xarg = np.clip(-0.5 * xarg ** 2, -20, None)\n",
    "            mtkern = np.exp(xarg)  # compute the gaussian\n",
    "            mtkern = mtkern / np.sum(mtkern)  # normalize the profile\n",
    "        else:\n",
    "            mtkern = np.zeros(2 * nmk + 1)  # init with 0d0\n",
    "            mtkern[nmk] = 1.0  # delta function\n",
    "\n",
    "        # Sum the radial and tangential components, weighted by surface area.\n",
    "        area_r = 0.5  # assume equal areas\n",
    "        area_t = 0.5  # ar+at must equal 1\n",
    "        mkern = area_r * mrkern + area_t * mtkern  # add both components\n",
    "\n",
    "        # Convolve the total flux profiles, again padding the spectrum on both ends to\n",
    "        # protect against Fourier ringing.\n",
    "        yfine = convolve(\n",
    "            yfine, mkern, mode=\"nearest\"\n",
    "        )  # add the padding and convolve\n",
    "\n",
    "        # Add contribution from current annulus to the running total.\n",
    "        flux = flux + wt[imu] * yfine  # add profile to running total\n",
    "\n",
    "    flux = np.reshape(flux, (npts, os))  # convert to an array\n",
    "    flux = np.pi * np.sum(flux, axis=1) / os  # sum, normalize\n",
    "    return flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def broaden_spectrum(wint_seg, sint_seg, wave_seg, cmod_seg, vsini=0, vmac=0, debug=False):\n",
    "\n",
    "    nw = len(wint_seg)\n",
    "    clight = 299792.5\n",
    "    mu = (np.sqrt(0.5*(2*np.arange(7)+1)/np.float(7)))[::-1]\n",
    "    nmu = 7\n",
    "    wmid = 0.5 * (wint_seg[nw-1] + wint_seg[0])\n",
    "    wspan = wint_seg[nw-1] - wint_seg[0]\n",
    "    jmin = np.argmin(wint_seg[1:nw-1] - wint_seg[0:nw-2])\n",
    "    vstep1 = min(wint_seg[1:nw-1] - wint_seg[0:nw-2])\n",
    "    vstep2 = 0.1 * wspan / (nw-1) / wmid * clight\n",
    "    vstep3 = 0.05\n",
    "    vstep = np.max([vstep1,vstep2,vstep3])\n",
    "\n",
    "    # Generate model wavelength scale X, with uniform wavelength step.\n",
    "    nx = int(np.floor(np.log10(wint_seg[nw-1] / wint_seg[0])/ np.log10(1.0+vstep / clight))+1)\n",
    "    if nx % 2 == 0: nx += 1\n",
    "    resol_out = 1.0/((wint_seg[nw-1] / wint_seg[0])**(1.0/(nx-1.0))-1.0)\n",
    "    vstep = clight / resol_out\n",
    "    x_seg = wint_seg[0] * (1.0 + 1.0 / resol_out)**np.arange(nx)\n",
    "\n",
    "    # Interpolate intensity spectra onto new model wavelength scale.  \n",
    "    yi_seg = np.empty((nmu, nx))\n",
    "\n",
    "    for imu in range(nmu):\n",
    "        yi_seg[imu] = np.interp(x_seg, wint_seg, sint_seg[imu])\n",
    "\n",
    "    y_seg = integrate_flux(mu, yi_seg, vstep, np.abs(vsini), np.abs(vmac))\n",
    "\n",
    "    dispersion = vstep1\n",
    "    wave_equi = np.arange(x_seg[0],x_seg[-1]+dispersion,dispersion)\n",
    "\n",
    "    c_seg = np.interp(wave_equi,wave_seg,cmod_seg)\n",
    "    y_seg = np.interp(wave_equi,x_seg,y_seg)\n",
    "\n",
    "    if debug:\n",
    "        print(vstep1,len(wave_equi))\n",
    "    \n",
    "    return(wave_equi,y_seg/c_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a common wavelength grid that we will use for all spectra\n",
    "wavelengths_for_each_ccd = dict()\n",
    "for ccd in [1,2,3,4]:\n",
    "    example_ccd = readsav(synthesis_files+'marcs2014_scaledsolar_210720_0_ccd'+str(ccd)+'_smod_sint.sav').results[0]\n",
    "    example_wave, example_flux = broaden_spectrum(example_ccd.wint, example_ccd.sint, example_ccd.wave, example_ccd.cmod, vsini=0, vmac=0)\n",
    "    wavelengths_for_each_ccd['CCD'+str(ccd)] = example_wave\n",
    "\n",
    "wavelength_array = np.concatenate(([wavelengths_for_each_ccd['CCD'+ccd] for ccd in ['1','2','3','4']]))\n",
    "\n",
    "wavelength_file_opener = open(wavelength_file,'wb')\n",
    "pickle.dump((wavelength_array),wavelength_file_opener)\n",
    "wavelength_file_opener.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a matrix that we will later fill with the normalised flux values\n",
    "normalized_flux = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "normalized_ivar = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_normalised_spectra(index, wavelengths_for_each_ccd, vsini=0.0, spectrum_path = '/Users/svenbuder/galah_solar_twins/dr3_spectra/hermes'):\n",
    "    \n",
    "    # For each stars, there are 4 spectra for the 4 different CCDs.\n",
    "    # We will interpolate the fluxes and uncertainties/inverse variances onto a common grid\n",
    "    normalised_flux_for_index = []\n",
    "    normalised_ivar_for_index = []\n",
    "    # For that we first interpolate over the individual CCDs\n",
    "    #for ccd in ['3','4']:\n",
    "    for ccd in ['1','2','3','4']:\n",
    "    #for ccd in ['2','3']:\n",
    "        \n",
    "        #synthetic_spectrum = readsav(synthesis_files+'/solar_twin_grid_210831_'+str(index)+'_ccd'+ccd+'_smod_sint.sav').results[0]\n",
    "        synthetic_spectrum = readsav(synthesis_files+'/marcs2014_scaledsolar_210720_'+str(index)+'_ccd'+ccd+'_smod_sint.sav').results[0]\n",
    "\n",
    "        wave_broadened,flux_broadened = broaden_spectrum(\n",
    "            synthetic_spectrum.wint,\n",
    "            synthetic_spectrum.sint,\n",
    "            synthetic_spectrum.wave,\n",
    "            synthetic_spectrum.cmod,\n",
    "            vsini=vsini)\n",
    "        \n",
    "        interpolated_broadened_smod = np.interp(wavelengths_for_each_ccd['CCD'+ccd],synthetic_spectrum.wave,broadened_smod)\n",
    "        \n",
    "        normalised_flux_for_index.append(interpolated_broadened_smod)\n",
    "        # We use synthetic spectra, so SNR == infinity\n",
    "        # Let's assume SNR = 1000, so std == 0.001, so 1/var = 1/std**2 = 1,000,000.\n",
    "        normalised_ivar_for_index.append(1000000.*np.ones_like(interpolated_broadened_smod))\n",
    "        \n",
    "    normalised_flux_for_index = np.concatenate((normalised_flux_for_index))\n",
    "    normalised_ivar_for_index = np.concatenate((normalised_ivar_for_index))\n",
    "    \n",
    "    return(normalised_flux_for_index,normalised_ivar_for_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def populate_normalised_flux_and_ivar_matrix(training_set, matrix_index, wavelengths_for_each_ccd):\n",
    "    index = training_set['INDEX'][matrix_index]\n",
    "    #try:\n",
    "    normalised_flux_for_index, normalised_ivar_for_index = load_normalised_spectra(index,wavelengths_for_each_ccd=wavelengths_for_each_ccd)\n",
    "    #except:\n",
    "    #    print('Failed to load spectrum for index '+str(matrix_index)+', that is, index '+str(index))\n",
    "    normalized_flux[matrix_index] = normalised_flux_for_index\n",
    "    normalized_ivar[matrix_index] = normalised_ivar_for_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250.0 4.5 0.0 75\n"
     ]
    }
   ],
   "source": [
    "training_set_all = Table.read('../../spectrum_grids/marcs2014/marcs2014_scaledsolar_210720.fits')\n",
    "\n",
    "# 2160 models with\n",
    "# teff 2500..(100/250)..8000 K including 5 at a time\n",
    "# logg = -0.5..(0.5)..5.0/5.5 dex\n",
    "# feh = -3.0..(0.5/0.25)..1.0 dex\n",
    "\n",
    "teff_points = np.unique(training_set_all['TEFF'])\n",
    "logg_points = np.unique(training_set_all['LOGG'])\n",
    "fe_h_points = np.unique(training_set_all['FEH'])\n",
    "\n",
    "cannon_index = []\n",
    "cannon_teff = []\n",
    "cannon_logg = []\n",
    "cannon_feh = []\n",
    "\n",
    "i = 0\n",
    "for teff in teff_points[2:-3]:\n",
    "    for logg in logg_points[1:-2]:\n",
    "        for fe_h in fe_h_points[4:-3]:\n",
    "            teff_i = np.where(teff==teff_points)[0][0]\n",
    "            logg_i = np.where(logg==logg_points)[0][0]\n",
    "            fe_h_i = np.where(fe_h==fe_h_points)[0][0]\n",
    "            \n",
    "            grid = (\n",
    "                    (training_set_all['TEFF'] >= teff_points[teff_i-2]) &\n",
    "                    (training_set_all['TEFF'] <= teff_points[teff_i+2]) &\n",
    "                    (training_set_all['LOGG'] >= logg_points[logg_i-1]) &\n",
    "                    (training_set_all['LOGG'] <= logg_points[logg_i+1]) &\n",
    "                    (training_set_all['FEH'] >= fe_h_points[fe_h_i-2]) &\n",
    "                    (training_set_all['FEH'] <= fe_h_points[fe_h_i+2]) &\n",
    "                    ((training_set_all['INDEX'] < 107) | (training_set_all['INDEX'] > 109)) &\n",
    "                    ((training_set_all['INDEX'] < 119) | (training_set_all['INDEX'] > 122)) &\n",
    "                    ((training_set_all['INDEX'] < 131) | (training_set_all['INDEX'] > 134)) &\n",
    "                    ((training_set_all['INDEX'] < 143) | (training_set_all['INDEX'] > 146)) &\n",
    "                    ((training_set_all['INDEX'] < 203) | (training_set_all['INDEX'] > 205)) &\n",
    "                    ((training_set_all['INDEX'] < 212) | (training_set_all['INDEX'] > 213)) &\n",
    "                    ((training_set_all['INDEX'] < 3980) | (training_set_all['INDEX'] > 3980)) \n",
    "                \n",
    "                )\n",
    "            \n",
    "            grid_size = len(training_set_all[grid])\n",
    "            \n",
    "            if grid_size > 10:\n",
    "\n",
    "                cannon_index.append(i)\n",
    "                cannon_teff.append(teff)\n",
    "                cannon_logg.append(logg)\n",
    "                cannon_feh.append(fe_h)\n",
    "\n",
    "                #if (teff == 5500) & (logg == 4.5) & (fe_h == 0.00):\n",
    "                #    print(i)\n",
    "                \n",
    "                if i in [1813]:#550,1000,1206,1656,1660,1664,1668,1669,1670,1671,1672,1673,1744,2000,2050]:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #if (grid_size > 0) & (grid_size < 50):\n",
    "                    print(teff,logg,fe_h,grid_size)\n",
    "\n",
    "                    training_set = copy.deepcopy(training_set_all[grid])\n",
    "                    training_set.write('training_sets/subgrid_'+str(i)+'_training_set_marcs2014.fits',overwrite=True)\n",
    "\n",
    "                    flux_ivar_file = 'training_sets/subgrid_'+str(i)+'_training_marcs2014_flux_ivar.pickle'\n",
    "\n",
    "                    # Let's create a matrix that we will later fill with the normalised flux values\n",
    "                    normalized_flux = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "                    normalized_ivar = np.ones((np.shape(training_set)[0],np.shape(wavelength_array)[0]))\n",
    "\n",
    "                    [populate_normalised_flux_and_ivar_matrix(training_set, matrix_index=index, wavelengths_for_each_ccd=wavelengths_for_each_ccd) for index in range(np.shape(training_set)[0])];\n",
    "\n",
    "                    flux_ivar_file_opener = open(flux_ivar_file,'wb')\n",
    "                    pickle.dump((normalized_flux,normalized_ivar),flux_ivar_file_opener)\n",
    "                    flux_ivar_file_opener.close()\n",
    "\n",
    "                i+= 1\n",
    "\n",
    "cannon_grid = Table()\n",
    "cannon_grid['index'] = np.array(cannon_index)\n",
    "cannon_grid['teff'] = np.array(cannon_teff)\n",
    "cannon_grid['logg'] = np.array(cannon_logg)\n",
    "cannon_grid['fe_h'] = np.array(cannon_feh)\n",
    "cannon_grid.write('Cannon_subgrid.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
